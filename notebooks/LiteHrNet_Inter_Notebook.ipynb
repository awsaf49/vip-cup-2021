{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LiteHrNet  Inter Notebook.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WrZu3cXM3IP_"
      },
      "source": [
        "# Set Variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m5f_o3nJH7ha"
      },
      "source": [
        "!wget 'https://storage.googleapis.com/kaggle-data-sets/1360215/2429215/bundle/archive.zip?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=gcp-kaggle-com%40kaggle-161607.iam.gserviceaccount.com%2F20210808%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20210808T122320Z&X-Goog-Expires=259199&X-Goog-SignedHeaders=host&X-Goog-Signature=95eb7522461a1e68faa579645cb304c52b09421fc17a4c1d9a26eaeb02d2473e87ca3c5e1eb5d467b1a6c10ca6224d727a398a5603a3a2dd8d53c96854b35e841801516ac34c9f0edf022e7289ba719940774414311b897342428d49753260be7f1e5df15222709bead811a7bf96610c7d7f07839b0c054ae4d71a9f877bc9b325ecfa368b878f950e3f2b58fd4c3f08ecbaa40d61ce513569bf6a42480c7190a4243d912e58f07e67d6dbd8f2b78d6b8a656adc1a43afc70769637f45eb6b419e90a6107ac95b32fc2ee9bb1c8217ba66cbbdfd67dc06351b81e8dfdae63a53f48fe82183d418da54f0b845e349d3d1759f31b7852b593481aafc387f431fe6' -O data.zip\n",
        "!mkdir -p /content/vipcup2021-dataset\n",
        "!unzip -q /content/data.zip -d /content/vipcup2021-dataset\n",
        "!rm -r /content/data.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_tE-ySP2ICdl"
      },
      "source": [
        "!pip install gdown\n",
        "!gdown --id 1N0Q9TOCdX-R4yI7kFhL0Dg39EytlP1QD"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ae_5SAc2u02"
      },
      "source": [
        "vip_data_dir = \"/content/vipcup2021-dataset\" \n",
        "WEIGHT_PATH = \"/content/liteHRNet.pth\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KFib4Gne3S9T"
      },
      "source": [
        "# Data Creation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EoLpnrEgRy4W"
      },
      "source": [
        "import shutil, os\n",
        "\n",
        "!mkdir personbbox\n",
        "%cd personbbox\n",
        "!gdown --id 1huBWdHHJYvXDa4nIp4o-0h_X6RB7UOH_ ## vip100\n",
        "vip100e_zip='/content/personbbox/vip21personbbox100e.zip'\n",
        "!unzip {vip100e_zip}\n",
        "!rm {vip100e_zip}\n",
        "shutil.copytree('/content/personbbox/vip21personbbox100e/yolo/yolov5/runs/detect/exp/labels', '/content/labels_test')\n",
        "\n",
        "%cd ..\n",
        "shutil.rmtree('/content/personbbox') "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R2XtrligR4MG"
      },
      "source": [
        "WIDTH = 384\n",
        "HEIGHT = 512\n",
        "BATCH_SIZE = 32\n",
        "Labels_test = \"/content/labels_test\"\n",
        "config_path = \"custom_configs/coco_configs/litehrnet_30_coco_384x288.py\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P5Km3i5m3SeW"
      },
      "source": [
        "!pip install -q imagesize"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FIwIg1dW3YAX"
      },
      "source": [
        "%%writefile /content/test_coco.py\n",
        "\n",
        "import argparse\n",
        "import numpy as np\n",
        "import cv2 \n",
        "import matplotlib.pyplot as plt\n",
        "import scipy.io as scio\n",
        "import pandas as pd\n",
        "from tqdm.notebook import tqdm\n",
        "import os, shutil\n",
        "from glob import glob\n",
        "tqdm.pandas()\n",
        "import json\n",
        "import datetime\n",
        "import imagesize\n",
        "from sklearn.model_selection import GroupKFold \n",
        "import scipy.io as scio\n",
        "import cv2\n",
        "\n",
        "def load_kps(kp_path, width, height, new_width, new_height):\n",
        "    gt  = scio.loadmat(kp_path)['joints_gt'] # label = if_ocluded\n",
        "    kps = gt.transpose(2, 1, 0).astype(np.float64) # => (num_image, num_limb, 3) or (None, 14, 3)\n",
        "    kps[..., 0] = (kps[...,0]-1)/width*new_width    # converting one indexing to zero indexing\n",
        "    kps[..., 1] = (kps[...,1]-1)/height*new_height\n",
        "    kps[..., 2] = 2- kps[...,2] # coco format\n",
        "    return kps.astype(np.int32)\n",
        "\n",
        "def load_image(image_path):\n",
        "    return cv2.imread(image_path)[...,::-1]\n",
        "\n",
        "\n",
        "def read_resize(file_path, dim=128, width=128, height=128, aspect_ratio=True):\n",
        "    img = load_image(file_path)\n",
        "    h, w = img.shape[:2]  # orig hw\n",
        "    if aspect_ratio:\n",
        "        r = dim / max(h, w)  # resize image to img_size\n",
        "        interp = cv2.INTER_AREA if r < 1 else cv2.INTER_LINEAR\n",
        "        if r != 1:  # always resize down, only resize up if training with augmentation\n",
        "            img = cv2.resize(img, (int(w * r), int(h * r)), interpolation=interp)\n",
        "            new_h, new_w = img.shape[:2]\n",
        "    else:\n",
        "        img = cv2.resize(img, (width,height), cv2.INTER_AREA)\n",
        "        new_w = dim; new_h = dim\n",
        "        \n",
        "    return img, w, h\n",
        "\n",
        "\n",
        "def get_image_info(file_name, height, width, id,\n",
        "                   license=1, date_captured='', \n",
        "                   coco_url='', flickr_url='',):\n",
        "    return dict(license=license, \n",
        "                file_name=file_name,\n",
        "                coco_url=coco_url,\n",
        "                height=height,\n",
        "                width=width, \n",
        "                date_captured=date_captured,\n",
        "                flickr_url=flickr_url,\n",
        "                id=id)\n",
        "    \n",
        "\n",
        "\n",
        "def get_annot_info(kps, id, image_id, category_id=1, bbox=None, area=None,\n",
        "                   iscrowd=0, segmentation=None):\n",
        "    \"\"\"\n",
        "    Parameters:\n",
        "    --------------- \n",
        "    Args:\n",
        "    \n",
        "    Returns:\n",
        "    \n",
        "    \"\"\"\n",
        "    return {\n",
        "        \"segmentation\": segmentation or [],\n",
        "        \"num_keypoints\": len(kps)//3,\n",
        "        \"area\": area if area else (bbox[2]*bbox[3]),\n",
        "        \"iscrowd\": iscrowd,\n",
        "        \"keypoints\": kps,\n",
        "        \"image_id\": image_id,\n",
        "        \"bbox\": bbox or [0, 0, 0, 0],\n",
        "        \"category_id\": category_id,\n",
        "        \"id\": id,\n",
        "    }\n",
        "\n",
        "\n",
        "def get_bbox_info(id, image_id, category_id=1, bbox=None, area=None,\n",
        "                   iscrowd=0, segmentation=None):\n",
        "    \"\"\"\n",
        "    Parameters:\n",
        "    --------------- \n",
        "    Args:\n",
        "    \n",
        "    Returns:\n",
        "    \n",
        "    \"\"\"\n",
        "    return {\n",
        "        \"segmentation\": segmentation or [],\n",
        "        \"num_keypoints\": 14,\n",
        "        \"area\": area if area else (bbox[2]*bbox[3]),\n",
        "        \"iscrowd\": iscrowd,\n",
        "        \"keypoints\": [0,0,2]*14,\n",
        "        \"image_id\": image_id,\n",
        "        \"bbox\": bbox or [0, 0, 0, 0],\n",
        "        \"category_id\": category_id,\n",
        "        \"id\": id,\n",
        "    }\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('--dim', type=int, default=128, help='resized image shape')\n",
        "    parser.add_argument('--fold', type=int, default=0, help='fold number')\n",
        "    parser.add_argument('--is_annot', action='store_true', help=\"is there annotaions to use\")\n",
        "    parser.add_argument('--is_test', action='store_true', help=\"testing\")\n",
        "    parser.add_argument('--vip_folder', type=str, default=\"train\", help=\"VIP CUP DATA FOLDER\")\n",
        "    parser.add_argument(\"--coco_folder\", type=str, default=\"train\", help=\"folder used in coco dataset\")\n",
        "    parser.add_argument(\"--bbox_label_test\", type=str, default=\"/content/labels\", help=\"folder containing yolo labels of test person bbox\")\n",
        "    parser.add_argument(\"--base_dir\", type=str, default=\"/content/data\", help=\"base dir for vip dataset folder\")\n",
        "    parser.add_argument(\"--label\", type=str, default=\"uncover\" , help=\"uncover, cover1, cover2\")\n",
        "    parser.add_argument(\"--label2\", type=str, default=\"null\" , help=\"cover1, cover2\")\n",
        "    parser.add_argument(\"--out_dir\", type=str, default=\"/content\" , help=\"output directory\")\n",
        "    parser.add_argument('--is_aspect_ratio', action='store_true', help=\"mainatain aspect ratio. Only use dim. don't use width and height\")\n",
        "    parser.add_argument('--width', type=int, default=128, help='fold number')\n",
        "    parser.add_argument('--height', type=int, default=128, help='fold number')\n",
        "\n",
        "    opt = parser.parse_args()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    skeleton = [(0, 1), (1, 2), (12, 2), (12, 3), (3, 4), (4, 5), (6, 7),\n",
        "                (7, 8), (8, 12), (12, 9), (9, 10), (10, 11), (12, 13)]\n",
        "    skeleton = [[x[0]+1, x[1]+1] for x in skeleton]\n",
        "    name2idx = {\n",
        "        \"Right ankle\":0,\n",
        "        \"Right knee\":1,\n",
        "        \"Right hip\":2,\n",
        "        \"Left hip\":3,\n",
        "        \"Left knee\":4,\n",
        "        \"Left ankle\":5,\n",
        "        \"Right wrist\":6,\n",
        "        \"Right elbow\":7,\n",
        "        \"Right shoulder\":8,\n",
        "        \"Left shoulder\":9,\n",
        "        \"Left elbow\":10,\n",
        "        \"Left wrist\":11,\n",
        "        \"thorax\":12,\n",
        "        \"head top\":13, \n",
        "    }\n",
        "    idx2name = {v:k for k,v in name2idx.items()}\n",
        "    names = list(idx2name.values())\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def get_info(filepath):\n",
        "        x = filepath.split('/')\n",
        "        image_id = x[-1]\n",
        "        label    = x[-2]\n",
        "        modality = x[-3]\n",
        "        study_id = x[-4]\n",
        "        split    = x[-5]\n",
        "        return [filepath, study_id, image_id, modality, label, split]\n",
        "\n",
        "\n",
        "\n",
        "    filepaths = glob(f'{opt.base_dir}/**/*png', recursive=True)\n",
        "    filepaths.sort()\n",
        "    df = pd.DataFrame(list(map(get_info, filepaths)), columns=['image_path', 'study_id', 'image_id',\n",
        "                                                            'modality', 'label', 'split'])\n",
        "\n",
        "\n",
        "\n",
        "    df['rgb_gt_path']    = df.image_path.map(lambda x: os.path.join(x.rsplit('/', 3)[0], 'joints_gt_RGB.mat'))\n",
        "    df['ir_gt_path']     = df.image_path.map(lambda x: os.path.join(x.rsplit('/', 3)[0], 'joints_gt_IR.mat'))\n",
        "    df['rgb_align_path'] = df.image_path.map(lambda x: os.path.join(x.rsplit('/', 3)[0], 'align_PTr_RGB.npy'))\n",
        "    df['ir_align_path']  = df.image_path.map(lambda x: os.path.join(x.rsplit('/', 3)[0], 'align_PTr_IR.npy'))\n",
        "\n",
        "    df[['width', 'height']] = df.image_path.progress_apply(lambda x: list(imagesize.get(x))).tolist()\n",
        "\n",
        "\n",
        "        \n",
        "    df = df[df.split == opt.vip_folder]\n",
        "    df = df[df.modality == \"IR\"]\n",
        "\n",
        "    if opt.vip_folder == \"train\":\n",
        "        gkf = GroupKFold(n_splits=5)\n",
        "        df['fold'] = -1\n",
        "        df.reset_index(drop=True, inplace=True)\n",
        "        for fold, (train_idx, val_idx) in enumerate(gkf.split(df, groups=df['study_id'])):\n",
        "            df.loc[val_idx, 'fold'] = fold\n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "    FOLD = opt.fold\n",
        "    if opt.vip_folder == \"train\" and opt.coco_folder == \"train\":\n",
        "        train_df = df[(df.fold!=FOLD) & (df.label==opt.label)]\n",
        "    elif opt.vip_folder == \"train\" and opt.coco_folder == \"val\":\n",
        "        train_df = df[(df.fold==FOLD) & (df.label==opt.label)]\n",
        "    else:\n",
        "        if opt.label2 == \"null\":\n",
        "            train_df = df[(df.label==opt.label)]\n",
        "        else:\n",
        "            train_df = df[(df.label==opt.label) | (df.label==opt.label2)]\n",
        "\n",
        "\n",
        "    INFO = {\n",
        "        \"description\": \"VIP CUP 2021 Dataset\",\n",
        "        \"url\": \"https://www.kaggle.com/awsaf49/ieee-vip-cup-2021-train-val-dataset\",\n",
        "        \"version\": \"0.1.0\",\n",
        "        \"year\": 2021,\n",
        "        \"contributor\": \"awsaf\",\n",
        "        \"date_created\": datetime.datetime.utcnow().isoformat(' ')\n",
        "    }\n",
        "\n",
        "    LICENSES = [\n",
        "        {\n",
        "            \"id\": 1,\n",
        "            \"name\": \"Attribution-NonCommercial-ShareAlike License\",\n",
        "            \"url\": \"http://creativecommons.org/licenses/by-nc-sa/2.0/\"\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    CATEGORIES = [\n",
        "        {\n",
        "            'id': 1,\n",
        "            'name': 'person',\n",
        "            'supercategory': 'person',\n",
        "            \"keypoints\": names,\n",
        "            \"skeleton\": skeleton\n",
        "        },\n",
        "    ]\n",
        "\n",
        "    coco_output = {\n",
        "        \"info\": INFO,\n",
        "        \"licenses\": LICENSES,\n",
        "        \"categories\": CATEGORIES,\n",
        "        \"images\": [],\n",
        "        \"annotations\": []\n",
        "    }\n",
        "\n",
        "\n",
        "\n",
        "    image_dir = f'{opt.out_dir}/coco2017/{opt.coco_folder}2017'\n",
        "    annot_dir = f'{opt.out_dir}/coco2017/annotations'\n",
        "    os.makedirs(image_dir, exist_ok=True)\n",
        "    os.makedirs(annot_dir, exist_ok=True)\n",
        "\n",
        "\n",
        "\n",
        "    IMAGES = []\n",
        "    ANNOTATIONS = []\n",
        "    \n",
        "\n",
        "    coco_image_id=1\n",
        "    coco_annot_id=1\n",
        "    for idx in tqdm(range(train_df.shape[0])):\n",
        "        image_path = train_df.image_path.iloc[idx]\n",
        "        image_id   = train_df.image_id.iloc[idx]\n",
        "        study_id   = train_df.study_id.iloc[idx]\n",
        "        image_idx  = int(image_id.split('.')[0].split('_')[-1])-1\n",
        "        if opt.is_aspect_ratio:\n",
        "            image, width, height  = read_resize(image_path, dim=opt.dim)\n",
        "        else:\n",
        "            image, width, height  = read_resize(image_path, width=opt.width, height=opt.height, aspect_ratio=False)\n",
        "        new_height, new_width = image.shape[:2]\n",
        "        file_name = study_id + '_' + image_path.split(\"/\")[3] + \"_\" + image_path.split('/')[-1]\n",
        "\n",
        "        \n",
        "        new_image_path  = os.path.join(image_dir,file_name)\n",
        "        # writing image\n",
        "        cv2.imwrite(new_image_path, image[...,::-1])\n",
        "        # writing data\n",
        "        IMAGES.append(get_image_info(file_name, \n",
        "                                    height=int(new_height), \n",
        "                                    width=int(new_width), \n",
        "                                    id=coco_image_id,))\n",
        "        \n",
        "        if opt.is_test:\n",
        "            label_file = opt.bbox_label_test + '/' + file_name[:-3] + 'txt'\n",
        "            # SEE THIS \n",
        "            label_file = label_file.replace(f'{opt.vip_folder}', 'ieee-vip-cup-2021-train-val-dataset')\n",
        "            with open(label_file, \"r\") as f:\n",
        "                data = np.array(f.read().replace('\\n', ' ').strip().split(' ')).astype(np.float32).reshape(-1, 6)[0]\n",
        "                xc, yc, w, h = data[1], data[2], data[3], data[4]\n",
        "                # using new height, new width --> REMEMBER THIS\n",
        "                xc, yc = xc*new_width, yc*new_height\n",
        "                w, h = w*new_width, h*new_height\n",
        "            \n",
        "            xmin, ymin = xc - (w/2), yc - (h/2)\n",
        "            bbox = [int(xmin), int(ymin), int(w), int(h)]\n",
        "            ANNOTATIONS.append(get_bbox_info(id=coco_annot_id, image_id=coco_image_id, category_id=1,\n",
        "                                            bbox=bbox, \n",
        "                                            area=w*h,\n",
        "                                            iscrowd=0,\n",
        "                                            segmentation=None))\n",
        "            # print(ANNOTATIONS)\n",
        "            coco_annot_id+=1\n",
        "            \n",
        "\n",
        "        if opt.is_annot:\n",
        "            kp_path = train_df.ir_gt_path.iloc[idx]\n",
        "            kps = load_kps(kp_path, \n",
        "                        width, height,\n",
        "                        new_width, new_height)\n",
        "            # kp of a image\n",
        "            kps_img = kps[image_idx]\n",
        "            # bbox from keypoints\n",
        "            xmin, ymin, xmax, ymax = np.min(kps_img[...,0]), np.min(kps_img[...,1]), np.max(kps_img[...,0]), np.max(kps_img[...,1])\n",
        "            offsetMin = int(15 * np.square((new_height*new_width) / (512*384)))\n",
        "            offsetMax = int(35 * np.square((new_height*new_width) / (512*384)))\n",
        "            xmin, ymin = int(xmin-offsetMin), int(ymin-offsetMax) # kp are too close to body so taking offset\n",
        "            xmin = max(0, xmin)\n",
        "            ymin = max(0, ymin)\n",
        "            w,h = int(xmax-xmin+offsetMin), int(ymax-ymin+offsetMax)\n",
        "            if opt.is_aspect_ratio:\n",
        "                w = min(w, opt.dim)\n",
        "                h = min(h, opt.dim)\n",
        "            else:\n",
        "                w = min(w, opt.width)\n",
        "                h = min(h, opt.height)\n",
        "            bbox = [xmin, ymin, w, h]\n",
        "\n",
        "            #============================\n",
        "            kps_img = [int(x) for x in kps_img.reshape(-1).tolist()]\n",
        "            \n",
        "            \n",
        "            \n",
        "            ANNOTATIONS.append(get_annot_info(kps=kps_img, id=coco_annot_id, image_id=coco_image_id, category_id=1,\n",
        "                                            bbox=bbox, \n",
        "                                            area=w*h,\n",
        "                                            iscrowd=0,\n",
        "                                            segmentation=None))\n",
        "            \n",
        "            coco_annot_id+=1\n",
        "        coco_image_id+=1\n",
        "        \n",
        "    #===========================\n",
        "    coco_output[\"images\"]      = IMAGES\n",
        "    coco_output[\"annotations\"] = ANNOTATIONS\n",
        "\n",
        "    # json file\n",
        "    with open(f'{annot_dir}/person_keypoints_{opt.coco_folder}2017.json', 'w') as output_json_file:\n",
        "        json.dump(coco_output, output_json_file)   \n",
        "\n",
        "\n",
        "    print(f\"Total {len(os.listdir(image_dir))} images found\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hgQkDB2b3c9D"
      },
      "source": [
        "!python  /content/test_coco.py --width 384 \\\n",
        "--height 512 \\\n",
        "--vip_folder \"test1\" \\\n",
        "--coco_folder \"test\" \\\n",
        "--label \"cover1\" \\\n",
        "--label2 \"cover2\" \\\n",
        "--base_dir $vip_data_dir  \\\n",
        "--out_dir \"/content\" \\\n",
        "--is_test \\\n",
        "--bbox_label_test $Labels_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QZpEiBtU3kji"
      },
      "source": [
        "# Model & Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0VHamxWK3mx0"
      },
      "source": [
        "!pip uninstall -y numpy\n",
        "!pip install numpy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LyUtLVRJ3ppC"
      },
      "source": [
        "!pip install mmcv-full -f https://download.openmmlab.com/mmcv/dist/{11.2}/{1.9.0}/index.html"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g5usS5Ju3plt"
      },
      "source": [
        "!git clone https://Md-Jahin-Alam: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@github.com/Najib-Haq/MMPose.git mmpose "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zI22XojC3piW"
      },
      "source": [
        "%cd mmpose/mmpose_folder\n",
        "!pip install -r requirements.txt \n",
        "!python setup.py develop "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-hUj_GOp3pfn"
      },
      "source": [
        "import shutil\n",
        "shutil.copytree(\"/content/coco2017\", \"/content/mmpose/mmpose_folder/data\") "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PZ5pvRU54j9Y"
      },
      "source": [
        "# Infer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KWgWUNZO3pcn"
      },
      "source": [
        "!python tools/test.py $config_path $WEIGHT_PATH  \\\n",
        "--batch $BATCH_SIZE \\\n",
        "--out \"/content/result.json\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FxEtstrZ3paW"
      },
      "source": [
        "%%writefile /content/submission.py\n",
        "\n",
        "\n",
        "\n",
        "import argparse\n",
        "import numpy as np\n",
        "import os, shutil\n",
        "import json\n",
        "import datetime\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('--train_width', type=int, default=0, help='height of training image')\n",
        "    parser.add_argument('--train_height', type=int, default=0, help='height of training image')\n",
        "    parser.add_argument('--orig_width', type=int, default=0, help='original width')\n",
        "    parser.add_argument('--orig_height', type=int, default=0, help='original height')\n",
        "    parser.add_argument('--result_json_path', type=str, default=\"result.json\", help=\"result json\")\n",
        "\n",
        "    opt = parser.parse_args()\n",
        "\n",
        "    with open(opt.result_json_path) as f:\n",
        "        result = json.load(f)\n",
        "\n",
        "    points = []\n",
        "    for pred in result:\n",
        "        one_person = []\n",
        "        pred = pred['preds'][0]\n",
        "        for point in pred:\n",
        "            one_person.append([int(point[0]*(opt.orig_width/opt.train_width)), int(point[1]*(opt.orig_height/opt.train_height))])\n",
        "        points.append(one_person)\n",
        "\n",
        "    array = np.array(points)\n",
        "    array[:,:,0] = array[:,:,0].clip(min=0, max=opt.orig_width-1)\n",
        "    array[:,:,1] = array[:,:,1].clip(min=0, max=opt.orig_height-1)\n",
        "\n",
        "    points = array.tolist()\n",
        "\n",
        "    \n",
        "    with open(\"preds.json\", \"w\") as f:\n",
        "        json.dump(points, f)\n",
        "\n",
        "    print(\"Result saved in preds.json. Look at your current directory. This is the prediction file. Zip it for submission\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JI2Gp_UR3pX0"
      },
      "source": [
        "!python /content/submission.py --train_width 384 \\\n",
        "--train_height 512 \\\n",
        "--orig_width 120 \\\n",
        "--orig_height 160 \\\n",
        "--result_json_path \"/content/result.json\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rUnqOKgd3pU2"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o9myHQto3pSs"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fWLZ_eWn3pNv"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}