{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "VIP21 TransPose H version.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q5hzz7DL7ojb",
        "outputId": "8bdd64a2-6164-40d5-f0d5-86265fd4ea78"
      },
      "source": [
        "# !wget 'https://storage.googleapis.com/kaggle-data-sets/1360215/2429215/bundle/archive.zip?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=gcp-kaggle-com%40kaggle-161607.iam.gserviceaccount.com%2F20210808%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20210808T122320Z&X-Goog-Expires=259199&X-Goog-SignedHeaders=host&X-Goog-Signature=95eb7522461a1e68faa579645cb304c52b09421fc17a4c1d9a26eaeb02d2473e87ca3c5e1eb5d467b1a6c10ca6224d727a398a5603a3a2dd8d53c96854b35e841801516ac34c9f0edf022e7289ba719940774414311b897342428d49753260be7f1e5df15222709bead811a7bf96610c7d7f07839b0c054ae4d71a9f877bc9b325ecfa368b878f950e3f2b58fd4c3f08ecbaa40d61ce513569bf6a42480c7190a4243d912e58f07e67d6dbd8f2b78d6b8a656adc1a43afc70769637f45eb6b419e90a6107ac95b32fc2ee9bb1c8217ba66cbbdfd67dc06351b81e8dfdae63a53f48fe82183d418da54f0b845e349d3d1759f31b7852b593481aafc387f431fe6' -O data.zip\n",
        "# !mkdir -p /content/vipcup2021-dataset\n",
        "# !unzip -q /content/data.zip -d /content/vipcup2021-dataset\n",
        "# !rm -r /content/data.zip"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-08-08 20:02:07--  https://storage.googleapis.com/kaggle-data-sets/1360215/2429215/bundle/archive.zip?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=gcp-kaggle-com%40kaggle-161607.iam.gserviceaccount.com%2F20210808%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20210808T122320Z&X-Goog-Expires=259199&X-Goog-SignedHeaders=host&X-Goog-Signature=95eb7522461a1e68faa579645cb304c52b09421fc17a4c1d9a26eaeb02d2473e87ca3c5e1eb5d467b1a6c10ca6224d727a398a5603a3a2dd8d53c96854b35e841801516ac34c9f0edf022e7289ba719940774414311b897342428d49753260be7f1e5df15222709bead811a7bf96610c7d7f07839b0c054ae4d71a9f877bc9b325ecfa368b878f950e3f2b58fd4c3f08ecbaa40d61ce513569bf6a42480c7190a4243d912e58f07e67d6dbd8f2b78d6b8a656adc1a43afc70769637f45eb6b419e90a6107ac95b32fc2ee9bb1c8217ba66cbbdfd67dc06351b81e8dfdae63a53f48fe82183d418da54f0b845e349d3d1759f31b7852b593481aafc387f431fe6\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 142.250.141.128, 142.251.2.128, 74.125.137.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|142.250.141.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3390751485 (3.2G) [application/zip]\n",
            "Saving to: ‘data.zip’\n",
            "\n",
            "data.zip            100%[===================>]   3.16G  39.7MB/s    in 54s     \n",
            "\n",
            "2021-08-08 20:03:01 (59.8 MB/s) - ‘data.zip’ saved [3390751485/3390751485]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XCMfHY-uFNDF"
      },
      "source": [
        "# PARAMS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GPG5M2tRFN9d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6675e9ce-aec7-4347-cf24-256c7f7e33ce"
      },
      "source": [
        "INFER_ONLY = True # if you only want to infer using provided weights, set True\n",
        "if INFER_ONLY:\n",
        "    !pip install gdown\n",
        "    !gdown --id 1VjkQD8tdNTwS0LXif9VkacpUEiuoRGB_ ## transpose weight H\n",
        "\n",
        "'''\n",
        "please make sure default directory is : /content/\n",
        "'''\n",
        "TRAINED_MODEL_PATH = '/content/transpose_H.pth'\n",
        "\n",
        "if not INFER_ONLY:\n",
        "    !gdown --id 16XAo2mwUQrKo_D_-f3aYnxg5qioj6DUj\n",
        "PSEUDO_DF_PATH = '/content/pseudo.csv' # path to pseudo data generated by evopose model\n",
        "\n",
        "\n",
        "########### CHANGE THIS ACCORDINGLY #######################\n",
        "vip_data_dir = \"/content/vipcup2021-dataset\"    # Make sure train, valid, test1 folders are immediately inside this directory"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=16XAo2mwUQrKo_D_-f3aYnxg5qioj6DUj\n",
            "To: /content/pseudo.csv\n",
            "100% 472k/472k [00:00<00:00, 3.16MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kThlwgjYFgrT"
      },
      "source": [
        "# Get Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hqoovBJH89LM",
        "outputId": "29a5d3cf-d968-42ea-fff3-bf925a22a976"
      },
      "source": [
        "%%writefile generate_cover.py\n",
        "\n",
        "import argparse\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2,os\n",
        "from glob import glob\n",
        "import scipy.io as scio\n",
        "import scipy\n",
        "import shutil\n",
        "from matplotlib.colors import rgb2hex\n",
        "from skimage.morphology import reconstruction\n",
        "from skimage.morphology import disk\n",
        "from skimage.filters.rank import gradient\n",
        "import skimage\n",
        "from tqdm import tqdm\n",
        "os.system('pip install git+https://github.com/albumentations-team/albumentations')\n",
        "import albumentations as A\n",
        "\n",
        "kplines = [(0, 1), (1, 2), (12, 2), (12, 3), (3, 4), (4, 5), (6, 7),\n",
        "            (7, 8), (8, 12), (12, 9), (9, 10), (10, 11), (12, 13)]\n",
        "name2idx = {\n",
        "    \"Right ankle\":0,\n",
        "    \"Right knee\":1,\n",
        "    \"Right hip\":2,\n",
        "    \"Left hip\":3,\n",
        "    \"Left knee\":4,\n",
        "    \"Left ankle\":5,\n",
        "    \"Right wrist\":6,\n",
        "    \"Right elbow\":7,\n",
        "    \"Right shoulder\":8,\n",
        "    \"Left shoulder\":9,\n",
        "    \"Left elbow\":10,\n",
        "    \"Left wrist\":11,\n",
        "    \"Thorax\":12,\n",
        "    \"Head top\":13, \n",
        "}\n",
        "idx2name = {v:k for k,v in name2idx.items()}\n",
        "\n",
        "def load_kps(kp_path):\n",
        "    gt  = scio.loadmat(kp_path)['joints_gt']\n",
        "    kps = gt[:2].transpose(2, 1, 0) # => (num_image, num_limb, 2) => (None, 14, 2)\n",
        "    return kps.astype(int)\n",
        "\n",
        "def draw_kp(img, kps, kplines, line_th=1, circle_th=2, fontScale=1, text_th =2, text=False):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        img      : image (R, G, B)\n",
        "        kps      : keypoints (num_points, 2)\n",
        "        kplines  : limb line tuple index\n",
        "        text     : show text or not\n",
        "    Returns:\n",
        "        drew image\n",
        "    \"\"\"\n",
        "    cmap   = plt.get_cmap('rainbow')\n",
        "    colors = [cmap(i) for i in np.linspace(0, 1, len(kps) + 2)]\n",
        "    colors = [(c[2] * 255, c[1] * 255, c[0] * 255) for c in colors]\n",
        "    for idx, kpline in enumerate(kplines):\n",
        "        img = cv2.line(img.astype(float), tuple(kps[kpline[0]]), tuple(kps[kpline[1]]), thickness=line_th,\n",
        "                       color=colors[idx], lineType=cv2.LINE_AA)\n",
        "    for idx in range(len(kps)):\n",
        "        color = colors[idx]\n",
        "        img = cv2.circle(img.astype(float),tuple(kps[idx]), circle_th, color , cv2.FILLED)\n",
        "        if text:\n",
        "            w = img.shape[1]\n",
        "            px = kps[idx][0]\n",
        "            py = kps[idx][1]\n",
        "            if px>w//2:\n",
        "                px+=10\n",
        "                color = (255,0,0)\n",
        "            else:\n",
        "                px-=30\n",
        "                color = (0,0,255)\n",
        "            img = cv2.putText(img, str(idx), (px, py), cv2.FONT_HERSHEY_SIMPLEX, \n",
        "                               fontScale=fontScale, color=color, thickness=text_th, lineType=cv2.LINE_AA)\n",
        "    return img.astype('uint8')\n",
        "\n",
        "\n",
        "            \n",
        "def apply_selective_thorax(main_img,transformed_image,point):\n",
        "    extra=int(point)\n",
        "    new_img=main_img.copy()\n",
        "    new_img[extra:,:]=transformed_image[extra:,:]\n",
        "    return new_img\n",
        "\n",
        "def pad_across_width(image):\n",
        "    shapes=image.shape\n",
        "    h=shapes[0]\n",
        "    w=shapes[1]\n",
        "    if len(shapes)>2:\n",
        "        c=shapes[-1]\n",
        "\n",
        "    diff=h-w\n",
        "    side1=int(diff/2)\n",
        "    side2=diff-side1\n",
        "\n",
        "    s1=np.zeros((h,side1)).astype(image.dtype) if len(shapes)==2 else np.zeros((h,side1,c)).astype(image.dtype) \n",
        "    s2=np.zeros((h,side2)).astype(image.dtype) if len(shapes)==2 else np.zeros((h,side2,c)).astype(image.dtype) \n",
        "\n",
        "    new_image=image.copy()\n",
        "    new_image=np.concatenate([s1,new_image,s2],axis=1)\n",
        "    return new_image\n",
        "tpoint=name2idx[\"Thorax\"]\n",
        "\n",
        "def cover_gen(points,image,return_coord=False):\n",
        "    cover=np.zeros(image.shape,dtype=image.dtype)\n",
        "    \n",
        "    cover_corner_right=points[:,0].max()+20\n",
        "    cover_corner_left=points[:,0].min()-20\n",
        "    low=150\n",
        "    hthorax=points[tpoint][1]\n",
        "\n",
        "    if len(image.shape)>2:\n",
        "        cover[hthorax:low,cover_corner_left:cover_corner_right,0]=1# if image.dtype=='uint8' else 255.0\n",
        "        cover=cover[:,:,0]\n",
        "    else:\n",
        "        cover[hthorax:low,cover_corner_left:cover_corner_right]=1 #if image.dtype=='uint8' else 255.0\n",
        "    \n",
        "    if return_coord:\n",
        "        return ((cover_corner_left,hthorax),(cover_corner_right,low)),cover\n",
        "    return cover\n",
        "\n",
        "def combine(image,path1,path2):\n",
        "    albu=A.Compose([\n",
        "                   A.augmentations.domain_adaptation.HistogramMatching(path2,blend_ratio=(0.5, 0.9),p=1),\n",
        "    A.augmentations.domain_adaptation.FDA(path1,p=1,beta_limit=0.05)])\n",
        "    albu_im= albu(image=image)['image']\n",
        "    return albu_im\n",
        "\n",
        "def combiner(image1,image2,cover,final_channel=1):\n",
        "    if len(cover.shape)==2:\n",
        "        cover=np.expand_dims(cover,axis=-1)\n",
        "    if len(image2.shape)==2:\n",
        "        image2=np.expand_dims(image2,axis=-1)\n",
        "    \n",
        "    if len(image1.shape)>2&image1.shape[-1]!=1:\n",
        "        image1=np.expand_dims(image1[:,:,0],axis=-1)\n",
        "    \n",
        "    total=(1-cover)*image1+cover*image2\n",
        "    if final_channel==3:\n",
        "        return np.stack([total,total,total])\n",
        "    return total\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('--data-dir', type=str, default='/kaggle/input/ieee-vip-cup-2021-train-val-dataset/', help='main directory of data')\n",
        "    parser.add_argument('--save-dir', type=str, default='/kaggle/working/VIP',help=\"where to save files, a new directory\")\n",
        "    opt = parser.parse_args()\n",
        "    \n",
        "    save_at=opt.save_dir#'/kaggle/working'\n",
        "    global_path=opt.data_dir\n",
        "    \n",
        "#     if os.path.exists(save_at):\n",
        "#         save_at=os.path.join(save_at,'VIP')\n",
        "        \n",
        "    print('Copying ...')\n",
        "    try:\n",
        "        shutil.copytree(global_path,save_at)\n",
        "    except:\n",
        "        shutil.rmtree(save_at)\n",
        "        shutil.copytree(global_path,save_at)\n",
        "    \n",
        "    print('Finding Train statistics...')\n",
        "    \n",
        "    uncover=[]\n",
        "    cover1=[]\n",
        "    cover2=[]\n",
        "    files_dir=os.path.join(global_path,'train')\n",
        "    if len(os.listdir(files_dir))<4:\n",
        "        files_dir=os.path.join(files_dir,'train')\n",
        "    files=os.listdir(files_dir)\n",
        "    uncover_images_list=[]\n",
        "    for f in files:\n",
        "        current_dir=os.path.join(files_dir,f,'IR')\n",
        "        if 'uncover' in os.listdir(current_dir):\n",
        "            uncover.append(f)\n",
        "            uncover_images_list.extend(glob(os.path.join(current_dir,'uncover/*')))\n",
        "\n",
        "        if 'cover1' in os.listdir(current_dir):\n",
        "            cover1.append(f)\n",
        "        if 'cover2' in os.listdir(current_dir):\n",
        "            cover2.append(f)\n",
        "\n",
        "    print(f'Uncover dirs : {len(uncover)}, Total files: {len(uncover_images_list)}')\n",
        "    print(f'Cover1 dirs : {len(cover1)}')\n",
        "    print(f'Cover2 dirs : {len(cover2)}')\n",
        "    \n",
        "    cover2_path=['image_000027.png',\n",
        "     'image_000041.png',\n",
        "     'image_000038.png',\n",
        "     'image_000023.png']\n",
        "    \n",
        "    cover2_path=[os.path.join(files_dir,'00062/IR/cover2',i) for i in cover2_path]\n",
        "    print('Selected cover images are : ',cover2_path)\n",
        "    \n",
        "    copy_dir=os.path.join(save_at,'train')\n",
        "    if len(os.listdir(copy_dir))<5:\n",
        "        copy_dir=os.path.join(copy_dir,'train')\n",
        "        \n",
        "    print('Generating cover images from uncover')   \n",
        "    for file_num in tqdm(sorted(uncover)):\n",
        "        image_dir=sorted(glob(os.path.join(copy_dir,file_num,'IR','uncover','*')))\n",
        "        keypoints_path=os.path.join(copy_dir,file_num,'joints_gt_IR.mat')\n",
        "\n",
        "        kp=load_kps(keypoints_path)\n",
        "        for i in range(len(image_dir)):\n",
        "            image_path=image_dir[i]\n",
        "\n",
        "            image=cv2.imread(image_path)\n",
        "            point=kp[i-1]\n",
        "            ((x1,y1),(x2,y2)),cover=cover_gen(point,image[:,:,0],return_coord=True)\n",
        "            final_im=combine(image,cover2_path,cover2_path)\n",
        "            final_im=combiner(image,final_im,cover)\n",
        "\n",
        "            cv2.imwrite(image_dir[i],final_im)\n",
        "    print('Completed')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing generate_cover.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EjIvdUh7895D",
        "outputId": "d0dee7d9-06ad-4bbe-a940-f0423341c2b9"
      },
      "source": [
        "vip_transformed_data_dir = \"/content/transformed\"  \n",
        "!python generate_cover.py --data-dir $vip_data_dir --save-dir $vip_transformed_data_dir"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/albumentations-team/albumentations\n",
            "  Cloning https://github.com/albumentations-team/albumentations to /tmp/pip-req-build-uhmtb_y6\n",
            "  Running command git clone -q https://github.com/albumentations-team/albumentations /tmp/pip-req-build-uhmtb_y6\n",
            "Requirement already satisfied: numpy>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from albumentations==1.0.3) (1.19.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from albumentations==1.0.3) (1.4.1)\n",
            "Requirement already satisfied: scikit-image>=0.16.1 in /usr/local/lib/python3.7/dist-packages (from albumentations==1.0.3) (0.16.2)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from albumentations==1.0.3) (3.13)\n",
            "Requirement already satisfied: opencv-python>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from albumentations==1.0.3) (4.1.2.30)\n",
            "Requirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations==1.0.3) (3.2.2)\n",
            "Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations==1.0.3) (2.4.1)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations==1.0.3) (2.5.1)\n",
            "Requirement already satisfied: PyWavelets>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations==1.0.3) (1.1.1)\n",
            "Requirement already satisfied: pillow>=4.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations==1.0.3) (7.1.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations==1.0.3) (2.4.7)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations==1.0.3) (2.8.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations==1.0.3) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations==1.0.3) (0.10.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from cycler>=0.10->matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations==1.0.3) (1.15.0)\n",
            "Requirement already satisfied: decorator<5,>=4.3 in /usr/local/lib/python3.7/dist-packages (from networkx>=2.0->scikit-image>=0.16.1->albumentations==1.0.3) (4.4.2)\n",
            "Building wheels for collected packages: albumentations\n",
            "  Building wheel for albumentations (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for albumentations: filename=albumentations-1.0.3-py3-none-any.whl size=98951 sha256=931b4def1382fc80f4b6baad5e726443c42edc0fe99b9459a0ffbc3c8d961865\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-soujxjcp/wheels/3a/25/ed/ec3b518e7a332d7f0a3bb37c280e1b784cf2f79b94b3c7d00b\n",
            "Successfully built albumentations\n",
            "Installing collected packages: albumentations\n",
            "  Attempting uninstall: albumentations\n",
            "    Found existing installation: albumentations 0.1.12\n",
            "    Uninstalling albumentations-0.1.12:\n",
            "      Successfully uninstalled albumentations-0.1.12\n",
            "Successfully installed albumentations-1.0.3\n",
            "Copying ...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hb89NO9WFnd6"
      },
      "source": [
        "import shutil\n",
        "\n",
        "shutil.copytree(vip_data_dir, '/content/data')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jvp3Z49YFo-p"
      },
      "source": [
        "import shutil, os\n",
        "\n",
        "!mkdir personbbox\n",
        "%cd personbbox\n",
        "!gdown --id 1EVCVl9H0mjJyCrhLqvxaBHPB62-ev9U1 ## mmposeunlabelledpersonbbox\n",
        "mmposeunlabel_zip='/content/personbbox/mmposeunlabelledpersonbbox.zip'\n",
        "!unzip {mmposeunlabel_zip}\n",
        "!rm {mmposeunlabel_zip}\n",
        "shutil.copytree('/content/personbbox/mmposeunlabelledpersonbbox/yolo/yolov5/runs/detect/exp/labels', '/content/labels')\n",
        "%cd ..\n",
        "shutil.rmtree('/content/personbbox') \n",
        "\n",
        "!mkdir personbbox\n",
        "%cd personbbox\n",
        "!gdown --id 1huBWdHHJYvXDa4nIp4o-0h_X6RB7UOH_ ## vip100\n",
        "vip100e_zip='/content/personbbox/vip21personbbox100e.zip'\n",
        "!unzip {vip100e_zip}\n",
        "!rm {vip100e_zip}\n",
        "shutil.copytree('/content/personbbox/vip21personbbox100e/yolo/yolov5/runs/detect/exp/labels', '/content/labels_test')\n",
        "\n",
        "%cd ..\n",
        "shutil.rmtree('/content/personbbox') "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7xy1hE_fGMu0"
      },
      "source": [
        "# Generate Data "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_EhjEC5hGBhc"
      },
      "source": [
        "%cd /content/\n",
        "\n",
        "LABELS = '/content/labels'\n",
        "LABELS_TEST = '/content/labels_test'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w98NyTwQGlls"
      },
      "source": [
        "%%writefile /content/test_coco.py\n",
        "\n",
        "import argparse\n",
        "import numpy as np\n",
        "import cv2 \n",
        "import matplotlib.pyplot as plt\n",
        "import scipy.io as scio\n",
        "import pandas as pd\n",
        "from tqdm.notebook import tqdm\n",
        "import os, shutil\n",
        "from glob import glob\n",
        "tqdm.pandas()\n",
        "import json\n",
        "import datetime\n",
        "import imagesize\n",
        "from sklearn.model_selection import GroupKFold \n",
        "import scipy.io as scio\n",
        "import cv2\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "def load_kps(kp_path, width, height, new_width, new_height):\n",
        "    gt  = scio.loadmat(kp_path)['joints_gt'] # label = if_ocluded\n",
        "    kps = gt.transpose(2, 1, 0).astype(np.float64) # => (num_image, num_limb, 3) or (None, 14, 3)\n",
        "    kps[..., 0] = (kps[...,0]-1)/width*new_width    # converting one indexing to zero indexing\n",
        "    kps[..., 1] = (kps[...,1]-1)/height*new_height\n",
        "    kps[..., 2] = 2- kps[...,2] # coco format\n",
        "    return kps.astype(np.int32)\n",
        "\n",
        "def load_image(image_path):\n",
        "    return cv2.imread(image_path)[...,::-1]\n",
        "\n",
        "\n",
        "def read_resize(file_path, dim=128, width=128, height=128, aspect_ratio=True):\n",
        "    img = load_image(file_path)\n",
        "    h, w = img.shape[:2]  # orig hw\n",
        "    if aspect_ratio:\n",
        "        r = dim / max(h, w)  # resize image to img_size\n",
        "        interp = cv2.INTER_AREA if r < 1 else cv2.INTER_LINEAR\n",
        "        if r != 1:  # always resize down, only resize up if training with augmentation\n",
        "            img = cv2.resize(img, (int(w * r), int(h * r)), interpolation=interp)\n",
        "            new_h, new_w = img.shape[:2]\n",
        "    else:\n",
        "        img = cv2.resize(img, (width,height), cv2.INTER_AREA)\n",
        "        new_w = dim; new_h = dim\n",
        "        \n",
        "    return img, w, h\n",
        "\n",
        "\n",
        "def get_image_info(file_name, height, width, id,\n",
        "                   license=1, date_captured='', \n",
        "                   coco_url='', flickr_url='',):\n",
        "    return dict(license=license, \n",
        "                file_name=file_name,\n",
        "                coco_url=coco_url,\n",
        "                height=height,\n",
        "                width=width, \n",
        "                date_captured=date_captured,\n",
        "                flickr_url=flickr_url,\n",
        "                id=id)\n",
        "    \n",
        "\n",
        "\n",
        "def get_annot_info(kps, id, image_id, category_id=1, bbox=None, area=None,\n",
        "                   iscrowd=0, segmentation=None):\n",
        "    \"\"\"\n",
        "    Parameters:\n",
        "    --------------- \n",
        "    Args:\n",
        "    \n",
        "    Returns:\n",
        "    \n",
        "    \"\"\"\n",
        "    return {\n",
        "        \"segmentation\": segmentation or [],\n",
        "        \"num_keypoints\": len(kps)//3,\n",
        "        \"area\": area if area else (bbox[2]*bbox[3]),\n",
        "        \"iscrowd\": iscrowd,\n",
        "        \"keypoints\": kps,\n",
        "        \"image_id\": image_id,\n",
        "        \"bbox\": bbox or [0, 0, 0, 0],\n",
        "        \"category_id\": category_id,\n",
        "        \"id\": id,\n",
        "    }\n",
        "\n",
        "\n",
        "def get_bbox_info(id, image_id, category_id=1, bbox=None, area=None,\n",
        "                   iscrowd=0, segmentation=None):\n",
        "    \"\"\"\n",
        "    Parameters:\n",
        "    --------------- \n",
        "    Args:\n",
        "    \n",
        "    Returns:\n",
        "    \n",
        "    \"\"\"\n",
        "    return {\n",
        "        \"segmentation\": segmentation or [],\n",
        "        \"num_keypoints\": 14,\n",
        "        \"area\": area if area else (bbox[2]*bbox[3]),\n",
        "        \"iscrowd\": iscrowd,\n",
        "        \"keypoints\": [0,0,2]*14,\n",
        "        \"image_id\": image_id,\n",
        "        \"bbox\": bbox or [0, 0, 0, 0],\n",
        "        \"category_id\": category_id,\n",
        "        \"id\": id,\n",
        "    }\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('--dim', type=int, default=128, help='resized image shape')\n",
        "    parser.add_argument('--fold', type=int, default=0, help='fold number')\n",
        "    parser.add_argument('--imgid', type=int, default=1, help='starting imgid number for coco')\n",
        "    parser.add_argument('--is_annot', action='store_true', help=\"is there annotaions to use\")\n",
        "    parser.add_argument('--is_test', action='store_true', help=\"testing\")\n",
        "    parser.add_argument('--vip_folder', type=str, default=\"train\", help=\"VIP CUP DATA FOLDER\")\n",
        "    parser.add_argument(\"--coco_folder\", type=str, default=\"train\", help=\"folder used in coco dataset\")\n",
        "    parser.add_argument(\"--bbox_label_test\", type=str, default=\"/content/labels\", help=\"folder containing yolo labels of test person bbox\")\n",
        "    parser.add_argument(\"--base_dir\", type=str, default=\"/content/data\", help=\"base dir for vip dataset folder\")\n",
        "    parser.add_argument(\"--label\", type=str, default=\"uncover\" , help=\"uncover, cover1, cover2\")\n",
        "    parser.add_argument(\"--label2\", type=str, default=\"null\" , help=\"cover1, cover2\")\n",
        "    parser.add_argument(\"--out_dir\", type=str, default=\"/content\" , help=\"output directory\")\n",
        "    parser.add_argument('--is_aspect_ratio', action='store_true', help=\"mainatain aspect ratio. Only use dim. don't use width and height\")\n",
        "    parser.add_argument('--width', type=int, default=128, help='fold number')\n",
        "    parser.add_argument('--height', type=int, default=128, help='fold number')\n",
        "\n",
        "    opt = parser.parse_args()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    skeleton = [(0, 1), (1, 2), (12, 2), (12, 3), (3, 4), (4, 5), (6, 7),\n",
        "                (7, 8), (8, 12), (12, 9), (9, 10), (10, 11), (12, 13)]\n",
        "    skeleton = [[x[0]+1, x[1]+1] for x in skeleton]\n",
        "    name2idx = {\n",
        "        \"Right ankle\":0,\n",
        "        \"Right knee\":1,\n",
        "        \"Right hip\":2,\n",
        "        \"Left hip\":3,\n",
        "        \"Left knee\":4,\n",
        "        \"Left ankle\":5,\n",
        "        \"Right wrist\":6,\n",
        "        \"Right elbow\":7,\n",
        "        \"Right shoulder\":8,\n",
        "        \"Left shoulder\":9,\n",
        "        \"Left elbow\":10,\n",
        "        \"Left wrist\":11,\n",
        "        \"thorax\":12,\n",
        "        \"head top\":13, \n",
        "    }\n",
        "    idx2name = {v:k for k,v in name2idx.items()}\n",
        "    names = list(idx2name.values())\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def get_info(filepath):\n",
        "        x = filepath.split('/')\n",
        "        image_id = x[-1]\n",
        "        label    = x[-2]\n",
        "        modality = x[-3]\n",
        "        study_id = x[-4]\n",
        "        split    = x[-5]\n",
        "        return [filepath, study_id, image_id, modality, label, split]\n",
        "\n",
        "\n",
        "\n",
        "    filepaths = glob(f'{opt.base_dir}/**/*png', recursive=True)\n",
        "    filepaths.sort()\n",
        "    df = pd.DataFrame(list(map(get_info, filepaths)), columns=['image_path', 'study_id', 'image_id',\n",
        "                                                            'modality', 'label', 'split'])\n",
        "\n",
        "\n",
        "\n",
        "    df['rgb_gt_path']    = df.image_path.map(lambda x: os.path.join(x.rsplit('/', 3)[0], 'joints_gt_RGB.mat'))\n",
        "    df['ir_gt_path']     = df.image_path.map(lambda x: os.path.join(x.rsplit('/', 3)[0], 'joints_gt_IR.mat'))\n",
        "    df['rgb_align_path'] = df.image_path.map(lambda x: os.path.join(x.rsplit('/', 3)[0], 'align_PTr_RGB.npy'))\n",
        "    df['ir_align_path']  = df.image_path.map(lambda x: os.path.join(x.rsplit('/', 3)[0], 'align_PTr_IR.npy'))\n",
        "\n",
        "    df[['width', 'height']] = df.image_path.progress_apply(lambda x: list(imagesize.get(x))).tolist()\n",
        "\n",
        "\n",
        "        \n",
        "    df = df[df.split == opt.vip_folder]\n",
        "    df = df[df.modality == \"IR\"]\n",
        "\n",
        "    if opt.vip_folder == \"train\":\n",
        "        gkf = GroupKFold(n_splits=5)\n",
        "        df['fold'] = -1\n",
        "        df.reset_index(drop=True, inplace=True)\n",
        "        for fold, (train_idx, val_idx) in enumerate(gkf.split(df, groups=df['study_id'])):\n",
        "            df.loc[val_idx, 'fold'] = fold\n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "    FOLD = opt.fold\n",
        "    if opt.vip_folder == \"train\" and opt.coco_folder == \"train\":\n",
        "        train_df = df[(df.fold!=FOLD) & (df.label==opt.label)]\n",
        "    elif opt.vip_folder == \"train\" and opt.coco_folder == \"val\":\n",
        "        train_df = df[(df.fold==FOLD) & (df.label==opt.label)]\n",
        "    else:\n",
        "        if opt.label2 == \"null\":\n",
        "            train_df = df[(df.label==opt.label)]\n",
        "        else:\n",
        "            train_df = df[(df.label==opt.label) | (df.label==opt.label2)]\n",
        "\n",
        "\n",
        "    INFO = {\n",
        "        \"description\": \"VIP CUP 2021 Dataset\",\n",
        "        \"url\": \"https://www.kaggle.com/awsaf49/ieee-vip-cup-2021-train-val-dataset\",\n",
        "        \"version\": \"0.1.0\",\n",
        "        \"year\": 2021,\n",
        "        \"contributor\": \"awsaf\",\n",
        "        \"date_created\": datetime.datetime.utcnow().isoformat(' ')\n",
        "    }\n",
        "\n",
        "    LICENSES = [\n",
        "        {\n",
        "            \"id\": 1,\n",
        "            \"name\": \"Attribution-NonCommercial-ShareAlike License\",\n",
        "            \"url\": \"http://creativecommons.org/licenses/by-nc-sa/2.0/\"\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    CATEGORIES = [\n",
        "        {\n",
        "            'id': 1,\n",
        "            'name': 'person',\n",
        "            'supercategory': 'person',\n",
        "            \"keypoints\": names,\n",
        "            \"skeleton\": skeleton\n",
        "        },\n",
        "    ]\n",
        "\n",
        "    coco_output = {\n",
        "        \"info\": INFO,\n",
        "        \"licenses\": LICENSES,\n",
        "        \"categories\": CATEGORIES,\n",
        "        \"images\": [],\n",
        "        \"annotations\": []\n",
        "    }\n",
        "\n",
        "\n",
        "\n",
        "    image_dir = f'{opt.out_dir}/coco2017/{opt.coco_folder}2017'\n",
        "    annot_dir = f'{opt.out_dir}/coco2017/annotations'\n",
        "    os.makedirs(image_dir, exist_ok=True)\n",
        "    os.makedirs(annot_dir, exist_ok=True)\n",
        "\n",
        "\n",
        "\n",
        "    IMAGES = []\n",
        "    ANNOTATIONS = []\n",
        "    \n",
        "    coco_df = {\n",
        "        'coco_name' : [],\n",
        "        'orig_name' : []\n",
        "    }\n",
        "    \n",
        "    coco_image_id=opt.imgid\n",
        "    coco_annot_id=opt.imgid\n",
        "    for idx in tqdm(range(train_df.shape[0])):\n",
        "        image_path = train_df.image_path.iloc[idx]\n",
        "        image_id   = train_df.image_id.iloc[idx]\n",
        "        study_id   = train_df.study_id.iloc[idx]\n",
        "        image_idx  = int(image_id.split('.')[0].split('_')[-1])-1\n",
        "        if opt.is_aspect_ratio:\n",
        "            image, width, height  = read_resize(image_path, dim=opt.dim)\n",
        "        else:\n",
        "            image, width, height  = read_resize(image_path, width=opt.width, height=opt.height, aspect_ratio=False)\n",
        "        new_height, new_width = image.shape[:2]\n",
        "        orig_file_name = study_id + '_' + image_path.split(\"/\")[3] + \"_\" + image_path.split('/')[-1]\n",
        "\n",
        "        # for transpose\n",
        "        file_name = '%012d.png' % coco_image_id\n",
        "        coco_df['coco_name'].append(file_name)\n",
        "        coco_df['orig_name'].append(orig_file_name)\n",
        "        \n",
        "        new_image_path  = os.path.join(image_dir,file_name)\n",
        "        # writing image\n",
        "        cv2.imwrite(new_image_path, image[...,::-1])\n",
        "        # writing data\n",
        "        IMAGES.append(get_image_info(file_name, \n",
        "                                    height=int(new_height), \n",
        "                                    width=int(new_width), \n",
        "                                    id=coco_image_id,))\n",
        "        \n",
        "        if opt.is_test:\n",
        "            label_file = opt.bbox_label_test + '/' + orig_file_name[:-3] + 'txt'\n",
        "            # SEE THIS \n",
        "            label_file = label_file.replace(opt.vip_folder, 'ieee-vip-cup-2021-train-val-dataset')\n",
        "            with open(label_file, \"r\") as f:\n",
        "                data = np.array(f.read().replace('\\n', ' ').strip().split(' ')).astype(np.float32).reshape(-1, 6)[0]\n",
        "                xc, yc, w, h = data[1], data[2], data[3], data[4]\n",
        "                # using new height, new width --> REMEMBER THIS\n",
        "                xc, yc = xc*new_width, yc*new_height\n",
        "                w, h = w*new_width, h*new_height\n",
        "            \n",
        "            xmin, ymin = xc - (w/2), yc - (h/2)\n",
        "            bbox = [int(xmin), int(ymin), int(w), int(h)]\n",
        "            ANNOTATIONS.append(get_bbox_info(id=coco_annot_id, image_id=coco_image_id, category_id=1,\n",
        "                                            bbox=bbox, \n",
        "                                            area=w*h,\n",
        "                                            iscrowd=0,\n",
        "                                            segmentation=None))\n",
        "            # print(ANNOTATIONS)\n",
        "            coco_annot_id+=1\n",
        "            \n",
        "\n",
        "        if opt.is_annot:\n",
        "            kp_path = train_df.ir_gt_path.iloc[idx]\n",
        "            kps = load_kps(kp_path, \n",
        "                        width, height,\n",
        "                        new_width, new_height)\n",
        "            # kp of a image\n",
        "            kps_img = kps[image_idx]\n",
        "            # bbox from keypoints\n",
        "            xmin, ymin, xmax, ymax = np.min(kps_img[...,0]), np.min(kps_img[...,1]), np.max(kps_img[...,0]), np.max(kps_img[...,1])\n",
        "            offsetMin = int(15 * np.square((new_height*new_width) / (512*384)))\n",
        "            offsetMax = int(35 * np.square((new_height*new_width) / (512*384)))\n",
        "            xmin, ymin = int(xmin-offsetMin), int(ymin-offsetMax) # kp are too close to body so taking offset\n",
        "            xmin = max(0, xmin)\n",
        "            ymin = max(0, ymin)\n",
        "            w,h = int(xmax-xmin+offsetMin), int(ymax-ymin+offsetMax)\n",
        "            if opt.is_aspect_ratio:\n",
        "                w = min(w, opt.dim)\n",
        "                h = min(h, opt.dim)\n",
        "            else:\n",
        "                w = min(w, opt.width)\n",
        "                h = min(h, opt.height)\n",
        "            bbox = [xmin, ymin, w, h]\n",
        "\n",
        "            #============================\n",
        "            kps_img = [int(x) for x in kps_img.reshape(-1).tolist()]\n",
        "            \n",
        "            \n",
        "            \n",
        "            ANNOTATIONS.append(get_annot_info(kps=kps_img, id=coco_annot_id, image_id=coco_image_id, category_id=1,\n",
        "                                            bbox=bbox, \n",
        "                                            area=w*h,\n",
        "                                            iscrowd=0,\n",
        "                                            segmentation=None))\n",
        "            \n",
        "            coco_annot_id+=1\n",
        "        coco_image_id+=1\n",
        "        \n",
        "    #===========================\n",
        "    coco_output[\"images\"]      = IMAGES\n",
        "    coco_output[\"annotations\"] = ANNOTATIONS\n",
        "\n",
        "    # json file\n",
        "    with open(f'{annot_dir}/person_keypoints_{opt.coco_folder}2017.json', 'w') as output_json_file:\n",
        "        json.dump(coco_output, output_json_file)   \n",
        "\n",
        "    coco_df = pd.DataFrame(coco_df)\n",
        "    coco_df.to_csv(f'{opt.coco_folder}.csv',index=False)\n",
        "    print(f\"Total {len(os.listdir(image_dir))} images found\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nUvYN-rqIYxg"
      },
      "source": [
        "# put unlabelled train data into test\n",
        "if not INFER_ONLY:\n",
        "    !python /content/test_coco.py --width 384 \\\n",
        "    --height 512 \\\n",
        "    --vip_folder \"train\" \\\n",
        "    --coco_folder \"test\" \\\n",
        "    --label \"cover1\" \\\n",
        "    --label2 \"cover2\" \\\n",
        "    --fold 0 \\\n",
        "    --base_dir \"/content/data\" \\\n",
        "    --out_dir \"/content\" \\\n",
        "    --is_test \\\n",
        "    --imgid 1351 \\\n",
        "    --bbox_label_test $LABELS"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6alVawhvIpZi"
      },
      "source": [
        "# transformed train data \n",
        "if not INFER_ONLY:\n",
        "    !python /content/test_coco.py --fold -1 \\\n",
        "    --is_annot \\\n",
        "    --vip_folder \"train\" \\\n",
        "    --coco_folder \"train\" \\\n",
        "    --base_dir \"/content/transformed\" \\\n",
        "    --label \"uncover\" \\\n",
        "    --out_dir \"/content\" \\\n",
        "    --width 384 \\\n",
        "    --height 512"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Lod6pszJKLV"
      },
      "source": [
        "# valid data\n",
        "if not INFER_ONLY:\n",
        "    !python /content/test_coco.py --width 384 \\\n",
        "    --height 512 \\\n",
        "    --is_annot \\\n",
        "    --vip_folder \"valid\" \\\n",
        "    --coco_folder \"val\" \\\n",
        "    --label \"cover1\" \\\n",
        "    --label2 \"cover2\" \\\n",
        "    --fold 0 \\\n",
        "    --base_dir \"/content/data\" \\\n",
        "    --out_dir \"/content\" "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cbsb0aphJQPc"
      },
      "source": [
        "import pandas as pd\n",
        "if not INFER_ONLY:\n",
        "    # sanity check\n",
        "    test_df = pd.read_csv('/content/test.csv')\n",
        "    test_df[test_df.coco_name == '000000001353.png']['orig_name'].iloc[0]  #.split('_')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rE9_k-Y8JUPn"
      },
      "source": [
        "# get pseudo labels and merge train data\n",
        "\n",
        "if not INFER_ONLY:\n",
        "    import json\n",
        "    import os\n",
        "    import numpy as np\n",
        "    import cv2\n",
        "    import datetime\n",
        "    import pandas as pd\n",
        "    import ast\n",
        "    from tqdm.notebook import tqdm\n",
        "\n",
        "\n",
        "    width = 384\n",
        "    height = 512\n",
        "\n",
        "\n",
        "    def get_image_info(file_name, height, width, id,\n",
        "                    license=1, date_captured='', \n",
        "                    coco_url='', flickr_url='',):\n",
        "        return dict(license=license, \n",
        "                    file_name=file_name,\n",
        "                    coco_url=coco_url,\n",
        "                    height=height,\n",
        "                    width=width, \n",
        "                    date_captured=date_captured,\n",
        "                    flickr_url=flickr_url,\n",
        "                    id=id)\n",
        "        \n",
        "\n",
        "    def get_annot_info(kps, id, image_id, category_id=1, bbox=None, area=None,\n",
        "                    iscrowd=0, segmentation=None):\n",
        "        return {\n",
        "            \"segmentation\": segmentation or [],\n",
        "            \"num_keypoints\": len(kps)//3,\n",
        "            \"area\": area if area else (bbox[2]*bbox[3]),\n",
        "            \"iscrowd\": iscrowd,\n",
        "            \"keypoints\": kps,\n",
        "            \"image_id\": image_id,\n",
        "            \"bbox\": bbox or [0, 0, 0, 0],\n",
        "            \"category_id\": category_id,\n",
        "            \"id\": id,\n",
        "        }\n",
        "\n",
        "    IMAGES = []\n",
        "    ANNOTATIONS = []\n",
        "    with open(\"/content/coco2017/annotations/person_keypoints_train2017.json\") as f: \n",
        "        annots = json.load(f)\n",
        "\n",
        "    folder = \"/content/coco2017/test2017\"\n",
        "\n",
        "    result_df = pd.read_csv(PSEUDO_DF_PATH)\n",
        "\n",
        "    IMAGES = annots['images']\n",
        "    ANNOTATIONS = annots['annotations']\n",
        "\n",
        "    coco_image_id = len(IMAGES) + 1\n",
        "    coco_annot_id = len(ANNOTATIONS) + 1\n",
        "    # print(coco_image_id, coco_annot_id)\n",
        "\n",
        "    image_paths = os.listdir(folder)\n",
        "    image_paths.sort()\n",
        "\n",
        "    sz = len(image_paths)\n",
        "    for idx in tqdm(range(sz), total=sz):\n",
        "        file_name = image_paths[idx]\n",
        "        image_path = \"/content/coco2017/test2017/\" + file_name\n",
        "        new_image_path = \"/content/coco2017/train2017/\" + file_name\n",
        "        image = cv2.imread(image_path)[...,::-1]\n",
        "        image = cv2.resize(image, (width,height), cv2.INTER_AREA)\n",
        "\n",
        "\n",
        "        IMAGES.append(get_image_info(file_name, \n",
        "                        height=int(height), \n",
        "                        width=int(width), \n",
        "                        id=coco_image_id,))\n",
        "        \n",
        "        \n",
        "        # preds = np.array(preds).astype(np.int32)\n",
        "        # preds = preds[:,:-1]\n",
        "        # preds = preds.tolist()\n",
        "        # kps = []\n",
        "        # for pred in preds:\n",
        "        #     kps += (pred + [2])\n",
        "        \n",
        "        \n",
        "        test_df = pd.read_csv('/content/test.csv')\n",
        "    #     print(file_name)\n",
        "        file_orig_name = test_df[test_df.coco_name == file_name]['orig_name'].iloc[0]\n",
        "        df_search_name = file_orig_name.split('_')\n",
        "        df_search_name = df_search_name[0] + '_' + df_search_name[2] + '_' + df_search_name[3]\n",
        "    #     df_search_name = file_name.replace(\"train_\", \"\")\n",
        "    #     print(df_search_name)\n",
        "        kps = ast.literal_eval(result_df[result_df.filename == df_search_name].kps.values[0])\n",
        "        kps = np.array(kps)\n",
        "        kps[:, 0] = kps[:, 0] * (width/120)\n",
        "        kps[:, 1] = kps[:, 1] * (height/160)\n",
        "        kps = kps.astype(np.int32)\n",
        "        kps = kps.tolist()\n",
        "        kpts = []\n",
        "        for kp in kps:\n",
        "            kpts += kp\n",
        "\n",
        "        label_file = LABELS + '/' + file_orig_name[:-3] + 'txt'\n",
        "        # SEE THIS \n",
        "        label_file = label_file.replace('train', 'ieee-vip-cup-2021-train-val-dataset')\n",
        "\n",
        "        with open(label_file, \"r\") as f:\n",
        "    #         print(data)\n",
        "            data = np.array(f.read().replace('\\n', ' ').strip().split(' ')).astype(np.float32).reshape(-1, 6)[0]\n",
        "            xc, yc, w, h = data[1], data[2], data[3], data[4]\n",
        "            # using new height, new width --> REMEMBER THIS\n",
        "            xc, yc = xc*width, yc*height\n",
        "            w, h = w*width, h*height\n",
        "        xmin, ymin = xc - (w/2), yc - (h/2)\n",
        "        bbox = [int(xmin), int(ymin), int(w), int(h)]\n",
        "\n",
        "        ANNOTATIONS.append(get_annot_info(kps=kpts, id=coco_annot_id, image_id=coco_image_id, category_id=1,\n",
        "                                    bbox=bbox, \n",
        "                                    area=int(w*h),\n",
        "                                    iscrowd=0,\n",
        "                                    segmentation=None))\n",
        "        \n",
        "        cv2.imwrite(new_image_path, image[...,::-1])\n",
        "        coco_image_id += 1\n",
        "        coco_annot_id+=1\n",
        "\n",
        "\n",
        "    skeleton = [(0, 1), (1, 2), (12, 2), (12, 3), (3, 4), (4, 5), (6, 7),\n",
        "                (7, 8), (8, 12), (12, 9), (9, 10), (10, 11), (12, 13)]\n",
        "    skeleton = [[x[0]+1, x[1]+1] for x in skeleton]\n",
        "    name2idx = {\n",
        "        \"Right ankle\":0,\n",
        "        \"Right knee\":1,\n",
        "        \"Right hip\":2,\n",
        "        \"Left hip\":3,\n",
        "        \"Left knee\":4,\n",
        "        \"Left ankle\":5,\n",
        "        \"Right wrist\":6,\n",
        "        \"Right elbow\":7,\n",
        "        \"Right shoulder\":8,\n",
        "        \"Left shoulder\":9,\n",
        "        \"Left elbow\":10,\n",
        "        \"Left wrist\":11,\n",
        "        \"thorax\":12,\n",
        "        \"head top\":13, \n",
        "    }\n",
        "    idx2name = {v:k for k,v in name2idx.items()}\n",
        "    names = list(idx2name.values())\n",
        "\n",
        "\n",
        "    INFO = {\n",
        "        \"description\": \"VIP CUP 2021 Dataset\",\n",
        "        \"url\": \"https://www.kaggle.com/awsaf49/ieee-vip-cup-2021-train-val-dataset\",\n",
        "        \"version\": \"0.1.0\",\n",
        "        \"year\": 2021,\n",
        "        \"contributor\": \"awsaf\",\n",
        "        \"date_created\": datetime.datetime.utcnow().isoformat(' ')\n",
        "    }\n",
        "\n",
        "    LICENSES = [\n",
        "        {\n",
        "            \"id\": 1,\n",
        "            \"name\": \"Attribution-NonCommercial-ShareAlike License\",\n",
        "            \"url\": \"http://creativecommons.org/licenses/by-nc-sa/2.0/\"\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    CATEGORIES = [\n",
        "        {\n",
        "            'id': 1,\n",
        "            'name': 'person',\n",
        "            'supercategory': 'person',\n",
        "            \"keypoints\": names,\n",
        "            \"skeleton\": skeleton\n",
        "        },\n",
        "    ]\n",
        "\n",
        "    coco_output = {\n",
        "        \"info\": INFO,\n",
        "        \"licenses\": LICENSES,\n",
        "        \"categories\": CATEGORIES,\n",
        "        \"images\": [],\n",
        "        \"annotations\": []\n",
        "    }\n",
        "\n",
        "\n",
        "    coco_output[\"images\"]      = IMAGES\n",
        "    coco_output[\"annotations\"] = ANNOTATIONS\n",
        "\n",
        "    with open(f'/content/coco2017/annotations/person_keypoints_train2017.json', 'w') as output_json_file:\n",
        "        json.dump(coco_output, output_json_file)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qp-vf4TuKT5X"
      },
      "source": [
        "import os, shutil\n",
        "from glob import glob\n",
        "\n",
        "if not INFER_ONLY:\n",
        "    # delete previous test\n",
        "    shutil.rmtree(\"/content/coco2017/test2017\")\n",
        "\n",
        "    print(\"Total training files : \", len(glob('/content/coco2017/train2017/*')))\n",
        "    shutil.move(\"/content/coco2017/val2017\", \"/content/coco2017/images/val2017\")\n",
        "    shutil.move(\"/content/coco2017/train2017\", \"/content/coco2017/images/train2017\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "63PwPuwmK7WC"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nH0pOaxaK_K3"
      },
      "source": [
        "# Installation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wR8jqkAlLAmf"
      },
      "source": [
        "!gdown --id 1mtJdjIo-CXJY9pDDBxWSHoXY5v9Pu8pd ## mmpose code\n",
        "mmpose_zip='/content/MMPose.zip'\n",
        "!unzip {mmpose_zip}\n",
        "!rm {mmpose_zip}\n",
        "\n",
        "shutil.copytree('MMPose/transpose', 'transpose')\n",
        "shutil.rmtree('MMPose')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BBOT_YqtLEuv"
      },
      "source": [
        "%cd transpose\n",
        "!pip install -r requirements.txt\n",
        "\n",
        "POSE_ROOT = '/content/transpose'\n",
        "%mkdir output log\n",
        "\n",
        "%mkdir models\n",
        "%cd models\n",
        "%mkdir pytorch\n",
        "%cd pytorch\n",
        "\n",
        "%mkdir transpose_coco\n",
        "%cd transpose_coco\n",
        "!wget https://github.com/yangsenius/TransPose/releases/download/Hub/tp_h_48_256x192_enc4_d96_h192_mh1.pth\n",
        "%cd /content/transpose"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jBDGDPuZLanG"
      },
      "source": [
        "%cd lib\n",
        "!make #need gpu"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BNtzPQbRLeZX"
      },
      "source": [
        "if not INFER_ONLY:\n",
        "    shutil.copytree('/content/coco2017', '/content/transpose/data/coco')\n",
        "    # sanity check\n",
        "    print(len(glob('/content/transpose/data/coco/images/train2017/*')))\n",
        "\n",
        "    import json \n",
        "    with open('/content/transpose/data/coco/annotations/person_keypoints_train2017.json') as f:\n",
        "        result = json.load(f)\n",
        "    len(result['annotations'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tg2JR0jZLt8X"
      },
      "source": [
        "# Config"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kalBkiTWLuuZ"
      },
      "source": [
        "%%writefile /content/config.yaml\n",
        "\n",
        "AUTO_RESUME: true\n",
        "CUDNN:\n",
        "  BENCHMARK: true\n",
        "  DETERMINISTIC: false\n",
        "  ENABLED: true\n",
        "DATA_DIR: ''\n",
        "GPUS: (0,)\n",
        "OUTPUT_DIR: 'output'\n",
        "LOG_DIR: 'log'\n",
        "WORKERS: 32\n",
        "PRINT_FREQ: 100\n",
        "\n",
        "DATASET:\n",
        "  COLOR_RGB: true\n",
        "  DATASET: 'coco'\n",
        "  DATA_FORMAT: jpg\n",
        "  FLIP: true\n",
        "  NUM_JOINTS_HALF_BODY: 7\n",
        "  PROB_HALF_BODY: 0.3\n",
        "  ROOT: 'data/coco/'\n",
        "  ROT_FACTOR: 45\n",
        "  SCALE_FACTOR: 0.35\n",
        "  TEST_SET: 'val2017'\n",
        "  TRAIN_SET: 'train2017'\n",
        "MODEL:\n",
        "  # Transformer Encoder\n",
        "  DIM_MODEL: 96\n",
        "  DIM_FEEDFORWARD: 192\n",
        "  N_HEAD: 1\n",
        "  ENCODER_LAYERS: 4\n",
        "  ATTENTION_ACTIVATION: relu\n",
        "  POS_EMBEDDING: sine\n",
        "  # #\n",
        "  INIT_WEIGHTS: true\n",
        "  NAME: transpose_h\n",
        "  NUM_JOINTS: 14\n",
        "  PRETRAINED: 'models/pytorch/transpose_coco/tp_h_48_256x192_enc4_d96_h192_mh1.pth'\n",
        "  TARGET_TYPE: gaussian\n",
        "  IMAGE_SIZE:\n",
        "  - 192\n",
        "  - 256\n",
        "  HEATMAP_SIZE:\n",
        "  - 48\n",
        "  - 64\n",
        "  SIGMA: 2\n",
        "  EXTRA:\n",
        "    PRETRAINED_LAYERS:\n",
        "    - 'conv1'\n",
        "    - 'bn1'\n",
        "    - 'conv2'\n",
        "    - 'bn2'\n",
        "    - 'layer1'\n",
        "    - 'transition1'\n",
        "    - 'stage2'\n",
        "    - 'transition2'\n",
        "    - 'stage3'\n",
        "    FINAL_CONV_KERNEL: 1\n",
        "    STAGE2:\n",
        "      NUM_MODULES: 1\n",
        "      NUM_BRANCHES: 2\n",
        "      BLOCK: BASIC\n",
        "      NUM_BLOCKS:\n",
        "      - 4\n",
        "      - 4\n",
        "      NUM_CHANNELS:\n",
        "      - 48\n",
        "      - 96\n",
        "      FUSE_METHOD: SUM\n",
        "    STAGE3:\n",
        "      NUM_MODULES: 4\n",
        "      NUM_BRANCHES: 3\n",
        "      BLOCK: BASIC\n",
        "      NUM_BLOCKS:\n",
        "      - 4\n",
        "      - 4\n",
        "      - 4\n",
        "      NUM_CHANNELS:\n",
        "      - 48\n",
        "      - 96\n",
        "      - 192\n",
        "      FUSE_METHOD: SUM\n",
        "LOSS:\n",
        "  USE_TARGET_WEIGHT: true\n",
        "TRAIN:\n",
        "  BATCH_SIZE_PER_GPU: 12 # ATTENTION : INPUT HIGHEST BATCH SIZE THAT DOESNT CAUSE MEMORY ERROR\n",
        "  SHUFFLE: true\n",
        "  BEGIN_EPOCH: 0\n",
        "  END_EPOCH: 100\n",
        "  OPTIMIZER: adam\n",
        "  LR: 0.00005  # Initial learning rate\n",
        "  LR_END: 0.000001  # Final learning rate\n",
        "  LR_FACTOR: 0.25  # for MultiStepLR\n",
        "  LR_STEP:  # for MultiStepLR\n",
        "  - 100\n",
        "  - 150\n",
        "  - 200\n",
        "  - 220\n",
        "  WD: 0.1\n",
        "  GAMMA1: 0.99\n",
        "  GAMMA2: 0.0\n",
        "  MOMENTUM: 0.9\n",
        "  NESTEROV: false\n",
        "TEST:\n",
        "  BLUR_KERNEL: 11 \n",
        "  BATCH_SIZE_PER_GPU: 24\n",
        "  # 'data/coco/person_detection_results/COCO_test-dev2017_detections_AP_H_609_person.json'\n",
        "  # 'data/coco/person_detection_results/COCO_val2017_detections_AP_H_56_person.json'\n",
        "  COCO_BBOX_FILE: ''\n",
        "  BBOX_THRE: 1.0\n",
        "  IMAGE_THRE: 0.0\n",
        "  IN_VIS_THRE: 0.2\n",
        "  MODEL_FILE: models/pytorch/transpose_coco/tp_h_48_256x192_enc4_d96_h192_mh1.pth\n",
        "  NMS_THRE: 1.0\n",
        "  OKS_THRE: 0.9\n",
        "  USE_GT_BBOX: true\n",
        "  FLIP_TEST: true\n",
        "  POST_PROCESS: true\n",
        "  SHIFT_HEATMAP: true\n",
        "DEBUG:\n",
        "  DEBUG: true\n",
        "  SAVE_BATCH_IMAGES_GT: true\n",
        "  SAVE_BATCH_IMAGES_PRED: true\n",
        "  SAVE_HEATMAPS_GT: true\n",
        "  SAVE_HEATMAPS_PRED: true"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "faJWN2k7L5g6"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iU4ySpTEL7IF"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3RFARZFlL7s2"
      },
      "source": [
        "if not INFER_ONLY:\n",
        "\n",
        "    %cd /content/transpose\n",
        "\n",
        "    !python tools/train.py --cfg /content/config.yaml --augTrain"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UZZyCZaFL_wA"
      },
      "source": [
        "if not INFER_ONLY:\n",
        "    MODEL_PATH = '/content/transpose/output/coco/transpose_h/config/model_best.pth'\n",
        "    shutil.rmtree('/content/transpose/data')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jVn9mdQrNBHL"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VrKMrccYNESE"
      },
      "source": [
        "# INFER"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pFVOSeTFNFuk"
      },
      "source": [
        "if INFER_ONLY:\n",
        "    MODEL_PATH = TRAINED_MODEL_PATH"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H36W3h4QNctt"
      },
      "source": [
        "%%writefile /content/infer_data_coco.py\n",
        "\n",
        "import argparse\n",
        "import numpy as np\n",
        "import cv2 \n",
        "import matplotlib.pyplot as plt\n",
        "import scipy.io as scio\n",
        "import pandas as pd\n",
        "from tqdm.notebook import tqdm\n",
        "import os, shutil\n",
        "from glob import glob\n",
        "tqdm.pandas()\n",
        "import json\n",
        "import datetime\n",
        "import imagesize\n",
        "from sklearn.model_selection import GroupKFold \n",
        "import scipy.io as scio\n",
        "import cv2\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "def load_kps(kp_path, width, height, new_width, new_height):\n",
        "    gt  = scio.loadmat(kp_path)['joints_gt'] # label = if_ocluded\n",
        "    kps = gt.transpose(2, 1, 0).astype(np.float64) # => (num_image, num_limb, 3) or (None, 14, 3)\n",
        "    kps[..., 0] = (kps[...,0]-1)/width*new_width    # converting one indexing to zero indexing\n",
        "    kps[..., 1] = (kps[...,1]-1)/height*new_height\n",
        "    kps[..., 2] = 2- kps[...,2] # coco format\n",
        "    return kps.astype(np.int32)\n",
        "\n",
        "def load_image(image_path):\n",
        "    return cv2.imread(image_path)[...,::-1]\n",
        "\n",
        "\n",
        "def read_resize(file_path, dim=128, width=128, height=128, aspect_ratio=True):\n",
        "    img = load_image(file_path)\n",
        "    h, w = img.shape[:2]  # orig hw\n",
        "    if aspect_ratio:\n",
        "        r = dim / max(h, w)  # resize image to img_size\n",
        "        interp = cv2.INTER_AREA if r < 1 else cv2.INTER_LINEAR\n",
        "        if r != 1:  # always resize down, only resize up if training with augmentation\n",
        "            img = cv2.resize(img, (int(w * r), int(h * r)), interpolation=interp)\n",
        "            new_h, new_w = img.shape[:2]\n",
        "    else:\n",
        "        img = cv2.resize(img, (width,height), cv2.INTER_AREA)\n",
        "        new_w = dim; new_h = dim\n",
        "        \n",
        "    return img, w, h\n",
        "\n",
        "\n",
        "def get_image_info(file_name, height, width, id,\n",
        "                   license=1, date_captured='', \n",
        "                   coco_url='', flickr_url='',):\n",
        "    return dict(license=license, \n",
        "                file_name=file_name,\n",
        "                coco_url=coco_url,\n",
        "                height=height,\n",
        "                width=width, \n",
        "                date_captured=date_captured,\n",
        "                flickr_url=flickr_url,\n",
        "                id=id)\n",
        "    \n",
        "\n",
        "\n",
        "def get_annot_info(kps, id, image_id, category_id=1, bbox=None, area=None,\n",
        "                   iscrowd=0, segmentation=None):\n",
        "    \"\"\"\n",
        "    Parameters:\n",
        "    --------------- \n",
        "    Args:\n",
        "    \n",
        "    Returns:\n",
        "    \n",
        "    \"\"\"\n",
        "    return {\n",
        "        \"segmentation\": segmentation or [],\n",
        "        \"num_keypoints\": len(kps)//3,\n",
        "        \"area\": area if area else (bbox[2]*bbox[3]),\n",
        "        \"iscrowd\": iscrowd,\n",
        "        \"keypoints\": kps,\n",
        "        \"image_id\": image_id,\n",
        "        \"bbox\": bbox or [0, 0, 0, 0],\n",
        "        \"category_id\": category_id,\n",
        "        \"id\": id,\n",
        "    }\n",
        "\n",
        "\n",
        "def get_bbox_info(id, image_id, category_id=1, bbox=None, area=None,\n",
        "                   iscrowd=0, segmentation=None):\n",
        "    \"\"\"\n",
        "    Parameters:\n",
        "    --------------- \n",
        "    Args:\n",
        "    \n",
        "    Returns:\n",
        "    \n",
        "    \"\"\"\n",
        "    return {\n",
        "        \"segmentation\": segmentation or [],\n",
        "        \"num_keypoints\": 14,\n",
        "        \"area\": area if area else (bbox[2]*bbox[3]),\n",
        "        \"iscrowd\": iscrowd,\n",
        "        \"keypoints\": [0,0,2]*14,\n",
        "        \"image_id\": image_id,\n",
        "        \"bbox\": bbox or [0, 0, 0, 0],\n",
        "        \"category_id\": category_id,\n",
        "        \"id\": id,\n",
        "    }\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('--dim', type=int, default=128, help='resized image shape')\n",
        "    parser.add_argument('--fold', type=int, default=0, help='fold number')\n",
        "    parser.add_argument('--is_annot', action='store_true', help=\"is there annotaions to use\")\n",
        "    parser.add_argument('--is_test', action='store_true', help=\"testing\")\n",
        "    parser.add_argument('--vip_folder', type=str, default=\"train\", help=\"VIP CUP DATA FOLDER\")\n",
        "    parser.add_argument(\"--coco_folder\", type=str, default=\"train\", help=\"folder used in coco dataset\")\n",
        "    parser.add_argument(\"--bbox_label_test\", type=str, default=\"/content/labels\", help=\"folder containing yolo labels of test person bbox\")\n",
        "    parser.add_argument(\"--base_dir\", type=str, default=\"/content/data\", help=\"base dir for vip dataset folder\")\n",
        "    parser.add_argument(\"--label\", type=str, default=\"uncover\" , help=\"uncover, cover1, cover2\")\n",
        "    parser.add_argument(\"--label2\", type=str, default=\"null\" , help=\"cover1, cover2\")\n",
        "    parser.add_argument(\"--out_dir\", type=str, default=\"/content\" , help=\"output directory\")\n",
        "    parser.add_argument('--is_aspect_ratio', action='store_true', help=\"mainatain aspect ratio. Only use dim. don't use width and height\")\n",
        "    parser.add_argument('--width', type=int, default=128, help='fold number')\n",
        "    parser.add_argument('--height', type=int, default=128, help='fold number')\n",
        "\n",
        "    opt = parser.parse_args()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    skeleton = [(0, 1), (1, 2), (12, 2), (12, 3), (3, 4), (4, 5), (6, 7),\n",
        "                (7, 8), (8, 12), (12, 9), (9, 10), (10, 11), (12, 13)]\n",
        "    skeleton = [[x[0]+1, x[1]+1] for x in skeleton]\n",
        "    name2idx = {\n",
        "        \"Right ankle\":0,\n",
        "        \"Right knee\":1,\n",
        "        \"Right hip\":2,\n",
        "        \"Left hip\":3,\n",
        "        \"Left knee\":4,\n",
        "        \"Left ankle\":5,\n",
        "        \"Right wrist\":6,\n",
        "        \"Right elbow\":7,\n",
        "        \"Right shoulder\":8,\n",
        "        \"Left shoulder\":9,\n",
        "        \"Left elbow\":10,\n",
        "        \"Left wrist\":11,\n",
        "        \"thorax\":12,\n",
        "        \"head top\":13, \n",
        "    }\n",
        "    idx2name = {v:k for k,v in name2idx.items()}\n",
        "    names = list(idx2name.values())\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def get_info(filepath):\n",
        "        x = filepath.split('/')\n",
        "        image_id = x[-1]\n",
        "        label    = x[-2]\n",
        "        modality = x[-3]\n",
        "        study_id = x[-4]\n",
        "        split    = x[-5]\n",
        "        return [filepath, study_id, image_id, modality, label, split]\n",
        "\n",
        "\n",
        "\n",
        "    filepaths = glob(f'{opt.base_dir}/**/*png', recursive=True)\n",
        "    filepaths.sort()\n",
        "    df = pd.DataFrame(list(map(get_info, filepaths)), columns=['image_path', 'study_id', 'image_id',\n",
        "                                                            'modality', 'label', 'split'])\n",
        "\n",
        "\n",
        "\n",
        "    df['rgb_gt_path']    = df.image_path.map(lambda x: os.path.join(x.rsplit('/', 3)[0], 'joints_gt_RGB.mat'))\n",
        "    df['ir_gt_path']     = df.image_path.map(lambda x: os.path.join(x.rsplit('/', 3)[0], 'joints_gt_IR.mat'))\n",
        "    df['rgb_align_path'] = df.image_path.map(lambda x: os.path.join(x.rsplit('/', 3)[0], 'align_PTr_RGB.npy'))\n",
        "    df['ir_align_path']  = df.image_path.map(lambda x: os.path.join(x.rsplit('/', 3)[0], 'align_PTr_IR.npy'))\n",
        "\n",
        "    df[['width', 'height']] = df.image_path.progress_apply(lambda x: list(imagesize.get(x))).tolist()\n",
        "\n",
        "\n",
        "        \n",
        "    df = df[df.split == opt.vip_folder]\n",
        "    df = df[df.modality == \"IR\"]\n",
        "\n",
        "    if opt.vip_folder == \"train\":\n",
        "        gkf = GroupKFold(n_splits=5)\n",
        "        df['fold'] = -1\n",
        "        df.reset_index(drop=True, inplace=True)\n",
        "        for fold, (train_idx, val_idx) in enumerate(gkf.split(df, groups=df['study_id'])):\n",
        "            df.loc[val_idx, 'fold'] = fold\n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "    FOLD = opt.fold\n",
        "    if opt.vip_folder == \"train\" and opt.coco_folder == \"train\":\n",
        "        train_df = df[(df.fold!=FOLD) & (df.label==opt.label)]\n",
        "    elif opt.vip_folder == \"train\" and opt.coco_folder == \"val\":\n",
        "        train_df = df[(df.fold==FOLD) & (df.label==opt.label)]\n",
        "    else:\n",
        "        if opt.label2 == \"null\":\n",
        "            train_df = df[(df.label==opt.label)]\n",
        "        else:\n",
        "            train_df = df[(df.label==opt.label) | (df.label==opt.label2)]\n",
        "\n",
        "\n",
        "    INFO = {\n",
        "        \"description\": \"VIP CUP 2021 Dataset\",\n",
        "        \"url\": \"https://www.kaggle.com/awsaf49/ieee-vip-cup-2021-train-val-dataset\",\n",
        "        \"version\": \"0.1.0\",\n",
        "        \"year\": 2021,\n",
        "        \"contributor\": \"awsaf\",\n",
        "        \"date_created\": datetime.datetime.utcnow().isoformat(' ')\n",
        "    }\n",
        "\n",
        "    LICENSES = [\n",
        "        {\n",
        "            \"id\": 1,\n",
        "            \"name\": \"Attribution-NonCommercial-ShareAlike License\",\n",
        "            \"url\": \"http://creativecommons.org/licenses/by-nc-sa/2.0/\"\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    CATEGORIES = [\n",
        "        {\n",
        "            'id': 1,\n",
        "            'name': 'person',\n",
        "            'supercategory': 'person',\n",
        "            \"keypoints\": names,\n",
        "            \"skeleton\": skeleton\n",
        "        },\n",
        "    ]\n",
        "\n",
        "    coco_output = {\n",
        "        \"info\": INFO,\n",
        "        \"licenses\": LICENSES,\n",
        "        \"categories\": CATEGORIES,\n",
        "        \"images\": [],\n",
        "        \"annotations\": []\n",
        "    }\n",
        "\n",
        "\n",
        "\n",
        "    image_dir = f'{opt.out_dir}/coco2017/{opt.coco_folder}2017'\n",
        "    annot_dir = f'{opt.out_dir}/coco2017/annotations'\n",
        "    os.makedirs(image_dir, exist_ok=True)\n",
        "    os.makedirs(annot_dir, exist_ok=True)\n",
        "\n",
        "\n",
        "\n",
        "    IMAGES = []\n",
        "    ANNOTATIONS = []\n",
        "    \n",
        "    coco_df = {\n",
        "        'coco_name' : [],\n",
        "        'orig_name' : []\n",
        "    }\n",
        "    \n",
        "    CHANGED = 0\n",
        "    coco_image_id=1\n",
        "    coco_annot_id=1\n",
        "    for idx in tqdm(range(train_df.shape[0]), total=len(train_df)):\n",
        "        image_path = train_df.image_path.iloc[idx]\n",
        "        image_id   = train_df.image_id.iloc[idx]\n",
        "        study_id   = train_df.study_id.iloc[idx]\n",
        "        image_idx  = int(image_id.split('.')[0].split('_')[-1])-1\n",
        "        if opt.is_aspect_ratio:\n",
        "            image, width, height  = read_resize(image_path, dim=opt.dim)\n",
        "        else:\n",
        "            image, width, height  = read_resize(image_path, width=opt.width, height=opt.height, aspect_ratio=False)\n",
        "        new_height, new_width = image.shape[:2]\n",
        "        orig_file_name = study_id + '_' + image_path.split(\"/\")[3] + \"_\" + image_path.split('/')[-1]\n",
        "\n",
        "        # for transpose\n",
        "        file_name = '%012d.png' % coco_image_id\n",
        "        coco_df['coco_name'].append(file_name)\n",
        "        coco_df['orig_name'].append(orig_file_name)\n",
        "        \n",
        "        new_image_path  = os.path.join(image_dir,file_name)\n",
        "        # writing image\n",
        "        cv2.imwrite(new_image_path, image[...,::-1])\n",
        "        # writing data\n",
        "        IMAGES.append(get_image_info(file_name, \n",
        "                                    height=int(new_height), \n",
        "                                    width=int(new_width), \n",
        "                                    id=coco_image_id,))\n",
        "        \n",
        "        if opt.is_test:\n",
        "            label_file = opt.bbox_label_test + '/' + orig_file_name[:-3] + 'txt'\n",
        "            # SEE THIS \n",
        "            label_file = label_file.replace('test1', 'ieee-vip-cup-2021-train-val-dataset')\n",
        "            with open(label_file, \"r\") as f:\n",
        "                data = np.array(f.read().replace('\\n', ' ').strip().split(' ')).astype(np.float32).reshape(-1, 6)[0]\n",
        "                xc, yc, w, h, conf = data[1], data[2], data[3], data[4], data[5]\n",
        "                # using new height, new width --> REMEMBER THIS\n",
        "                xc, yc = xc*new_width, yc*new_height\n",
        "                w, h = w*new_width, h*new_height\n",
        "                if conf < 0.5:\n",
        "                    new_h = h + h*.1\n",
        "                    h = min(new_h, new_height)\n",
        "                    CHANGED += 1\n",
        "                \n",
        "            xmin, ymin = xc - (w/2), yc - (h/2)\n",
        "            bbox = [int(xmin), int(ymin), int(w), int(h)]\n",
        "            ANNOTATIONS.append(get_bbox_info(id=coco_annot_id, image_id=coco_image_id, category_id=1,\n",
        "                                            bbox=bbox, \n",
        "                                            area=w*h,\n",
        "                                            iscrowd=0,\n",
        "                                            segmentation=None))\n",
        "            # print(ANNOTATIONS)\n",
        "            coco_annot_id+=1\n",
        "            \n",
        "\n",
        "        if opt.is_annot:\n",
        "            kp_path = train_df.ir_gt_path.iloc[idx]\n",
        "            kps = load_kps(kp_path, \n",
        "                        width, height,\n",
        "                        new_width, new_height)\n",
        "            # kp of a image\n",
        "            kps_img = kps[image_idx]\n",
        "            # bbox from keypoints\n",
        "            xmin, ymin, xmax, ymax = np.min(kps_img[...,0]), np.min(kps_img[...,1]), np.max(kps_img[...,0]), np.max(kps_img[...,1])\n",
        "            offsetMin = int(15 * np.square((new_height*new_width) / (512*384)))\n",
        "            offsetMax = int(35 * np.square((new_height*new_width) / (512*384)))\n",
        "            xmin, ymin = int(xmin-offsetMin), int(ymin-offsetMax) # kp are too close to body so taking offset\n",
        "            xmin = max(0, xmin)\n",
        "            ymin = max(0, ymin)\n",
        "            w,h = int(xmax-xmin+offsetMin), int(ymax-ymin+offsetMax)\n",
        "            if opt.is_aspect_ratio:\n",
        "                w = min(w, opt.dim)\n",
        "                h = min(h, opt.dim)\n",
        "            else:\n",
        "                w = min(w, opt.width)\n",
        "                h = min(h, opt.height)\n",
        "            bbox = [xmin, ymin, w, h]\n",
        "\n",
        "            #============================\n",
        "            kps_img = [int(x) for x in kps_img.reshape(-1).tolist()]\n",
        "            \n",
        "            \n",
        "            \n",
        "            ANNOTATIONS.append(get_annot_info(kps=kps_img, id=coco_annot_id, image_id=coco_image_id, category_id=1,\n",
        "                                            bbox=bbox, \n",
        "                                            area=w*h,\n",
        "                                            iscrowd=0,\n",
        "                                            segmentation=None))\n",
        "            \n",
        "            coco_annot_id+=1\n",
        "        coco_image_id+=1\n",
        "        \n",
        "    #===========================\n",
        "    coco_output[\"images\"]      = IMAGES\n",
        "    coco_output[\"annotations\"] = ANNOTATIONS\n",
        "\n",
        "    # json file\n",
        "    with open(f'{annot_dir}/person_keypoints_{opt.coco_folder}2017.json', 'w') as output_json_file:\n",
        "        json.dump(coco_output, output_json_file)   \n",
        "\n",
        "    coco_df = pd.DataFrame(coco_df)\n",
        "    coco_df.to_csv(f'{opt.coco_folder}.csv',index=False)\n",
        "    print(f\"Total {len(os.listdir(image_dir))} images found\")\n",
        "    print(f\"Changed bboxes : {CHANGED}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gyZAGMAqNo1d"
      },
      "source": [
        "!python /content/infer_data_coco.py --width 384 \\\n",
        "--height 512 \\\n",
        "--vip_folder \"test1\" \\\n",
        "--coco_folder \"val\" \\\n",
        "--label \"cover1\" \\\n",
        "--label2 \"cover2\" \\\n",
        "--fold 0 \\\n",
        "--base_dir \"/content/data\" \\\n",
        "--out_dir \"/content\" \\\n",
        "--is_test \\\n",
        "--bbox_label_test $LABELS_TEST"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jf9gRqxBNz53"
      },
      "source": [
        "shutil.move(\"/content/coco2017/val2017\", \"/content/coco2017/images/val2017\")\n",
        "shutil.copytree('/content/coco2017', '/content/transpose/data/coco')\n",
        "\n",
        "# configuring config\n",
        "import yaml\n",
        "with open(\"/content/config.yaml\", 'r') as stream:\n",
        "    try:\n",
        "        model_config = yaml.safe_load(stream)\n",
        "    except yaml.YAMLError as exc:\n",
        "        print(exc)\n",
        "\n",
        "model_config['TEST']['MODEL_FILE'] = MODEL_PATH\n",
        "\n",
        "with open('/content/config.yaml', 'w') as file:\n",
        "    yaml.dump(model_config, file)\n",
        "\n",
        "model_config['TEST']['MODEL_FILE']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "59oEF05FONCo"
      },
      "source": [
        "%cd /content/transpose\n",
        "\n",
        "\n",
        "# The score shown here is dummy. Only inference is done.\n",
        "\n",
        "!python tools/test.py --cfg /content/config.yaml"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q8LOmsp1OVYV"
      },
      "source": [
        "RESULT = '/content/transpose/output/coco/transpose_h/config/results/keypoints_val2017_results_0.json'\n",
        "RESULT"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7dmwgzVwOfo1"
      },
      "source": [
        "%%writefile /content/submission.py\n",
        "\n",
        "import argparse\n",
        "import numpy as np\n",
        "import os, shutil\n",
        "import json\n",
        "import datetime\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('--train_width', type=int, default=0, help='height of training image')\n",
        "    parser.add_argument('--train_height', type=int, default=0, help='height of training image')\n",
        "    parser.add_argument('--orig_width', type=int, default=0, help='original width')\n",
        "    parser.add_argument('--orig_height', type=int, default=0, help='original height')\n",
        "    parser.add_argument('--result_json_path', type=str, default=\"result.json\", help=\"result json\")\n",
        "\n",
        "    opt = parser.parse_args()\n",
        "\n",
        "    with open(opt.result_json_path) as f:\n",
        "        result = json.load(f)\n",
        "\n",
        "    points = []\n",
        "    for pred in result:\n",
        "        one_person = []\n",
        "        pred = pred['keypoints']\n",
        "        for idx in range(14):\n",
        "            x = int(pred[3*idx])\n",
        "            y = int(pred[3*idx + 1])\n",
        "            \n",
        "            x = int(x*(opt.orig_width/opt.train_width))\n",
        "            y = int(y*(opt.orig_height/opt.train_height))\n",
        "            one_person.append([x, y])\n",
        "        points.append(one_person)\n",
        "\n",
        "    array = np.array(points)\n",
        "    array[:,:,0] = array[:,:,0].clip(min=0, max=opt.orig_width-1)\n",
        "    array[:,:,1] = array[:,:,1].clip(min=0, max=opt.orig_height-1)\n",
        "\n",
        "    points = array.tolist()\n",
        "\n",
        "    \n",
        "    with open(\"/content/preds.json\", \"w\") as f:\n",
        "        json.dump(points, f)\n",
        "\n",
        "    print(\"Result saved in /content/preds.json.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k139e8srOtO8"
      },
      "source": [
        "!python /content/submission.py --result_json_path $RESULT \\\n",
        "--train_width 384 \\\n",
        "--train_height 512 \\\n",
        "--orig_width 120 \\\n",
        "--orig_height 160"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zPNytxYeOwI6"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}