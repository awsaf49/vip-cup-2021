{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "VIP21 TransPose R version.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "9a7b242076bd4a0cbf6c5b180e491db4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_23d3dd92db754bd7b851d9eadae1b735",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_05dc5e2e638f4baaa05d7b14d8e52735",
              "IPY_MODEL_9225a81f1ea04780bf60640bb73f9ff3"
            ]
          }
        },
        "23d3dd92db754bd7b851d9eadae1b735": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "05dc5e2e638f4baaa05d7b14d8e52735": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_876dc5835df64e688f98708a2fbcd681",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 2250,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 2250,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d694e3eaec3c48e793d8303638774d6b"
          }
        },
        "9225a81f1ea04780bf60640bb73f9ff3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_ee621af3d76646e98cfac01d12d63967",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 2250/2250 [00:38&lt;00:00, 58.61it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_2fff72c38de4443095ae09b239b853c5"
          }
        },
        "876dc5835df64e688f98708a2fbcd681": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d694e3eaec3c48e793d8303638774d6b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ee621af3d76646e98cfac01d12d63967": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "2fff72c38de4443095ae09b239b853c5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "eA_2PwahBSk0"
      },
      "source": [
        "# !wget 'https://storage.googleapis.com/kaggle-data-sets/1360215/2429215/bundle/archive.zip?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=gcp-kaggle-com%40kaggle-161607.iam.gserviceaccount.com%2F20210808%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20210808T122320Z&X-Goog-Expires=259199&X-Goog-SignedHeaders=host&X-Goog-Signature=95eb7522461a1e68faa579645cb304c52b09421fc17a4c1d9a26eaeb02d2473e87ca3c5e1eb5d467b1a6c10ca6224d727a398a5603a3a2dd8d53c96854b35e841801516ac34c9f0edf022e7289ba719940774414311b897342428d49753260be7f1e5df15222709bead811a7bf96610c7d7f07839b0c054ae4d71a9f877bc9b325ecfa368b878f950e3f2b58fd4c3f08ecbaa40d61ce513569bf6a42480c7190a4243d912e58f07e67d6dbd8f2b78d6b8a656adc1a43afc70769637f45eb6b419e90a6107ac95b32fc2ee9bb1c8217ba66cbbdfd67dc06351b81e8dfdae63a53f48fe82183d418da54f0b845e349d3d1759f31b7852b593481aafc387f431fe6' -O data.zip\n",
        "# !mkdir -p /content/vipcup2021-dataset\n",
        "# !unzip -q /content/data.zip -d /content/vipcup2021-dataset\n",
        "# !rm -r /content/data.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XCMfHY-uFNDF"
      },
      "source": [
        "# PARAMS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GPG5M2tRFN9d"
      },
      "source": [
        "INFER_ONLY = True # if you only want to infer using provided weights, set True\n",
        "if INFER_ONLY:\n",
        "    !pip install gdown\n",
        "    !gdown --id 1uvne6rVXv8hGPnES0xXbA5IX9kHcyd6D  ## transpose weight R\n",
        "\n",
        "'''\n",
        "please make sure default directory is : /content/\n",
        "'''\n",
        "TRAINED_MODEL_PATH = '/content/transpose_R.pth'\n",
        "\n",
        "if not INFER_ONLY:\n",
        "    !gdown --id 16XAo2mwUQrKo_D_-f3aYnxg5qioj6DUj\n",
        "PSEUDO_DF_PATH = '/content/pseudo.csv' # path to pseudo data generated by evopose model\n",
        "\n",
        "\n",
        "########### CHANGE THIS ACCORDINGLY #######################\n",
        "vip_data_dir = \"/content/vipcup2021-dataset\"    # Make sure train, valid, test1 folders are immediately inside this directory"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kThlwgjYFgrT"
      },
      "source": [
        "# Get Data\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GQooqqQSD9PY"
      },
      "source": [
        "%%writefile generate_cover.py\n",
        "\n",
        "import argparse\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2,os\n",
        "from glob import glob\n",
        "import scipy.io as scio\n",
        "import scipy\n",
        "import shutil\n",
        "from matplotlib.colors import rgb2hex\n",
        "from skimage.morphology import reconstruction\n",
        "from skimage.morphology import disk\n",
        "from skimage.filters.rank import gradient\n",
        "import skimage\n",
        "from tqdm import tqdm\n",
        "os.system('pip install git+https://github.com/albumentations-team/albumentations')\n",
        "import albumentations as A\n",
        "\n",
        "kplines = [(0, 1), (1, 2), (12, 2), (12, 3), (3, 4), (4, 5), (6, 7),\n",
        "            (7, 8), (8, 12), (12, 9), (9, 10), (10, 11), (12, 13)]\n",
        "name2idx = {\n",
        "    \"Right ankle\":0,\n",
        "    \"Right knee\":1,\n",
        "    \"Right hip\":2,\n",
        "    \"Left hip\":3,\n",
        "    \"Left knee\":4,\n",
        "    \"Left ankle\":5,\n",
        "    \"Right wrist\":6,\n",
        "    \"Right elbow\":7,\n",
        "    \"Right shoulder\":8,\n",
        "    \"Left shoulder\":9,\n",
        "    \"Left elbow\":10,\n",
        "    \"Left wrist\":11,\n",
        "    \"Thorax\":12,\n",
        "    \"Head top\":13, \n",
        "}\n",
        "idx2name = {v:k for k,v in name2idx.items()}\n",
        "\n",
        "def load_kps(kp_path):\n",
        "    gt  = scio.loadmat(kp_path)['joints_gt']\n",
        "    kps = gt[:2].transpose(2, 1, 0) # => (num_image, num_limb, 2) => (None, 14, 2)\n",
        "    return kps.astype(int)\n",
        "\n",
        "def draw_kp(img, kps, kplines, line_th=1, circle_th=2, fontScale=1, text_th =2, text=False):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        img      : image (R, G, B)\n",
        "        kps      : keypoints (num_points, 2)\n",
        "        kplines  : limb line tuple index\n",
        "        text     : show text or not\n",
        "    Returns:\n",
        "        drew image\n",
        "    \"\"\"\n",
        "    cmap   = plt.get_cmap('rainbow')\n",
        "    colors = [cmap(i) for i in np.linspace(0, 1, len(kps) + 2)]\n",
        "    colors = [(c[2] * 255, c[1] * 255, c[0] * 255) for c in colors]\n",
        "    for idx, kpline in enumerate(kplines):\n",
        "        img = cv2.line(img.astype(float), tuple(kps[kpline[0]]), tuple(kps[kpline[1]]), thickness=line_th,\n",
        "                       color=colors[idx], lineType=cv2.LINE_AA)\n",
        "    for idx in range(len(kps)):\n",
        "        color = colors[idx]\n",
        "        img = cv2.circle(img.astype(float),tuple(kps[idx]), circle_th, color , cv2.FILLED)\n",
        "        if text:\n",
        "            w = img.shape[1]\n",
        "            px = kps[idx][0]\n",
        "            py = kps[idx][1]\n",
        "            if px>w//2:\n",
        "                px+=10\n",
        "                color = (255,0,0)\n",
        "            else:\n",
        "                px-=30\n",
        "                color = (0,0,255)\n",
        "            img = cv2.putText(img, str(idx), (px, py), cv2.FONT_HERSHEY_SIMPLEX, \n",
        "                               fontScale=fontScale, color=color, thickness=text_th, lineType=cv2.LINE_AA)\n",
        "    return img.astype('uint8')\n",
        "\n",
        "\n",
        "            \n",
        "def apply_selective_thorax(main_img,transformed_image,point):\n",
        "    extra=int(point)\n",
        "    new_img=main_img.copy()\n",
        "    new_img[extra:,:]=transformed_image[extra:,:]\n",
        "    return new_img\n",
        "\n",
        "def pad_across_width(image):\n",
        "    shapes=image.shape\n",
        "    h=shapes[0]\n",
        "    w=shapes[1]\n",
        "    if len(shapes)>2:\n",
        "        c=shapes[-1]\n",
        "\n",
        "    diff=h-w\n",
        "    side1=int(diff/2)\n",
        "    side2=diff-side1\n",
        "\n",
        "    s1=np.zeros((h,side1)).astype(image.dtype) if len(shapes)==2 else np.zeros((h,side1,c)).astype(image.dtype) \n",
        "    s2=np.zeros((h,side2)).astype(image.dtype) if len(shapes)==2 else np.zeros((h,side2,c)).astype(image.dtype) \n",
        "\n",
        "    new_image=image.copy()\n",
        "    new_image=np.concatenate([s1,new_image,s2],axis=1)\n",
        "    return new_image\n",
        "tpoint=name2idx[\"Thorax\"]\n",
        "\n",
        "def cover_gen(points,image,return_coord=False):\n",
        "    cover=np.zeros(image.shape,dtype=image.dtype)\n",
        "    \n",
        "    cover_corner_right=points[:,0].max()+20\n",
        "    cover_corner_left=points[:,0].min()-20\n",
        "    low=150\n",
        "    hthorax=points[tpoint][1]\n",
        "\n",
        "    if len(image.shape)>2:\n",
        "        cover[hthorax:low,cover_corner_left:cover_corner_right,0]=1# if image.dtype=='uint8' else 255.0\n",
        "        cover=cover[:,:,0]\n",
        "    else:\n",
        "        cover[hthorax:low,cover_corner_left:cover_corner_right]=1 #if image.dtype=='uint8' else 255.0\n",
        "    \n",
        "    if return_coord:\n",
        "        return ((cover_corner_left,hthorax),(cover_corner_right,low)),cover\n",
        "    return cover\n",
        "\n",
        "def combine(image,path1,path2):\n",
        "    albu=A.Compose([\n",
        "                   A.augmentations.domain_adaptation.HistogramMatching(path2,blend_ratio=(0.5, 0.9),p=1),\n",
        "    A.augmentations.domain_adaptation.FDA(path1,p=1,beta_limit=0.05)])\n",
        "    albu_im= albu(image=image)['image']\n",
        "    return albu_im\n",
        "\n",
        "def combiner(image1,image2,cover,final_channel=1):\n",
        "    if len(cover.shape)==2:\n",
        "        cover=np.expand_dims(cover,axis=-1)\n",
        "    if len(image2.shape)==2:\n",
        "        image2=np.expand_dims(image2,axis=-1)\n",
        "    \n",
        "    if len(image1.shape)>2&image1.shape[-1]!=1:\n",
        "        image1=np.expand_dims(image1[:,:,0],axis=-1)\n",
        "    \n",
        "    total=(1-cover)*image1+cover*image2\n",
        "    if final_channel==3:\n",
        "        return np.stack([total,total,total])\n",
        "    return total\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('--data-dir', type=str, default='/kaggle/input/ieee-vip-cup-2021-train-val-dataset/', help='main directory of data')\n",
        "    parser.add_argument('--save-dir', type=str, default='/kaggle/working/VIP',help=\"where to save files, a new directory\")\n",
        "    opt = parser.parse_args()\n",
        "    \n",
        "    save_at=opt.save_dir#'/kaggle/working'\n",
        "    global_path=opt.data_dir\n",
        "    \n",
        "#     if os.path.exists(save_at):\n",
        "#         save_at=os.path.join(save_at,'VIP')\n",
        "        \n",
        "    print('Copying ...')\n",
        "    try:\n",
        "        shutil.copytree(global_path,save_at)\n",
        "    except:\n",
        "        shutil.rmtree(save_at)\n",
        "        shutil.copytree(global_path,save_at)\n",
        "    \n",
        "    print('Finding Train statistics...')\n",
        "    \n",
        "    uncover=[]\n",
        "    cover1=[]\n",
        "    cover2=[]\n",
        "    files_dir=os.path.join(global_path,'train')\n",
        "    if len(os.listdir(files_dir))<4:\n",
        "        files_dir=os.path.join(files_dir,'train')\n",
        "    files=os.listdir(files_dir)\n",
        "    uncover_images_list=[]\n",
        "    for f in files:\n",
        "        current_dir=os.path.join(files_dir,f,'IR')\n",
        "        if 'uncover' in os.listdir(current_dir):\n",
        "            uncover.append(f)\n",
        "            uncover_images_list.extend(glob(os.path.join(current_dir,'uncover/*')))\n",
        "\n",
        "        if 'cover1' in os.listdir(current_dir):\n",
        "            cover1.append(f)\n",
        "        if 'cover2' in os.listdir(current_dir):\n",
        "            cover2.append(f)\n",
        "\n",
        "    print(f'Uncover dirs : {len(uncover)}, Total files: {len(uncover_images_list)}')\n",
        "    print(f'Cover1 dirs : {len(cover1)}')\n",
        "    print(f'Cover2 dirs : {len(cover2)}')\n",
        "    \n",
        "    cover2_path=['image_000027.png',\n",
        "     'image_000041.png',\n",
        "     'image_000038.png',\n",
        "     'image_000023.png']\n",
        "    \n",
        "    cover2_path=[os.path.join(files_dir,'00062/IR/cover2',i) for i in cover2_path]\n",
        "    print('Selected cover images are : ',cover2_path)\n",
        "    \n",
        "    copy_dir=os.path.join(save_at,'train')\n",
        "    if len(os.listdir(copy_dir))<5:\n",
        "        copy_dir=os.path.join(copy_dir,'train')\n",
        "        \n",
        "    print('Generating cover images from uncover')   \n",
        "    for file_num in tqdm(sorted(uncover)):\n",
        "        image_dir=sorted(glob(os.path.join(copy_dir,file_num,'IR','uncover','*')))\n",
        "        keypoints_path=os.path.join(copy_dir,file_num,'joints_gt_IR.mat')\n",
        "\n",
        "        kp=load_kps(keypoints_path)\n",
        "        for i in range(len(image_dir)):\n",
        "            image_path=image_dir[i]\n",
        "\n",
        "            image=cv2.imread(image_path)\n",
        "            point=kp[i-1]\n",
        "            ((x1,y1),(x2,y2)),cover=cover_gen(point,image[:,:,0],return_coord=True)\n",
        "            final_im=combine(image,cover2_path,cover2_path)\n",
        "            final_im=combiner(image,final_im,cover)\n",
        "\n",
        "            cv2.imwrite(image_dir[i],final_im)\n",
        "    print('Completed')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yMg6SaafD_0F"
      },
      "source": [
        "vip_transformed_data_dir = \"/content/transformed\"  \n",
        "!python generate_cover.py --data-dir $vip_data_dir --save-dir $vip_transformed_data_dir"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hb89NO9WFnd6",
        "outputId": "691d6e21-cde1-4baf-852b-6bb452bc412f"
      },
      "source": [
        "import shutil\n",
        "\n",
        "shutil.copytree(vip_data_dir, '/content/data')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/transformed\n",
            "Downloading vip-cup-2021-transformed-images-dataset.zip to /content/transformed\n",
            " 73% 42.0M/57.6M [00:00<00:00, 203MB/s]\n",
            "100% 57.6M/57.6M [00:00<00:00, 192MB/s]\n",
            "/content\n",
            "/content/data\n",
            "Downloading ieee-vip-cup-2021-train-val-dataset.zip to /content/data\n",
            "100% 3.16G/3.16G [00:55<00:00, 33.6MB/s]\n",
            "\n",
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jvp3Z49YFo-p",
        "outputId": "467c12ad-77b6-4f7b-b2eb-a0786dc4ba8d"
      },
      "source": [
        "import shutil, os\n",
        "\n",
        "!mkdir personbbox\n",
        "%cd personbbox\n",
        "!gdown --id 1EVCVl9H0mjJyCrhLqvxaBHPB62-ev9U1 ## mmposeunlabelledpersonbbox\n",
        "mmposeunlabel_zip='/content/personbbox/mmposeunlabelledpersonbbox.zip'\n",
        "!unzip {mmposeunlabel_zip}\n",
        "!rm {mmposeunlabel_zip}\n",
        "shutil.copytree('/content/personbbox/mmposeunlabelledpersonbbox/yolo/yolov5/runs/detect/exp/labels', '/content/labels')\n",
        "%cd ..\n",
        "shutil.rmtree('/content/personbbox') \n",
        "\n",
        "!mkdir personbbox\n",
        "%cd personbbox\n",
        "!gdown --id 1huBWdHHJYvXDa4nIp4o-0h_X6RB7UOH_ ## vip100\n",
        "vip100e_zip='/content/personbbox/vip21personbbox100e.zip'\n",
        "!unzip {vip100e_zip}\n",
        "!rm {vip100e_zip}\n",
        "shutil.copytree('/content/personbbox/vip21personbbox100e/yolo/yolov5/runs/detect/exp/labels', '/content/labels_test')\n",
        "\n",
        "%cd ..\n",
        "shutil.rmtree('/content/personbbox') "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7xy1hE_fGMu0"
      },
      "source": [
        "# Generate Data "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_EhjEC5hGBhc",
        "outputId": "c8489cbf-76d9-4425-fdc6-24ca94c38fb7"
      },
      "source": [
        "%cd /content/\n",
        "\n",
        "LABELS = '/content/labels'\n",
        "LABELS_TEST = '/content/labels_test'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w98NyTwQGlls",
        "outputId": "d1d9407b-9839-4d35-9264-e44468c9f877"
      },
      "source": [
        "%%writefile /content/test_coco.py\n",
        "\n",
        "import argparse\n",
        "import numpy as np\n",
        "import cv2 \n",
        "import matplotlib.pyplot as plt\n",
        "import scipy.io as scio\n",
        "import pandas as pd\n",
        "from tqdm.notebook import tqdm\n",
        "import os, shutil\n",
        "from glob import glob\n",
        "tqdm.pandas()\n",
        "import json\n",
        "import datetime\n",
        "import imagesize\n",
        "from sklearn.model_selection import GroupKFold \n",
        "import scipy.io as scio\n",
        "import cv2\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "def load_kps(kp_path, width, height, new_width, new_height):\n",
        "    gt  = scio.loadmat(kp_path)['joints_gt'] # label = if_ocluded\n",
        "    kps = gt.transpose(2, 1, 0).astype(np.float64) # => (num_image, num_limb, 3) or (None, 14, 3)\n",
        "    kps[..., 0] = (kps[...,0]-1)/width*new_width    # converting one indexing to zero indexing\n",
        "    kps[..., 1] = (kps[...,1]-1)/height*new_height\n",
        "    kps[..., 2] = 2- kps[...,2] # coco format\n",
        "    return kps.astype(np.int32)\n",
        "\n",
        "def load_image(image_path):\n",
        "    return cv2.imread(image_path)[...,::-1]\n",
        "\n",
        "\n",
        "def read_resize(file_path, dim=128, width=128, height=128, aspect_ratio=True):\n",
        "    img = load_image(file_path)\n",
        "    h, w = img.shape[:2]  # orig hw\n",
        "    if aspect_ratio:\n",
        "        r = dim / max(h, w)  # resize image to img_size\n",
        "        interp = cv2.INTER_AREA if r < 1 else cv2.INTER_LINEAR\n",
        "        if r != 1:  # always resize down, only resize up if training with augmentation\n",
        "            img = cv2.resize(img, (int(w * r), int(h * r)), interpolation=interp)\n",
        "            new_h, new_w = img.shape[:2]\n",
        "    else:\n",
        "        img = cv2.resize(img, (width,height), cv2.INTER_AREA)\n",
        "        new_w = dim; new_h = dim\n",
        "        \n",
        "    return img, w, h\n",
        "\n",
        "\n",
        "def get_image_info(file_name, height, width, id,\n",
        "                   license=1, date_captured='', \n",
        "                   coco_url='', flickr_url='',):\n",
        "    return dict(license=license, \n",
        "                file_name=file_name,\n",
        "                coco_url=coco_url,\n",
        "                height=height,\n",
        "                width=width, \n",
        "                date_captured=date_captured,\n",
        "                flickr_url=flickr_url,\n",
        "                id=id)\n",
        "    \n",
        "\n",
        "\n",
        "def get_annot_info(kps, id, image_id, category_id=1, bbox=None, area=None,\n",
        "                   iscrowd=0, segmentation=None):\n",
        "    \"\"\"\n",
        "    Parameters:\n",
        "    --------------- \n",
        "    Args:\n",
        "    \n",
        "    Returns:\n",
        "    \n",
        "    \"\"\"\n",
        "    return {\n",
        "        \"segmentation\": segmentation or [],\n",
        "        \"num_keypoints\": len(kps)//3,\n",
        "        \"area\": area if area else (bbox[2]*bbox[3]),\n",
        "        \"iscrowd\": iscrowd,\n",
        "        \"keypoints\": kps,\n",
        "        \"image_id\": image_id,\n",
        "        \"bbox\": bbox or [0, 0, 0, 0],\n",
        "        \"category_id\": category_id,\n",
        "        \"id\": id,\n",
        "    }\n",
        "\n",
        "\n",
        "def get_bbox_info(id, image_id, category_id=1, bbox=None, area=None,\n",
        "                   iscrowd=0, segmentation=None):\n",
        "    \"\"\"\n",
        "    Parameters:\n",
        "    --------------- \n",
        "    Args:\n",
        "    \n",
        "    Returns:\n",
        "    \n",
        "    \"\"\"\n",
        "    return {\n",
        "        \"segmentation\": segmentation or [],\n",
        "        \"num_keypoints\": 14,\n",
        "        \"area\": area if area else (bbox[2]*bbox[3]),\n",
        "        \"iscrowd\": iscrowd,\n",
        "        \"keypoints\": [0,0,2]*14,\n",
        "        \"image_id\": image_id,\n",
        "        \"bbox\": bbox or [0, 0, 0, 0],\n",
        "        \"category_id\": category_id,\n",
        "        \"id\": id,\n",
        "    }\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('--dim', type=int, default=128, help='resized image shape')\n",
        "    parser.add_argument('--fold', type=int, default=0, help='fold number')\n",
        "    parser.add_argument('--imgid', type=int, default=1, help='starting imgid number for coco')\n",
        "    parser.add_argument('--is_annot', action='store_true', help=\"is there annotaions to use\")\n",
        "    parser.add_argument('--is_test', action='store_true', help=\"testing\")\n",
        "    parser.add_argument('--vip_folder', type=str, default=\"train\", help=\"VIP CUP DATA FOLDER\")\n",
        "    parser.add_argument(\"--coco_folder\", type=str, default=\"train\", help=\"folder used in coco dataset\")\n",
        "    parser.add_argument(\"--bbox_label_test\", type=str, default=\"/content/labels\", help=\"folder containing yolo labels of test person bbox\")\n",
        "    parser.add_argument(\"--base_dir\", type=str, default=\"/content/data\", help=\"base dir for vip dataset folder\")\n",
        "    parser.add_argument(\"--label\", type=str, default=\"uncover\" , help=\"uncover, cover1, cover2\")\n",
        "    parser.add_argument(\"--label2\", type=str, default=\"null\" , help=\"cover1, cover2\")\n",
        "    parser.add_argument(\"--out_dir\", type=str, default=\"/content\" , help=\"output directory\")\n",
        "    parser.add_argument('--is_aspect_ratio', action='store_true', help=\"mainatain aspect ratio. Only use dim. don't use width and height\")\n",
        "    parser.add_argument('--width', type=int, default=128, help='fold number')\n",
        "    parser.add_argument('--height', type=int, default=128, help='fold number')\n",
        "\n",
        "    opt = parser.parse_args()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    skeleton = [(0, 1), (1, 2), (12, 2), (12, 3), (3, 4), (4, 5), (6, 7),\n",
        "                (7, 8), (8, 12), (12, 9), (9, 10), (10, 11), (12, 13)]\n",
        "    skeleton = [[x[0]+1, x[1]+1] for x in skeleton]\n",
        "    name2idx = {\n",
        "        \"Right ankle\":0,\n",
        "        \"Right knee\":1,\n",
        "        \"Right hip\":2,\n",
        "        \"Left hip\":3,\n",
        "        \"Left knee\":4,\n",
        "        \"Left ankle\":5,\n",
        "        \"Right wrist\":6,\n",
        "        \"Right elbow\":7,\n",
        "        \"Right shoulder\":8,\n",
        "        \"Left shoulder\":9,\n",
        "        \"Left elbow\":10,\n",
        "        \"Left wrist\":11,\n",
        "        \"thorax\":12,\n",
        "        \"head top\":13, \n",
        "    }\n",
        "    idx2name = {v:k for k,v in name2idx.items()}\n",
        "    names = list(idx2name.values())\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def get_info(filepath):\n",
        "        x = filepath.split('/')\n",
        "        image_id = x[-1]\n",
        "        label    = x[-2]\n",
        "        modality = x[-3]\n",
        "        study_id = x[-4]\n",
        "        split    = x[-5]\n",
        "        return [filepath, study_id, image_id, modality, label, split]\n",
        "\n",
        "\n",
        "\n",
        "    filepaths = glob(f'{opt.base_dir}/**/*png', recursive=True)\n",
        "    filepaths.sort()\n",
        "    df = pd.DataFrame(list(map(get_info, filepaths)), columns=['image_path', 'study_id', 'image_id',\n",
        "                                                            'modality', 'label', 'split'])\n",
        "\n",
        "\n",
        "\n",
        "    df['rgb_gt_path']    = df.image_path.map(lambda x: os.path.join(x.rsplit('/', 3)[0], 'joints_gt_RGB.mat'))\n",
        "    df['ir_gt_path']     = df.image_path.map(lambda x: os.path.join(x.rsplit('/', 3)[0], 'joints_gt_IR.mat'))\n",
        "    df['rgb_align_path'] = df.image_path.map(lambda x: os.path.join(x.rsplit('/', 3)[0], 'align_PTr_RGB.npy'))\n",
        "    df['ir_align_path']  = df.image_path.map(lambda x: os.path.join(x.rsplit('/', 3)[0], 'align_PTr_IR.npy'))\n",
        "\n",
        "    df[['width', 'height']] = df.image_path.progress_apply(lambda x: list(imagesize.get(x))).tolist()\n",
        "\n",
        "\n",
        "        \n",
        "    df = df[df.split == opt.vip_folder]\n",
        "    df = df[df.modality == \"IR\"]\n",
        "\n",
        "    if opt.vip_folder == \"train\":\n",
        "        gkf = GroupKFold(n_splits=5)\n",
        "        df['fold'] = -1\n",
        "        df.reset_index(drop=True, inplace=True)\n",
        "        for fold, (train_idx, val_idx) in enumerate(gkf.split(df, groups=df['study_id'])):\n",
        "            df.loc[val_idx, 'fold'] = fold\n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "    FOLD = opt.fold\n",
        "    if opt.vip_folder == \"train\" and opt.coco_folder == \"train\":\n",
        "        train_df = df[(df.fold!=FOLD) & (df.label==opt.label)]\n",
        "    elif opt.vip_folder == \"train\" and opt.coco_folder == \"val\":\n",
        "        train_df = df[(df.fold==FOLD) & (df.label==opt.label)]\n",
        "    else:\n",
        "        if opt.label2 == \"null\":\n",
        "            train_df = df[(df.label==opt.label)]\n",
        "        else:\n",
        "            train_df = df[(df.label==opt.label) | (df.label==opt.label2)]\n",
        "\n",
        "\n",
        "    INFO = {\n",
        "        \"description\": \"VIP CUP 2021 Dataset\",\n",
        "        \"url\": \"https://www.kaggle.com/awsaf49/ieee-vip-cup-2021-train-val-dataset\",\n",
        "        \"version\": \"0.1.0\",\n",
        "        \"year\": 2021,\n",
        "        \"contributor\": \"awsaf\",\n",
        "        \"date_created\": datetime.datetime.utcnow().isoformat(' ')\n",
        "    }\n",
        "\n",
        "    LICENSES = [\n",
        "        {\n",
        "            \"id\": 1,\n",
        "            \"name\": \"Attribution-NonCommercial-ShareAlike License\",\n",
        "            \"url\": \"http://creativecommons.org/licenses/by-nc-sa/2.0/\"\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    CATEGORIES = [\n",
        "        {\n",
        "            'id': 1,\n",
        "            'name': 'person',\n",
        "            'supercategory': 'person',\n",
        "            \"keypoints\": names,\n",
        "            \"skeleton\": skeleton\n",
        "        },\n",
        "    ]\n",
        "\n",
        "    coco_output = {\n",
        "        \"info\": INFO,\n",
        "        \"licenses\": LICENSES,\n",
        "        \"categories\": CATEGORIES,\n",
        "        \"images\": [],\n",
        "        \"annotations\": []\n",
        "    }\n",
        "\n",
        "\n",
        "\n",
        "    image_dir = f'{opt.out_dir}/coco2017/{opt.coco_folder}2017'\n",
        "    annot_dir = f'{opt.out_dir}/coco2017/annotations'\n",
        "    os.makedirs(image_dir, exist_ok=True)\n",
        "    os.makedirs(annot_dir, exist_ok=True)\n",
        "\n",
        "\n",
        "\n",
        "    IMAGES = []\n",
        "    ANNOTATIONS = []\n",
        "    \n",
        "    coco_df = {\n",
        "        'coco_name' : [],\n",
        "        'orig_name' : []\n",
        "    }\n",
        "    \n",
        "    coco_image_id=opt.imgid\n",
        "    coco_annot_id=opt.imgid\n",
        "    for idx in tqdm(range(train_df.shape[0])):\n",
        "        image_path = train_df.image_path.iloc[idx]\n",
        "        image_id   = train_df.image_id.iloc[idx]\n",
        "        study_id   = train_df.study_id.iloc[idx]\n",
        "        image_idx  = int(image_id.split('.')[0].split('_')[-1])-1\n",
        "        if opt.is_aspect_ratio:\n",
        "            image, width, height  = read_resize(image_path, dim=opt.dim)\n",
        "        else:\n",
        "            image, width, height  = read_resize(image_path, width=opt.width, height=opt.height, aspect_ratio=False)\n",
        "        new_height, new_width = image.shape[:2]\n",
        "        orig_file_name = study_id + '_' + image_path.split(\"/\")[3] + \"_\" + image_path.split('/')[-1]\n",
        "\n",
        "        # for transpose\n",
        "        file_name = '%012d.png' % coco_image_id\n",
        "        coco_df['coco_name'].append(file_name)\n",
        "        coco_df['orig_name'].append(orig_file_name)\n",
        "        \n",
        "        new_image_path  = os.path.join(image_dir,file_name)\n",
        "        # writing image\n",
        "        cv2.imwrite(new_image_path, image[...,::-1])\n",
        "        # writing data\n",
        "        IMAGES.append(get_image_info(file_name, \n",
        "                                    height=int(new_height), \n",
        "                                    width=int(new_width), \n",
        "                                    id=coco_image_id,))\n",
        "        \n",
        "        if opt.is_test:\n",
        "            label_file = opt.bbox_label_test + '/' + orig_file_name[:-3] + 'txt'\n",
        "            # SEE THIS \n",
        "            label_file = label_file.replace(opt.vip_folder, 'ieee-vip-cup-2021-train-val-dataset')\n",
        "            with open(label_file, \"r\") as f:\n",
        "                data = np.array(f.read().replace('\\n', ' ').strip().split(' ')).astype(np.float32).reshape(-1, 6)[0]\n",
        "                xc, yc, w, h = data[1], data[2], data[3], data[4]\n",
        "                # using new height, new width --> REMEMBER THIS\n",
        "                xc, yc = xc*new_width, yc*new_height\n",
        "                w, h = w*new_width, h*new_height\n",
        "            \n",
        "            xmin, ymin = xc - (w/2), yc - (h/2)\n",
        "            bbox = [int(xmin), int(ymin), int(w), int(h)]\n",
        "            ANNOTATIONS.append(get_bbox_info(id=coco_annot_id, image_id=coco_image_id, category_id=1,\n",
        "                                            bbox=bbox, \n",
        "                                            area=w*h,\n",
        "                                            iscrowd=0,\n",
        "                                            segmentation=None))\n",
        "            # print(ANNOTATIONS)\n",
        "            coco_annot_id+=1\n",
        "            \n",
        "\n",
        "        if opt.is_annot:\n",
        "            kp_path = train_df.ir_gt_path.iloc[idx]\n",
        "            kps = load_kps(kp_path, \n",
        "                        width, height,\n",
        "                        new_width, new_height)\n",
        "            # kp of a image\n",
        "            kps_img = kps[image_idx]\n",
        "            # bbox from keypoints\n",
        "            xmin, ymin, xmax, ymax = np.min(kps_img[...,0]), np.min(kps_img[...,1]), np.max(kps_img[...,0]), np.max(kps_img[...,1])\n",
        "            offsetMin = int(15 * np.square((new_height*new_width) / (512*384)))\n",
        "            offsetMax = int(35 * np.square((new_height*new_width) / (512*384)))\n",
        "            xmin, ymin = int(xmin-offsetMin), int(ymin-offsetMax) # kp are too close to body so taking offset\n",
        "            xmin = max(0, xmin)\n",
        "            ymin = max(0, ymin)\n",
        "            w,h = int(xmax-xmin+offsetMin), int(ymax-ymin+offsetMax)\n",
        "            if opt.is_aspect_ratio:\n",
        "                w = min(w, opt.dim)\n",
        "                h = min(h, opt.dim)\n",
        "            else:\n",
        "                w = min(w, opt.width)\n",
        "                h = min(h, opt.height)\n",
        "            bbox = [xmin, ymin, w, h]\n",
        "\n",
        "            #============================\n",
        "            kps_img = [int(x) for x in kps_img.reshape(-1).tolist()]\n",
        "            \n",
        "            \n",
        "            \n",
        "            ANNOTATIONS.append(get_annot_info(kps=kps_img, id=coco_annot_id, image_id=coco_image_id, category_id=1,\n",
        "                                            bbox=bbox, \n",
        "                                            area=w*h,\n",
        "                                            iscrowd=0,\n",
        "                                            segmentation=None))\n",
        "            \n",
        "            coco_annot_id+=1\n",
        "        coco_image_id+=1\n",
        "        \n",
        "    #===========================\n",
        "    coco_output[\"images\"]      = IMAGES\n",
        "    coco_output[\"annotations\"] = ANNOTATIONS\n",
        "\n",
        "    # json file\n",
        "    with open(f'{annot_dir}/person_keypoints_{opt.coco_folder}2017.json', 'w') as output_json_file:\n",
        "        json.dump(coco_output, output_json_file)   \n",
        "\n",
        "    coco_df = pd.DataFrame(coco_df)\n",
        "    coco_df.to_csv(f'{opt.coco_folder}.csv',index=False)\n",
        "    print(f\"Total {len(os.listdir(image_dir))} images found\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing /content/test_coco.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nUvYN-rqIYxg",
        "outputId": "684140c0-fde8-425e-da00-9866da2e7026"
      },
      "source": [
        "# put unlabelled train data into test\n",
        "if not INFER_ONLY:\n",
        "    !python /content/test_coco.py --width 384 \\\n",
        "    --height 512 \\\n",
        "    --vip_folder \"train\" \\\n",
        "    --coco_folder \"test\" \\\n",
        "    --label \"cover1\" \\\n",
        "    --label2 \"cover2\" \\\n",
        "    --fold 0 \\\n",
        "    --base_dir \"/content/data\" \\\n",
        "    --out_dir \"/content\" \\\n",
        "    --is_test \\\n",
        "    --imgid 1351 \\\n",
        "    --bbox_label_test $LABELS"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tqdm/std.py:658: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
            "  from pandas import Panel\n",
            "HBox(children=(FloatProgress(value=0.0, max=8550.0), HTML(value='')))\n",
            "\n",
            "HBox(children=(FloatProgress(value=0.0, max=2250.0), HTML(value='')))\n",
            "\n",
            "Total 2250 images found\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6alVawhvIpZi",
        "outputId": "73d17ea0-a00a-48fc-cb7c-5c3e46075b11"
      },
      "source": [
        "# transformed train data \n",
        "if not INFER_ONLY:\n",
        "    !python /content/test_coco.py --fold -1 \\\n",
        "    --is_annot \\\n",
        "    --vip_folder \"train\" \\\n",
        "    --coco_folder \"train\" \\\n",
        "    --base_dir \"/content/transformed\" \\\n",
        "    --label \"uncover\" \\\n",
        "    --out_dir \"/content\" \\\n",
        "    --width 384 \\\n",
        "    --height 512"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tqdm/std.py:658: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
            "  from pandas import Panel\n",
            "HBox(children=(FloatProgress(value=0.0, max=4500.0), HTML(value='')))\n",
            "\n",
            "HBox(children=(FloatProgress(value=0.0, max=1350.0), HTML(value='')))\n",
            "\n",
            "Total 1350 images found\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Lod6pszJKLV",
        "outputId": "3890e149-ac95-43d4-ae0a-efc57b5b0246"
      },
      "source": [
        "# valid data\n",
        "if not INFER_ONLY:\n",
        "    !python /content/test_coco.py --width 384 \\\n",
        "    --height 512 \\\n",
        "    --is_annot \\\n",
        "    --vip_folder \"valid\" \\\n",
        "    --coco_folder \"val\" \\\n",
        "    --label \"cover1\" \\\n",
        "    --label2 \"cover2\" \\\n",
        "    --fold 0 \\\n",
        "    --base_dir \"/content/data\" \\\n",
        "    --out_dir \"/content\" "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tqdm/std.py:658: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
            "  from pandas import Panel\n",
            "HBox(children=(FloatProgress(value=0.0, max=8550.0), HTML(value='')))\n",
            "\n",
            "HBox(children=(FloatProgress(value=0.0, max=450.0), HTML(value='')))\n",
            "\n",
            "Total 450 images found\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cbsb0aphJQPc"
      },
      "source": [
        "import pandas as pd\n",
        "if not INFER_ONLY:\n",
        "    # sanity check\n",
        "    test_df = pd.read_csv('/content/test.csv')\n",
        "    test_df[test_df.coco_name == '000000001353.png']['orig_name'].iloc[0]  #.split('_')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67,
          "referenced_widgets": [
            "9a7b242076bd4a0cbf6c5b180e491db4",
            "23d3dd92db754bd7b851d9eadae1b735",
            "05dc5e2e638f4baaa05d7b14d8e52735",
            "9225a81f1ea04780bf60640bb73f9ff3",
            "876dc5835df64e688f98708a2fbcd681",
            "d694e3eaec3c48e793d8303638774d6b",
            "ee621af3d76646e98cfac01d12d63967",
            "2fff72c38de4443095ae09b239b853c5"
          ]
        },
        "id": "rE9_k-Y8JUPn",
        "outputId": "11ee1669-dd54-4e6d-fa32-24eb9fc88b61"
      },
      "source": [
        "# get pseudo labels and merge train data\n",
        "\n",
        "if not INFER_ONLY:\n",
        "    import json\n",
        "    import os\n",
        "    import numpy as np\n",
        "    import cv2\n",
        "    import datetime\n",
        "    import pandas as pd\n",
        "    import ast\n",
        "    from tqdm.notebook import tqdm\n",
        "\n",
        "\n",
        "    width = 384\n",
        "    height = 512\n",
        "\n",
        "\n",
        "    def get_image_info(file_name, height, width, id,\n",
        "                    license=1, date_captured='', \n",
        "                    coco_url='', flickr_url='',):\n",
        "        return dict(license=license, \n",
        "                    file_name=file_name,\n",
        "                    coco_url=coco_url,\n",
        "                    height=height,\n",
        "                    width=width, \n",
        "                    date_captured=date_captured,\n",
        "                    flickr_url=flickr_url,\n",
        "                    id=id)\n",
        "        \n",
        "\n",
        "    def get_annot_info(kps, id, image_id, category_id=1, bbox=None, area=None,\n",
        "                    iscrowd=0, segmentation=None):\n",
        "        return {\n",
        "            \"segmentation\": segmentation or [],\n",
        "            \"num_keypoints\": len(kps)//3,\n",
        "            \"area\": area if area else (bbox[2]*bbox[3]),\n",
        "            \"iscrowd\": iscrowd,\n",
        "            \"keypoints\": kps,\n",
        "            \"image_id\": image_id,\n",
        "            \"bbox\": bbox or [0, 0, 0, 0],\n",
        "            \"category_id\": category_id,\n",
        "            \"id\": id,\n",
        "        }\n",
        "\n",
        "    IMAGES = []\n",
        "    ANNOTATIONS = []\n",
        "    with open(\"/content/coco2017/annotations/person_keypoints_train2017.json\") as f: \n",
        "        annots = json.load(f)\n",
        "\n",
        "    folder = \"/content/coco2017/test2017\"\n",
        "\n",
        "    result_df = pd.read_csv(PSEUDO_DF_PATH)\n",
        "\n",
        "    IMAGES = annots['images']\n",
        "    ANNOTATIONS = annots['annotations']\n",
        "\n",
        "    coco_image_id = len(IMAGES) + 1\n",
        "    coco_annot_id = len(ANNOTATIONS) + 1\n",
        "    # print(coco_image_id, coco_annot_id)\n",
        "\n",
        "    image_paths = os.listdir(folder)\n",
        "    image_paths.sort()\n",
        "\n",
        "    sz = len(image_paths)\n",
        "    for idx in tqdm(range(sz), total=sz):\n",
        "        file_name = image_paths[idx]\n",
        "        image_path = \"/content/coco2017/test2017/\" + file_name\n",
        "        new_image_path = \"/content/coco2017/train2017/\" + file_name\n",
        "        image = cv2.imread(image_path)[...,::-1]\n",
        "        image = cv2.resize(image, (width,height), cv2.INTER_AREA)\n",
        "\n",
        "\n",
        "        IMAGES.append(get_image_info(file_name, \n",
        "                        height=int(height), \n",
        "                        width=int(width), \n",
        "                        id=coco_image_id,))\n",
        "        \n",
        "        \n",
        "        # preds = np.array(preds).astype(np.int32)\n",
        "        # preds = preds[:,:-1]\n",
        "        # preds = preds.tolist()\n",
        "        # kps = []\n",
        "        # for pred in preds:\n",
        "        #     kps += (pred + [2])\n",
        "        \n",
        "        \n",
        "        test_df = pd.read_csv('/content/test.csv')\n",
        "    #     print(file_name)\n",
        "        file_orig_name = test_df[test_df.coco_name == file_name]['orig_name'].iloc[0]\n",
        "        df_search_name = file_orig_name.split('_')\n",
        "        df_search_name = df_search_name[0] + '_' + df_search_name[2] + '_' + df_search_name[3]\n",
        "    #     df_search_name = file_name.replace(\"train_\", \"\")\n",
        "    #     print(df_search_name)\n",
        "        kps = ast.literal_eval(result_df[result_df.filename == df_search_name].kps.values[0])\n",
        "        kps = np.array(kps)\n",
        "        kps[:, 0] = kps[:, 0] * (width/120)\n",
        "        kps[:, 1] = kps[:, 1] * (height/160)\n",
        "        kps = kps.astype(np.int32)\n",
        "        kps = kps.tolist()\n",
        "        kpts = []\n",
        "        for kp in kps:\n",
        "            kpts += kp\n",
        "\n",
        "        label_file = LABELS + '/' + file_orig_name[:-3] + 'txt'\n",
        "        # SEE THIS \n",
        "        label_file = label_file.replace('train', 'ieee-vip-cup-2021-train-val-dataset')\n",
        "\n",
        "        with open(label_file, \"r\") as f:\n",
        "    #         print(data)\n",
        "            data = np.array(f.read().replace('\\n', ' ').strip().split(' ')).astype(np.float32).reshape(-1, 6)[0]\n",
        "            xc, yc, w, h = data[1], data[2], data[3], data[4]\n",
        "            # using new height, new width --> REMEMBER THIS\n",
        "            xc, yc = xc*width, yc*height\n",
        "            w, h = w*width, h*height\n",
        "        xmin, ymin = xc - (w/2), yc - (h/2)\n",
        "        bbox = [int(xmin), int(ymin), int(w), int(h)]\n",
        "\n",
        "        ANNOTATIONS.append(get_annot_info(kps=kpts, id=coco_annot_id, image_id=coco_image_id, category_id=1,\n",
        "                                    bbox=bbox, \n",
        "                                    area=int(w*h),\n",
        "                                    iscrowd=0,\n",
        "                                    segmentation=None))\n",
        "        \n",
        "        cv2.imwrite(new_image_path, image[...,::-1])\n",
        "        coco_image_id += 1\n",
        "        coco_annot_id+=1\n",
        "\n",
        "\n",
        "    skeleton = [(0, 1), (1, 2), (12, 2), (12, 3), (3, 4), (4, 5), (6, 7),\n",
        "                (7, 8), (8, 12), (12, 9), (9, 10), (10, 11), (12, 13)]\n",
        "    skeleton = [[x[0]+1, x[1]+1] for x in skeleton]\n",
        "    name2idx = {\n",
        "        \"Right ankle\":0,\n",
        "        \"Right knee\":1,\n",
        "        \"Right hip\":2,\n",
        "        \"Left hip\":3,\n",
        "        \"Left knee\":4,\n",
        "        \"Left ankle\":5,\n",
        "        \"Right wrist\":6,\n",
        "        \"Right elbow\":7,\n",
        "        \"Right shoulder\":8,\n",
        "        \"Left shoulder\":9,\n",
        "        \"Left elbow\":10,\n",
        "        \"Left wrist\":11,\n",
        "        \"thorax\":12,\n",
        "        \"head top\":13, \n",
        "    }\n",
        "    idx2name = {v:k for k,v in name2idx.items()}\n",
        "    names = list(idx2name.values())\n",
        "\n",
        "\n",
        "    INFO = {\n",
        "        \"description\": \"VIP CUP 2021 Dataset\",\n",
        "        \"url\": \"https://www.kaggle.com/awsaf49/ieee-vip-cup-2021-train-val-dataset\",\n",
        "        \"version\": \"0.1.0\",\n",
        "        \"year\": 2021,\n",
        "        \"contributor\": \"awsaf\",\n",
        "        \"date_created\": datetime.datetime.utcnow().isoformat(' ')\n",
        "    }\n",
        "\n",
        "    LICENSES = [\n",
        "        {\n",
        "            \"id\": 1,\n",
        "            \"name\": \"Attribution-NonCommercial-ShareAlike License\",\n",
        "            \"url\": \"http://creativecommons.org/licenses/by-nc-sa/2.0/\"\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    CATEGORIES = [\n",
        "        {\n",
        "            'id': 1,\n",
        "            'name': 'person',\n",
        "            'supercategory': 'person',\n",
        "            \"keypoints\": names,\n",
        "            \"skeleton\": skeleton\n",
        "        },\n",
        "    ]\n",
        "\n",
        "    coco_output = {\n",
        "        \"info\": INFO,\n",
        "        \"licenses\": LICENSES,\n",
        "        \"categories\": CATEGORIES,\n",
        "        \"images\": [],\n",
        "        \"annotations\": []\n",
        "    }\n",
        "\n",
        "\n",
        "    coco_output[\"images\"]      = IMAGES\n",
        "    coco_output[\"annotations\"] = ANNOTATIONS\n",
        "\n",
        "    with open(f'/content/coco2017/annotations/person_keypoints_train2017.json', 'w') as output_json_file:\n",
        "        json.dump(coco_output, output_json_file)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9a7b242076bd4a0cbf6c5b180e491db4",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=2250.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qp-vf4TuKT5X",
        "outputId": "c5aeceda-2843-400f-9b6b-7e02b9eec54b"
      },
      "source": [
        "import os, shutil\n",
        "from glob import glob\n",
        "\n",
        "if not INFER_ONLY:\n",
        "    # delete previous test\n",
        "    shutil.rmtree(\"/content/coco2017/test2017\")\n",
        "\n",
        "    print(\"Total training files : \", len(glob('/content/coco2017/train2017/*')))\n",
        "    shutil.move(\"/content/coco2017/val2017\", \"/content/coco2017/images/val2017\")\n",
        "    shutil.move(\"/content/coco2017/train2017\", \"/content/coco2017/images/train2017\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total training files :  3600\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "63PwPuwmK7WC"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nH0pOaxaK_K3"
      },
      "source": [
        "# Installation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wR8jqkAlLAmf",
        "outputId": "ffc4447f-6be6-4cca-8e22-578910b6ea3c"
      },
      "source": [
        "!gdown --id 1mtJdjIo-CXJY9pDDBxWSHoXY5v9Pu8pd ## mmpose code\n",
        "mmpose_zip='/content/MMPose.zip'\n",
        "!unzip {mmpose_zip}\n",
        "!rm {mmpose_zip}\n",
        "\n",
        "shutil.copytree('MMPose/transpose', 'transpose')\n",
        "shutil.rmtree('MMPose')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'mmpose'...\n",
            "remote: Enumerating objects: 2366, done.\u001b[K\n",
            "remote: Counting objects: 100% (2366/2366), done.\u001b[K\n",
            "remote: Compressing objects: 100% (1204/1204), done.\u001b[K\n",
            "remote: Total 2366 (delta 1442), reused 2048 (delta 1124), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (2366/2366), 13.13 MiB | 27.54 MiB/s, done.\n",
            "Resolving deltas: 100% (1442/1442), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BBOT_YqtLEuv",
        "outputId": "15750c9c-1579-4b70-8490-125ee48a6b69"
      },
      "source": [
        "%cd transpose\n",
        "!pip install -r requirements.txt\n",
        "\n",
        "POSE_ROOT = '/content/transpose'\n",
        "%mkdir output log\n",
        "\n",
        "%mkdir models\n",
        "%cd models\n",
        "%mkdir pytorch\n",
        "%cd pytorch\n",
        "\n",
        "%mkdir transpose_coco\n",
        "%cd transpose_coco\n",
        "!wget https://github.com/yangsenius/TransPose/releases/download/Hub/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
        "%cd /content/transpose"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/transpose\n",
            "Requirement already satisfied: pycocotools in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 1)) (2.0.2)\n",
            "Collecting EasyDict==1.7\n",
            "  Downloading easydict-1.7.tar.gz (6.2 kB)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 3)) (4.1.2.30)\n",
            "Requirement already satisfied: shapely in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 4)) (1.7.1)\n",
            "Requirement already satisfied: Cython in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 5)) (0.29.23)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 6)) (1.4.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 7)) (1.1.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 8)) (3.13)\n",
            "Collecting json_tricks\n",
            "  Downloading json_tricks-3.15.5-py2.py3-none-any.whl (26 kB)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 10)) (0.16.2)\n",
            "Collecting yacs>=0.1.5\n",
            "  Downloading yacs-0.1.8-py3-none-any.whl (14 kB)\n",
            "Collecting tensorboardX\n",
            "  Downloading tensorboardX-2.4-py2.py3-none-any.whl (124 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 124 kB 12.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools>=18.0 in /usr/local/lib/python3.7/dist-packages (from pycocotools->-r requirements.txt (line 1)) (57.2.0)\n",
            "Requirement already satisfied: matplotlib>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from pycocotools->-r requirements.txt (line 1)) (3.2.2)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.1.0->pycocotools->-r requirements.txt (line 1)) (1.19.5)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.1.0->pycocotools->-r requirements.txt (line 1)) (2.4.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.1.0->pycocotools->-r requirements.txt (line 1)) (1.3.1)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.1.0->pycocotools->-r requirements.txt (line 1)) (2.8.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.1.0->pycocotools->-r requirements.txt (line 1)) (0.10.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from cycler>=0.10->matplotlib>=2.1.0->pycocotools->-r requirements.txt (line 1)) (1.15.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->-r requirements.txt (line 7)) (2018.9)\n",
            "Requirement already satisfied: pillow>=4.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->-r requirements.txt (line 10)) (7.1.2)\n",
            "Requirement already satisfied: PyWavelets>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->-r requirements.txt (line 10)) (1.1.1)\n",
            "Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->-r requirements.txt (line 10)) (2.4.1)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->-r requirements.txt (line 10)) (2.5.1)\n",
            "Requirement already satisfied: decorator<5,>=4.3 in /usr/local/lib/python3.7/dist-packages (from networkx>=2.0->scikit-image->-r requirements.txt (line 10)) (4.4.2)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorboardX->-r requirements.txt (line 12)) (3.17.3)\n",
            "Building wheels for collected packages: EasyDict\n",
            "  Building wheel for EasyDict (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for EasyDict: filename=easydict-1.7-py3-none-any.whl size=6121 sha256=b495f97c8ec6ba3f60714fd9ae280be6cf145d258e3eca7ee4f56fcd39c94013\n",
            "  Stored in directory: /root/.cache/pip/wheels/1d/be/cf/14f66f4c4cea5ebf212d9ce559c8a0d8f4882ecc7f59a6710c\n",
            "Successfully built EasyDict\n",
            "Installing collected packages: yacs, tensorboardX, json-tricks, EasyDict\n",
            "  Attempting uninstall: EasyDict\n",
            "    Found existing installation: easydict 1.9\n",
            "    Uninstalling easydict-1.9:\n",
            "      Successfully uninstalled easydict-1.9\n",
            "Successfully installed EasyDict-1.7 json-tricks-3.15.5 tensorboardX-2.4 yacs-0.1.8\n",
            "/content/transpose/models\n",
            "/content/transpose/models/pytorch\n",
            "/content/transpose/models/pytorch/transpose_coco\n",
            "--2021-08-01 20:40:55--  https://github.com/yangsenius/TransPose/releases/download/Hub/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
            "Resolving github.com (github.com)... 192.30.255.113\n",
            "Connecting to github.com (github.com)|192.30.255.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://github-releases.githubusercontent.com/325727200/d2eb3d80-4f42-11eb-9ad4-ba65cd83a50c?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20210801%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20210801T204055Z&X-Amz-Expires=300&X-Amz-Signature=b5fab25fa5c971a4f32adb427f5854e1c9d8d59e76a6988491c8b431ac1d5f01&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=325727200&response-content-disposition=attachment%3B%20filename%3Dtp_r_256x192_enc4_d256_h1024_mh8.pth&response-content-type=application%2Foctet-stream [following]\n",
            "--2021-08-01 20:40:55--  https://github-releases.githubusercontent.com/325727200/d2eb3d80-4f42-11eb-9ad4-ba65cd83a50c?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20210801%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20210801T204055Z&X-Amz-Expires=300&X-Amz-Signature=b5fab25fa5c971a4f32adb427f5854e1c9d8d59e76a6988491c8b431ac1d5f01&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=325727200&response-content-disposition=attachment%3B%20filename%3Dtp_r_256x192_enc4_d256_h1024_mh8.pth&response-content-type=application%2Foctet-stream\n",
            "Resolving github-releases.githubusercontent.com (github-releases.githubusercontent.com)... 185.199.108.154, 185.199.109.154, 185.199.111.154, ...\n",
            "Connecting to github-releases.githubusercontent.com (github-releases.githubusercontent.com)|185.199.108.154|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 24055881 (23M) [application/octet-stream]\n",
            "Saving to: â€˜tp_r_256x192_enc4_d256_h1024_mh8.pthâ€™\n",
            "\n",
            "tp_r_256x192_enc4_d 100%[===================>]  22.94M  72.5MB/s    in 0.3s    \n",
            "\n",
            "2021-08-01 20:40:56 (72.5 MB/s) - â€˜tp_r_256x192_enc4_d256_h1024_mh8.pthâ€™ saved [24055881/24055881]\n",
            "\n",
            "/content/transpose\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jBDGDPuZLanG",
        "outputId": "aa5fda0d-9e92-40e1-fc20-3290ddf6b853"
      },
      "source": [
        "%cd lib\n",
        "!make #need gpu"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/transpose/lib\n",
            "cd nms; python setup_linux.py build_ext --inplace; rm -rf build; cd ../../\n",
            "running build_ext\n",
            "skipping 'cpu_nms.c' Cython extension (up-to-date)\n",
            "skipping 'gpu_nms.cpp' Cython extension (up-to-date)\n",
            "building 'cpu_nms' extension\n",
            "creating build\n",
            "creating build/temp.linux-x86_64-3.7\n",
            "x86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fdebug-prefix-map=/build/python3.7-LSlbJj/python3.7-3.7.11=. -fstack-protector-strong -Wformat -Werror=format-security -g -fdebug-prefix-map=/build/python3.7-LSlbJj/python3.7-3.7.11=. -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.7/dist-packages/numpy/core/include -I/usr/include/python3.7m -c cpu_nms.c -o build/temp.linux-x86_64-3.7/cpu_nms.o -Wno-cpp -Wno-unused-function\n",
            "x86_64-linux-gnu-gcc -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fdebug-prefix-map=/build/python3.7-LSlbJj/python3.7-3.7.11=. -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.7/cpu_nms.o -o /content/transpose/lib/nms/cpu_nms.cpython-37m-x86_64-linux-gnu.so\n",
            "building 'gpu_nms' extension\n",
            "/usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.7/dist-packages/numpy/core/include -I/usr/local/cuda/include -I/usr/include/python3.7m -c nms_kernel.cu -o build/temp.linux-x86_64-3.7/nms_kernel.o -arch=sm_35 --ptxas-options=-v -c --compiler-options '-fPIC'\n",
            "nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n",
            "ptxas info    : 0 bytes gmem\n",
            "ptxas info    : Compiling entry function '_Z10nms_kernelifPKfPy' for 'sm_35'\n",
            "ptxas info    : Function properties for _Z10nms_kernelifPKfPy\n",
            "    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "ptxas info    : Used 22 registers, 1280 bytes smem, 344 bytes cmem[0], 16 bytes cmem[2]\n",
            "x86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fdebug-prefix-map=/build/python3.7-LSlbJj/python3.7-3.7.11=. -fstack-protector-strong -Wformat -Werror=format-security -g -fdebug-prefix-map=/build/python3.7-LSlbJj/python3.7-3.7.11=. -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.7/dist-packages/numpy/core/include -I/usr/local/cuda/include -I/usr/include/python3.7m -c gpu_nms.cpp -o build/temp.linux-x86_64-3.7/gpu_nms.o -Wno-unused-function\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/numpy/core/include/numpy/ndarraytypes.h:1822:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/numpy/core/include/numpy/ndarrayobject.h:12\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/numpy/core/include/numpy/arrayobject.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kgpu_nms.cpp:624\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/numpy/core/include/numpy/npy_1_7_deprecated_api.h:17:2:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K#warning \"Using deprecated NumPy API, disable it with \" \"#define NPY_NO_DEPRECATED_API NPY_1_7_API_VERSION\" [\u001b[01;35m\u001b[K-Wcpp\u001b[m\u001b[K]\n",
            " #\u001b[01;35m\u001b[Kwarning\u001b[m\u001b[K \"Using deprecated NumPy API, disable it with \" \\\n",
            "  \u001b[01;35m\u001b[K^~~~~~~\u001b[m\u001b[K\n",
            "x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fdebug-prefix-map=/build/python3.7-LSlbJj/python3.7-3.7.11=. -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.7/nms_kernel.o build/temp.linux-x86_64-3.7/gpu_nms.o -L/usr/local/cuda/lib64 -Wl,--enable-new-dtags,-R/usr/local/cuda/lib64 -lcudart -o /content/transpose/lib/nms/gpu_nms.cpython-37m-x86_64-linux-gnu.so\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BNtzPQbRLeZX",
        "outputId": "e402e25e-7cd7-406a-9808-a88242428fb4"
      },
      "source": [
        "if not INFER_ONLY:\n",
        "    shutil.copytree('/content/coco2017', '/content/transpose/data/coco')\n",
        "    # sanity check\n",
        "    print(len(glob('/content/transpose/data/coco/images/train2017/*')))\n",
        "\n",
        "    import json \n",
        "    with open('/content/transpose/data/coco/annotations/person_keypoints_train2017.json') as f:\n",
        "        result = json.load(f)\n",
        "    len(result['annotations'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3600\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wYvcmi4GLsZw"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tg2JR0jZLt8X"
      },
      "source": [
        "# Config"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kalBkiTWLuuZ",
        "outputId": "c6fb3ce6-975c-40b0-cf12-cd8cee2f8f32"
      },
      "source": [
        "%%writefile /content/config.yaml\n",
        "\n",
        "AUTO_RESUME: true\n",
        "CUDNN:\n",
        "  BENCHMARK: true\n",
        "  DETERMINISTIC: false\n",
        "  ENABLED: true\n",
        "DATA_DIR: ''\n",
        "GPUS: (0,)\n",
        "OUTPUT_DIR: 'output'\n",
        "LOG_DIR: 'log'\n",
        "WORKERS: 24\n",
        "PRINT_FREQ: 100\n",
        "\n",
        "DATASET:\n",
        "  COLOR_RGB: true\n",
        "  DATASET: 'coco'\n",
        "  DATA_FORMAT: jpg\n",
        "  FLIP: true\n",
        "  NUM_JOINTS_HALF_BODY: 7\n",
        "  PROB_HALF_BODY: 0.3\n",
        "  ROOT: 'data/coco/'\n",
        "  ROT_FACTOR: 45\n",
        "  SCALE_FACTOR: 0.35\n",
        "  TEST_SET: 'val2017'\n",
        "  TRAIN_SET: 'train2017'\n",
        "MODEL:\n",
        "  # Transformer Encoder\n",
        "  DIM_MODEL: 256\n",
        "  DIM_FEEDFORWARD: 1024\n",
        "  N_HEAD: 8\n",
        "  ENCODER_LAYERS: 4\n",
        "  ATTENTION_ACTIVATION: relu\n",
        "  POS_EMBEDDING: sine\n",
        "  # #\n",
        "  NAME: 'transpose_r'\n",
        "  PRETRAINED: 'models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth'\n",
        "  IMAGE_SIZE:\n",
        "  - 192\n",
        "  - 256\n",
        "  HEATMAP_SIZE:\n",
        "  - 48\n",
        "  - 64\n",
        "  SIGMA: 2\n",
        "  NUM_JOINTS: 14\n",
        "  TARGET_TYPE: 'gaussian'\n",
        "  EXTRA:\n",
        "    FINAL_CONV_KERNEL: 1\n",
        "    DECONV_WITH_BIAS: false\n",
        "    NUM_DECONV_LAYERS: 1\n",
        "    NUM_DECONV_FILTERS:\n",
        "    - 256\n",
        "    NUM_DECONV_KERNELS:\n",
        "    - 4\n",
        "    NUM_LAYERS: 50\n",
        "LOSS:\n",
        "  USE_TARGET_WEIGHT: true\n",
        "TRAIN:\n",
        "  BATCH_SIZE_PER_GPU: 16 # ATTENTION : INPUT HIGHEST BATCH SIZE THAT DOESNT CAUSE MEMORY ERROR\n",
        "  SHUFFLE: true\n",
        "  BEGIN_EPOCH: 0\n",
        "  END_EPOCH: 100\n",
        "  OPTIMIZER: adam\n",
        "  LR: 0.0001  # Initial learning rate\n",
        "  LR_END: 0.00001  # Final learning rate\n",
        "  LR_FACTOR: 0.25  # for MultiStepLR\n",
        "  LR_STEP:  # for MultiStepLR\n",
        "  - 100\n",
        "  - 150\n",
        "  - 200\n",
        "  - 220\n",
        "  WD: 0.0001\n",
        "  GAMMA1: 0.99\n",
        "  GAMMA2: 0.0\n",
        "  MOMENTUM: 0.9\n",
        "  NESTEROV: false\n",
        "TEST:\n",
        "  BLUR_KERNEL: 11\n",
        "  BATCH_SIZE_PER_GPU: 8\n",
        "  COCO_BBOX_FILE: ''\n",
        "  BBOX_THRE: 1.0\n",
        "  IMAGE_THRE: 0.0\n",
        "  IN_VIS_THRE: 0.2\n",
        "  MODEL_FILE: models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
        "  NMS_THRE: 1.0\n",
        "  OKS_THRE: 0.9\n",
        "  USE_GT_BBOX: true\n",
        "  FLIP_TEST: true\n",
        "  POST_PROCESS: true\n",
        "  SHIFT_HEATMAP: true\n",
        "DEBUG:\n",
        "  DEBUG: true\n",
        "  SAVE_BATCH_IMAGES_GT: true\n",
        "  SAVE_BATCH_IMAGES_PRED: true\n",
        "  SAVE_HEATMAPS_GT: true\n",
        "  SAVE_HEATMAPS_PRED: true\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing /content/config.yaml\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "faJWN2k7L5g6"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iU4ySpTEL7IF"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3RFARZFlL7s2",
        "outputId": "0a126d54-11ec-46ad-8160-f4ec06636622"
      },
      "source": [
        "if not INFER_ONLY:\n",
        "\n",
        "    %cd /content/transpose\n",
        "\n",
        "    !python tools/train.py --cfg /content/config.yaml --augTrain"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/transpose\n",
            "=> creating output/coco/transpose_r/config\n",
            "=> creating log/coco/transpose_r/config_2021-08-01-20-41\n",
            "Namespace(augTrain=True, cfg='/content/config.yaml', dataDir='', logDir='', modelDir='', opts=[], prevModelDir='')\n",
            "AUTO_RESUME: True\n",
            "CUDNN:\n",
            "  BENCHMARK: True\n",
            "  DETERMINISTIC: False\n",
            "  ENABLED: True\n",
            "DATASET:\n",
            "  COLOR_RGB: True\n",
            "  DATASET: coco\n",
            "  DATA_FORMAT: jpg\n",
            "  FLIP: True\n",
            "  HYBRID_JOINTS_TYPE: \n",
            "  NUM_JOINTS_HALF_BODY: 7\n",
            "  PROB_HALF_BODY: 0.3\n",
            "  ROOT: data/coco/\n",
            "  ROT_FACTOR: 45\n",
            "  SCALE_FACTOR: 0.35\n",
            "  SELECT_DATA: False\n",
            "  TEST_SET: val2017\n",
            "  TRAIN_SET: train2017\n",
            "DATA_DIR: \n",
            "DEBUG:\n",
            "  DEBUG: True\n",
            "  SAVE_BATCH_IMAGES_GT: True\n",
            "  SAVE_BATCH_IMAGES_PRED: True\n",
            "  SAVE_HEATMAPS_GT: True\n",
            "  SAVE_HEATMAPS_PRED: True\n",
            "GPUS: (0,)\n",
            "LOG_DIR: log\n",
            "LOSS:\n",
            "  TOPK: 8\n",
            "  USE_DIFFERENT_JOINTS_WEIGHT: False\n",
            "  USE_OHKM: False\n",
            "  USE_TARGET_WEIGHT: True\n",
            "MODEL:\n",
            "  ATTENTION_ACTIVATION: relu\n",
            "  BOTTLENECK_NUM: 0\n",
            "  DIM_FEEDFORWARD: 1024\n",
            "  DIM_MODEL: 256\n",
            "  ENCODER_LAYERS: 4\n",
            "  EXTRA:\n",
            "    DECONV_WITH_BIAS: False\n",
            "    FINAL_CONV_KERNEL: 1\n",
            "    NUM_DECONV_FILTERS: [256]\n",
            "    NUM_DECONV_KERNELS: [4]\n",
            "    NUM_DECONV_LAYERS: 1\n",
            "    NUM_LAYERS: 50\n",
            "  HEATMAP_SIZE: [48, 64]\n",
            "  IMAGE_SIZE: [192, 256]\n",
            "  INIT_WEIGHTS: True\n",
            "  INTERMEDIATE_SUP: False\n",
            "  NAME: transpose_r\n",
            "  NUM_JOINTS: 14\n",
            "  N_HEAD: 8\n",
            "  PE_ONLY_AT_BEGIN: False\n",
            "  POS_EMBEDDING: sine\n",
            "  PRETRAINED: models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
            "  SIGMA: 2\n",
            "  TAG_PER_JOINT: True\n",
            "  TARGET_TYPE: gaussian\n",
            "OUTPUT_DIR: output\n",
            "PIN_MEMORY: True\n",
            "PRINT_FREQ: 100\n",
            "RANK: 0\n",
            "TEST:\n",
            "  BATCH_SIZE_PER_GPU: 8\n",
            "  BBOX_THRE: 1.0\n",
            "  BLUR_KERNEL: 11\n",
            "  COCO_BBOX_FILE: \n",
            "  FLIP_TEST: True\n",
            "  IMAGE_THRE: 0.0\n",
            "  IN_VIS_THRE: 0.2\n",
            "  MODEL_FILE: models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
            "  NMS_THRE: 1.0\n",
            "  OKS_THRE: 0.9\n",
            "  POST_PROCESS: True\n",
            "  SHIFT_HEATMAP: True\n",
            "  SOFT_NMS: False\n",
            "  USE_GT_BBOX: True\n",
            "TRAIN:\n",
            "  BATCH_SIZE_PER_GPU: 20\n",
            "  BEGIN_EPOCH: 0\n",
            "  CHECKPOINT: \n",
            "  END_EPOCH: 100\n",
            "  GAMMA1: 0.99\n",
            "  GAMMA2: 0.0\n",
            "  LR: 0.0001\n",
            "  LR_END: 1e-05\n",
            "  LR_FACTOR: 0.25\n",
            "  LR_STEP: [100, 150, 200, 220]\n",
            "  MOMENTUM: 0.9\n",
            "  NESTEROV: False\n",
            "  OPTIMIZER: adam\n",
            "  RESUME: False\n",
            "  SHUFFLE: True\n",
            "  WD: 0.0001\n",
            "WORKERS: 24\n",
            "/usr/local/lib/python3.7/dist-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\n",
            "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /pytorch/aten/src/ATen/native/BinaryOps.cpp:467.)\n",
            "  return torch.floor_divide(self, other)\n",
            "==> Add Sine PositionEmbedding~\n",
            "=> init final conv weights from normal distribution\n",
            "=> init .weight as normal(0, 0.001)\n",
            "=> init .bias as 0\n",
            "=> loading pretrained model models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
            ":: pos_embedding is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
            ":: conv1.weight is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
            ":: bn1.weight is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
            ":: bn1.bias is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
            ":: bn1.running_mean is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
            ":: bn1.running_var is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
            ":: bn1.num_batches_tracked is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
            ":: layer1.0.conv1.weight is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
            ":: layer1.0.bn1.weight is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
            ":: layer1.0.bn1.bias is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
            ":: layer1.0.bn1.running_mean is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
            ":: layer1.0.bn1.running_var is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
            ":: layer1.0.bn1.num_batches_tracked is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
            ":: layer1.0.conv2.weight is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
            ":: layer1.0.bn2.weight is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
            ":: layer1.0.bn2.bias is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
            ":: layer1.0.bn2.running_mean is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
            ":: layer1.0.bn2.running_var is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
            ":: layer1.0.bn2.num_batches_tracked is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
            ":: layer1.0.conv3.weight is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
            ":: layer1.0.bn3.weight is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
            ":: layer1.0.bn3.bias is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
            ":: layer1.0.bn3.running_mean is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
            ":: layer1.0.bn3.running_var is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
            ":: layer1.0.bn3.num_batches_tracked is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
            ":: layer1.0.downsample.0.weight is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
            ":: layer1.0.downsample.1.weight is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
            ":: layer1.0.downsample.1.bias is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
            ":: layer1.0.downsample.1.running_mean is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
            ":: layer1.0.downsample.1.running_var is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
            ":: layer1.0.downsample.1.num_batches_tracked is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
            ":: layer1.1.conv1.weight is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
            ":: layer1.1.bn1.weight is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
            ":: layer1.1.bn1.bias is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
            ":: layer1.1.bn1.running_mean is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
            ":: layer1.1.bn1.running_var is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
            ":: layer1.1.bn1.num_batches_tracked is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
            ":: layer1.1.conv2.weight is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
            ":: layer1.1.bn2.weight is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
            ":: layer1.1.bn2.bias is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
            ":: layer1.1.bn2.running_mean is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
            ":: layer1.1.bn2.running_var is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
            ":: layer1.1.bn2.num_batches_tracked is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
            ":: layer1.1.conv3.weight is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
            ":: layer1.1.bn3.weight is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
            ":: layer1.1.bn3.bias is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
            ":: layer1.1.bn3.running_mean is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
            ":: layer1.1.bn3.running_var is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
            ":: layer1.1.bn3.num_batches_tracked is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
            ":: layer1.2.conv1.weight is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
            ":: layer1.2.bn1.weight is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
            ":: layer1.2.bn1.bias is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
            ":: layer1.2.bn1.running_mean is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
            ":: layer1.2.bn1.running_var is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
            ":: layer1.2.bn1.num_batches_tracked is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
            ":: layer1.2.conv2.weight is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
            ":: layer1.2.bn2.weight is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
            ":: layer1.2.bn2.bias is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
            ":: layer1.2.bn2.running_mean is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
            ":: layer1.2.bn2.running_var is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
            ":: layer1.2.bn2.num_batches_tracked is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
            ":: layer1.2.conv3.weight is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
            ":: layer1.2.bn3.weight is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
            ":: layer1.2.bn3.bias is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
            ":: layer1.2.bn3.running_mean is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
            ":: layer1.2.bn3.running_var is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
            ":: layer1.2.bn3.num_batches_tracked is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
            ":: layer2.0.conv1.weight is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
            ":: layer2.0.bn1.weight is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
            ":: layer2.0.bn1.bias is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
            ":: layer2.0.bn1.running_mean is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
            ":: layer2.0.bn1.running_var is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
            ":: layer2.0.bn1.num_batches_tracked is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
            ":: layer2.0.conv2.weight is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
            ":: layer2.0.bn2.weight is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
            ":: layer2.0.bn2.bias is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
            ":: layer2.0.bn2.running_mean is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
            ":: layer2.0.bn2.running_var is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
            ":: layer2.0.bn2.num_batches_tracked is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
            ":: layer2.0.conv3.weight is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
            ":: layer2.0.bn3.weight is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
            ":: layer2.0.bn3.bias is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
            ":: layer2.0.bn3.running_mean is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
            ":: layer2.0.bn3.running_var is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
            ":: layer2.0.bn3.num_batches_tracked is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
            ":: layer2.0.downsample.0.weight is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
            ":: layer2.0.downsample.1.weight is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
            ":: layer2.0.downsample.1.bias is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
            ":: layer2.0.downsample.1.running_mean is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
            ":: layer2.0.downsample.1.running_var is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
            ":: layer2.0.downsample.1.num_batches_tracked is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
            ":: layer2.1.conv1.weight is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
            ":: layer2.1.bn1.weight is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
            ":: layer2.1.bn1.bias is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
            ":: layer2.1.bn1.running_mean is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
            ":: layer2.1.bn1.running_var is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
            ":: layer2.1.bn1.num_batches_tracked is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
            ":: layer2.1.conv2.weight is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
            ":: layer2.1.bn2.weight is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
            ":: layer2.1.bn2.bias is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
            ":: layer2.1.bn2.running_mean is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
            ":: layer2.1.bn2.running_var is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
            ":: layer2.1.bn2.num_batches_tracked is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
            ":: layer2.1.conv3.weight is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
            ":: layer2.1.bn3.weight is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
            ":: layer2.1.bn3.bias is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
            ":: layer2.1.bn3.running_mean is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
            ":: layer2.1.bn3.running_var is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
            ":: layer2.1.bn3.num_batches_tracked is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
            ":: layer2.2.conv1.weight is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
            ":: layer2.2.bn1.weight is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
            ":: layer2.2.bn1.bias is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
            ":: layer2.2.bn1.running_mean is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
            ":: layer2.2.bn1.running_var is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
            ":: layer2.2.bn1.num_batches_tracked is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
            ":: layer2.2.conv2.weight is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
            ":: layer2.2.bn2.weight is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
            ":: layer2.2.bn2.bias is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
            ":: layer2.2.bn2.running_mean is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
            ":: layer2.2.bn2.running_var is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
            ":: layer2.2.bn2.num_batches_tracked is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
            ":: layer2.2.conv3.weight is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
            ":: layer2.2.bn3.weight is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
            ":: layer2.2.bn3.bias is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
            ":: layer2.2.bn3.running_mean is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
            ":: layer2.2.bn3.running_var is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
            ":: layer2.2.bn3.num_batches_tracked is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
            ":: layer2.3.conv1.weight is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
            ":: layer2.3.bn1.weight is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
            ":: layer2.3.bn1.bias is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
            ":: layer2.3.bn1.running_mean is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
            ":: layer2.3.bn1.running_var is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
            ":: layer2.3.bn1.num_batches_tracked is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
            ":: layer2.3.conv2.weight is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
            ":: layer2.3.bn2.weight is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
            ":: layer2.3.bn2.bias is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
            ":: layer2.3.bn2.running_mean is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
            ":: layer2.3.bn2.running_var is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
            ":: layer2.3.bn2.num_batches_tracked is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
            ":: layer2.3.conv3.weight is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
            ":: layer2.3.bn3.weight is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
            ":: layer2.3.bn3.bias is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
            ":: layer2.3.bn3.running_mean is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
            ":: layer2.3.bn3.running_var is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
            ":: layer2.3.bn3.num_batches_tracked is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
            ":: reduce.weight is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
            ":: global_encoder.layers.0.self_attn.in_proj_weight is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
            ":: global_encoder.layers.0.self_attn.in_proj_bias is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
            ":: global_encoder.layers.0.self_attn.out_proj.weight is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
            ":: global_encoder.layers.0.self_attn.out_proj.bias is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
            ":: global_encoder.layers.0.linear1.weight is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
            ":: global_encoder.layers.0.linear1.bias is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
            ":: global_encoder.layers.0.linear2.weight is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
            ":: global_encoder.layers.0.linear2.bias is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
            ":: global_encoder.layers.0.norm1.weight is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
            ":: global_encoder.layers.0.norm1.bias is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
            ":: global_encoder.layers.0.norm2.weight is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
            ":: global_encoder.layers.0.norm2.bias is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
            ":: global_encoder.layers.1.self_attn.in_proj_weight is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
            ":: global_encoder.layers.1.self_attn.in_proj_bias is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
            ":: global_encoder.layers.1.self_attn.out_proj.weight is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
            ":: global_encoder.layers.1.self_attn.out_proj.bias is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
            ":: global_encoder.layers.1.linear1.weight is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
            ":: global_encoder.layers.1.linear1.bias is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
            ":: global_encoder.layers.1.linear2.weight is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
            ":: global_encoder.layers.1.linear2.bias is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
            ":: global_encoder.layers.1.norm1.weight is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
            ":: global_encoder.layers.1.norm1.bias is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
            ":: global_encoder.layers.1.norm2.weight is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
            ":: global_encoder.layers.1.norm2.bias is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
            ":: global_encoder.layers.2.self_attn.in_proj_weight is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
            ":: global_encoder.layers.2.self_attn.in_proj_bias is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
            ":: global_encoder.layers.2.self_attn.out_proj.weight is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
            ":: global_encoder.layers.2.self_attn.out_proj.bias is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
            ":: global_encoder.layers.2.linear1.weight is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
            ":: global_encoder.layers.2.linear1.bias is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
            ":: global_encoder.layers.2.linear2.weight is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
            ":: global_encoder.layers.2.linear2.bias is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
            ":: global_encoder.layers.2.norm1.weight is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
            ":: global_encoder.layers.2.norm1.bias is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
            ":: global_encoder.layers.2.norm2.weight is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
            ":: global_encoder.layers.2.norm2.bias is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
            ":: global_encoder.layers.3.self_attn.in_proj_weight is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
            ":: global_encoder.layers.3.self_attn.in_proj_bias is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
            ":: global_encoder.layers.3.self_attn.out_proj.weight is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
            ":: global_encoder.layers.3.self_attn.out_proj.bias is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
            ":: global_encoder.layers.3.linear1.weight is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
            ":: global_encoder.layers.3.linear1.bias is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
            ":: global_encoder.layers.3.linear2.weight is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
            ":: global_encoder.layers.3.linear2.bias is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
            ":: global_encoder.layers.3.norm1.weight is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
            ":: global_encoder.layers.3.norm1.bias is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
            ":: global_encoder.layers.3.norm2.weight is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
            ":: global_encoder.layers.3.norm2.bias is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
            ":: deconv_layers.0.weight is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
            ":: deconv_layers.1.weight is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
            ":: deconv_layers.1.bias is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
            ":: deconv_layers.1.running_mean is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
            ":: deconv_layers.1.running_var is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
            ":: deconv_layers.1.num_batches_tracked is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
            "Using aug in training\n",
            "loading annotations into memory...\n",
            "Done (t=0.08s)\n",
            "creating index...\n",
            "index created!\n",
            "=> classes: ['__background__', 'person']\n",
            "=> num_images: 3600\n",
            "=> load 3600 samples\n",
            "loading annotations into memory...\n",
            "Done (t=0.00s)\n",
            "creating index...\n",
            "index created!\n",
            "=> classes: ['__background__', 'person']\n",
            "=> num_images: 450\n",
            "=> load 450 samples\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 24 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "=> current learning rate is 0.000100\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
            "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n",
            "Epoch: [0][0/180]\tTime 12.346s (12.346s)\tSpeed 1.6 samples/s\tData 8.616s (8.616s)\tLoss 0.00172 (0.00172)\tAccuracy 0.062 (0.062)\n",
            "Epoch: [0][100/180]\tTime 0.427s (0.568s)\tSpeed 46.8 samples/s\tData 0.000s (0.092s)\tLoss 0.00068 (0.00117)\tAccuracy 0.801 (0.487)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "Test: [0/57]\tTime 3.390 (3.390)\tLoss 0.0003 (0.0003)\tAccuracy 0.964 (0.964)\n",
            "=> writing results json to output/coco/transpose_r/config/results/keypoints_val2017_results_0.json\n",
            "Loading and preparing results...\n",
            "DONE (t=0.02s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *keypoints*\n",
            "DONE (t=0.12s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.01s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets= 20 ] = 0.704\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets= 20 ] = 0.989\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets= 20 ] = 0.791\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets= 20 ] = -1.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets= 20 ] = 0.704\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 20 ] = 0.738\n",
            " Average Recall     (AR) @[ IoU=0.50      | area=   all | maxDets= 20 ] = 0.993\n",
            " Average Recall     (AR) @[ IoU=0.75      | area=   all | maxDets= 20 ] = 0.824\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets= 20 ] = -1.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets= 20 ] = 0.738\n",
            "| Arch | AP | Ap .5 | AP .75 | AP (M) | AP (L) | AR | AR .5 | AR .75 | AR (M) | AR (L) |\n",
            "|---|---|---|---|---|---|---|---|---|---|---|\n",
            "| transpose_r | 0.704 | 0.989 | 0.791 | -1.000 | 0.704 | 0.738 | 0.993 | 0.824 | -1.000 | 0.738 |\n",
            "=> saving checkpoint to output/coco/transpose_r/config\n",
            "=> current learning rate is 0.000100\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "Epoch: [1][0/180]\tTime 9.224s (9.224s)\tSpeed 2.2 samples/s\tData 7.787s (7.787s)\tLoss 0.00058 (0.00058)\tAccuracy 0.787 (0.787)\n",
            "Epoch: [1][100/180]\tTime 0.453s (0.578s)\tSpeed 44.1 samples/s\tData 0.009s (0.084s)\tLoss 0.00048 (0.00059)\tAccuracy 0.861 (0.785)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "Test: [0/57]\tTime 1.751 (1.751)\tLoss 0.0002 (0.0002)\tAccuracy 1.000 (1.000)\n",
            "=> writing results json to output/coco/transpose_r/config/results/keypoints_val2017_results_0.json\n",
            "Loading and preparing results...\n",
            "DONE (t=0.02s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *keypoints*\n",
            "DONE (t=0.12s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.01s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets= 20 ] = 0.716\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets= 20 ] = 0.978\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets= 20 ] = 0.815\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets= 20 ] = -1.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets= 20 ] = 0.716\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 20 ] = 0.746\n",
            " Average Recall     (AR) @[ IoU=0.50      | area=   all | maxDets= 20 ] = 0.987\n",
            " Average Recall     (AR) @[ IoU=0.75      | area=   all | maxDets= 20 ] = 0.840\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets= 20 ] = -1.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets= 20 ] = 0.746\n",
            "| Arch | AP | Ap .5 | AP .75 | AP (M) | AP (L) | AR | AR .5 | AR .75 | AR (M) | AR (L) |\n",
            "|---|---|---|---|---|---|---|---|---|---|---|\n",
            "| transpose_r | 0.716 | 0.978 | 0.815 | -1.000 | 0.716 | 0.746 | 0.987 | 0.840 | -1.000 | 0.746 |\n",
            "=> saving checkpoint to output/coco/transpose_r/config\n",
            "=> current learning rate is 0.000100\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "Epoch: [2][0/180]\tTime 8.742s (8.742s)\tSpeed 2.3 samples/s\tData 7.764s (7.764s)\tLoss 0.00061 (0.00061)\tAccuracy 0.787 (0.787)\n",
            "Epoch: [2][100/180]\tTime 0.451s (0.579s)\tSpeed 44.4 samples/s\tData 0.000s (0.092s)\tLoss 0.00051 (0.00054)\tAccuracy 0.834 (0.810)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "Test: [0/57]\tTime 1.615 (1.615)\tLoss 0.0003 (0.0003)\tAccuracy 0.973 (0.973)\n",
            "=> writing results json to output/coco/transpose_r/config/results/keypoints_val2017_results_0.json\n",
            "Loading and preparing results...\n",
            "DONE (t=0.09s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *keypoints*\n",
            "DONE (t=0.11s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.01s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets= 20 ] = 0.731\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets= 20 ] = 0.989\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets= 20 ] = 0.821\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets= 20 ] = -1.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets= 20 ] = 0.731\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 20 ] = 0.759\n",
            " Average Recall     (AR) @[ IoU=0.50      | area=   all | maxDets= 20 ] = 0.991\n",
            " Average Recall     (AR) @[ IoU=0.75      | area=   all | maxDets= 20 ] = 0.844\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets= 20 ] = -1.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets= 20 ] = 0.759\n",
            "| Arch | AP | Ap .5 | AP .75 | AP (M) | AP (L) | AR | AR .5 | AR .75 | AR (M) | AR (L) |\n",
            "|---|---|---|---|---|---|---|---|---|---|---|\n",
            "| transpose_r | 0.731 | 0.989 | 0.821 | -1.000 | 0.731 | 0.759 | 0.991 | 0.844 | -1.000 | 0.759 |\n",
            "=> saving checkpoint to output/coco/transpose_r/config\n",
            "=> current learning rate is 0.000100\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "Epoch: [3][0/180]\tTime 9.036s (9.036s)\tSpeed 2.2 samples/s\tData 8.027s (8.027s)\tLoss 0.00045 (0.00045)\tAccuracy 0.834 (0.834)\n",
            "Epoch: [3][100/180]\tTime 0.459s (0.592s)\tSpeed 43.6 samples/s\tData 0.007s (0.086s)\tLoss 0.00049 (0.00052)\tAccuracy 0.761 (0.815)\n",
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f7cb0c06170>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\", line 1282, in _shutdown_workers\n",
            "    self._worker_result_queue.put((None, None))\n",
            "  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 87, in put\n",
            "    self._start_thread()\n",
            "  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 170, in _start_thread\n",
            "    self._thread.start()\n",
            "  File \"/usr/lib/python3.7/threading.py\", line 857, in start\n",
            "    self._started.wait()\n",
            "  File \"/usr/lib/python3.7/threading.py\", line 552, in wait\n",
            "    signaled = self._cond.wait(timeout)\n",
            "  File \"/usr/lib/python3.7/threading.py\", line 296, in wait\n",
            "    waiter.acquire()\n",
            "KeyboardInterrupt: \n",
            "Traceback (most recent call last):\n",
            "  File \"tools/train.py\", line 263, in <module>\n",
            "    main()\n",
            "  File \"tools/train.py\", line 224, in main\n",
            "    final_output_dir, tb_log_dir, writer_dict)\n",
            "  File \"/content/transpose/tools/../lib/core/function.py\", line 62, in train\n",
            "    optimizer.step()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py\", line 65, in wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/optim/optimizer.py\", line 88, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/autograd/grad_mode.py\", line 28, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/optim/adam.py\", line 118, in step\n",
            "    eps=group['eps'])\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/optim/_functional.py\", line 94, in adam\n",
            "    denom = (exp_avg_sq.sqrt() / math.sqrt(bias_correction2)).add_(eps)\n",
            "KeyboardInterrupt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UZZyCZaFL_wA"
      },
      "source": [
        "if not INFER_ONLY:\n",
        "    MODEL_PATH = '/content/transpose/output/coco/transpose_r/config/model_best.pth'\n",
        "    shutil.rmtree('/content/transpose/data')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jVn9mdQrNBHL"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VrKMrccYNESE"
      },
      "source": [
        "# INFER"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pFVOSeTFNFuk"
      },
      "source": [
        "if INFER_ONLY:\n",
        "    MODEL_PATH = TRAINED_MODEL_PATH"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H36W3h4QNctt",
        "outputId": "a495e9fc-4ddb-4d92-b36d-520e341890ff"
      },
      "source": [
        "%%writefile /content/infer_data_coco.py\n",
        "\n",
        "import argparse\n",
        "import numpy as np\n",
        "import cv2 \n",
        "import matplotlib.pyplot as plt\n",
        "import scipy.io as scio\n",
        "import pandas as pd\n",
        "from tqdm.notebook import tqdm\n",
        "import os, shutil\n",
        "from glob import glob\n",
        "tqdm.pandas()\n",
        "import json\n",
        "import datetime\n",
        "import imagesize\n",
        "from sklearn.model_selection import GroupKFold \n",
        "import scipy.io as scio\n",
        "import cv2\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "def load_kps(kp_path, width, height, new_width, new_height):\n",
        "    gt  = scio.loadmat(kp_path)['joints_gt'] # label = if_ocluded\n",
        "    kps = gt.transpose(2, 1, 0).astype(np.float64) # => (num_image, num_limb, 3) or (None, 14, 3)\n",
        "    kps[..., 0] = (kps[...,0]-1)/width*new_width    # converting one indexing to zero indexing\n",
        "    kps[..., 1] = (kps[...,1]-1)/height*new_height\n",
        "    kps[..., 2] = 2- kps[...,2] # coco format\n",
        "    return kps.astype(np.int32)\n",
        "\n",
        "def load_image(image_path):\n",
        "    return cv2.imread(image_path)[...,::-1]\n",
        "\n",
        "\n",
        "def read_resize(file_path, dim=128, width=128, height=128, aspect_ratio=True):\n",
        "    img = load_image(file_path)\n",
        "    h, w = img.shape[:2]  # orig hw\n",
        "    if aspect_ratio:\n",
        "        r = dim / max(h, w)  # resize image to img_size\n",
        "        interp = cv2.INTER_AREA if r < 1 else cv2.INTER_LINEAR\n",
        "        if r != 1:  # always resize down, only resize up if training with augmentation\n",
        "            img = cv2.resize(img, (int(w * r), int(h * r)), interpolation=interp)\n",
        "            new_h, new_w = img.shape[:2]\n",
        "    else:\n",
        "        img = cv2.resize(img, (width,height), cv2.INTER_AREA)\n",
        "        new_w = dim; new_h = dim\n",
        "        \n",
        "    return img, w, h\n",
        "\n",
        "\n",
        "def get_image_info(file_name, height, width, id,\n",
        "                   license=1, date_captured='', \n",
        "                   coco_url='', flickr_url='',):\n",
        "    return dict(license=license, \n",
        "                file_name=file_name,\n",
        "                coco_url=coco_url,\n",
        "                height=height,\n",
        "                width=width, \n",
        "                date_captured=date_captured,\n",
        "                flickr_url=flickr_url,\n",
        "                id=id)\n",
        "    \n",
        "\n",
        "\n",
        "def get_annot_info(kps, id, image_id, category_id=1, bbox=None, area=None,\n",
        "                   iscrowd=0, segmentation=None):\n",
        "    \"\"\"\n",
        "    Parameters:\n",
        "    --------------- \n",
        "    Args:\n",
        "    \n",
        "    Returns:\n",
        "    \n",
        "    \"\"\"\n",
        "    return {\n",
        "        \"segmentation\": segmentation or [],\n",
        "        \"num_keypoints\": len(kps)//3,\n",
        "        \"area\": area if area else (bbox[2]*bbox[3]),\n",
        "        \"iscrowd\": iscrowd,\n",
        "        \"keypoints\": kps,\n",
        "        \"image_id\": image_id,\n",
        "        \"bbox\": bbox or [0, 0, 0, 0],\n",
        "        \"category_id\": category_id,\n",
        "        \"id\": id,\n",
        "    }\n",
        "\n",
        "\n",
        "def get_bbox_info(id, image_id, category_id=1, bbox=None, area=None,\n",
        "                   iscrowd=0, segmentation=None):\n",
        "    \"\"\"\n",
        "    Parameters:\n",
        "    --------------- \n",
        "    Args:\n",
        "    \n",
        "    Returns:\n",
        "    \n",
        "    \"\"\"\n",
        "    return {\n",
        "        \"segmentation\": segmentation or [],\n",
        "        \"num_keypoints\": 14,\n",
        "        \"area\": area if area else (bbox[2]*bbox[3]),\n",
        "        \"iscrowd\": iscrowd,\n",
        "        \"keypoints\": [0,0,2]*14,\n",
        "        \"image_id\": image_id,\n",
        "        \"bbox\": bbox or [0, 0, 0, 0],\n",
        "        \"category_id\": category_id,\n",
        "        \"id\": id,\n",
        "    }\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('--dim', type=int, default=128, help='resized image shape')\n",
        "    parser.add_argument('--fold', type=int, default=0, help='fold number')\n",
        "    parser.add_argument('--is_annot', action='store_true', help=\"is there annotaions to use\")\n",
        "    parser.add_argument('--is_test', action='store_true', help=\"testing\")\n",
        "    parser.add_argument('--vip_folder', type=str, default=\"train\", help=\"VIP CUP DATA FOLDER\")\n",
        "    parser.add_argument(\"--coco_folder\", type=str, default=\"train\", help=\"folder used in coco dataset\")\n",
        "    parser.add_argument(\"--bbox_label_test\", type=str, default=\"/content/labels\", help=\"folder containing yolo labels of test person bbox\")\n",
        "    parser.add_argument(\"--base_dir\", type=str, default=\"/content/data\", help=\"base dir for vip dataset folder\")\n",
        "    parser.add_argument(\"--label\", type=str, default=\"uncover\" , help=\"uncover, cover1, cover2\")\n",
        "    parser.add_argument(\"--label2\", type=str, default=\"null\" , help=\"cover1, cover2\")\n",
        "    parser.add_argument(\"--out_dir\", type=str, default=\"/content\" , help=\"output directory\")\n",
        "    parser.add_argument('--is_aspect_ratio', action='store_true', help=\"mainatain aspect ratio. Only use dim. don't use width and height\")\n",
        "    parser.add_argument('--width', type=int, default=128, help='fold number')\n",
        "    parser.add_argument('--height', type=int, default=128, help='fold number')\n",
        "\n",
        "    opt = parser.parse_args()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    skeleton = [(0, 1), (1, 2), (12, 2), (12, 3), (3, 4), (4, 5), (6, 7),\n",
        "                (7, 8), (8, 12), (12, 9), (9, 10), (10, 11), (12, 13)]\n",
        "    skeleton = [[x[0]+1, x[1]+1] for x in skeleton]\n",
        "    name2idx = {\n",
        "        \"Right ankle\":0,\n",
        "        \"Right knee\":1,\n",
        "        \"Right hip\":2,\n",
        "        \"Left hip\":3,\n",
        "        \"Left knee\":4,\n",
        "        \"Left ankle\":5,\n",
        "        \"Right wrist\":6,\n",
        "        \"Right elbow\":7,\n",
        "        \"Right shoulder\":8,\n",
        "        \"Left shoulder\":9,\n",
        "        \"Left elbow\":10,\n",
        "        \"Left wrist\":11,\n",
        "        \"thorax\":12,\n",
        "        \"head top\":13, \n",
        "    }\n",
        "    idx2name = {v:k for k,v in name2idx.items()}\n",
        "    names = list(idx2name.values())\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def get_info(filepath):\n",
        "        x = filepath.split('/')\n",
        "        image_id = x[-1]\n",
        "        label    = x[-2]\n",
        "        modality = x[-3]\n",
        "        study_id = x[-4]\n",
        "        split    = x[-5]\n",
        "        return [filepath, study_id, image_id, modality, label, split]\n",
        "\n",
        "\n",
        "\n",
        "    filepaths = glob(f'{opt.base_dir}/**/*png', recursive=True)\n",
        "    filepaths.sort()\n",
        "    df = pd.DataFrame(list(map(get_info, filepaths)), columns=['image_path', 'study_id', 'image_id',\n",
        "                                                            'modality', 'label', 'split'])\n",
        "\n",
        "\n",
        "\n",
        "    df['rgb_gt_path']    = df.image_path.map(lambda x: os.path.join(x.rsplit('/', 3)[0], 'joints_gt_RGB.mat'))\n",
        "    df['ir_gt_path']     = df.image_path.map(lambda x: os.path.join(x.rsplit('/', 3)[0], 'joints_gt_IR.mat'))\n",
        "    df['rgb_align_path'] = df.image_path.map(lambda x: os.path.join(x.rsplit('/', 3)[0], 'align_PTr_RGB.npy'))\n",
        "    df['ir_align_path']  = df.image_path.map(lambda x: os.path.join(x.rsplit('/', 3)[0], 'align_PTr_IR.npy'))\n",
        "\n",
        "    df[['width', 'height']] = df.image_path.progress_apply(lambda x: list(imagesize.get(x))).tolist()\n",
        "\n",
        "\n",
        "        \n",
        "    df = df[df.split == opt.vip_folder]\n",
        "    df = df[df.modality == \"IR\"]\n",
        "\n",
        "    if opt.vip_folder == \"train\":\n",
        "        gkf = GroupKFold(n_splits=5)\n",
        "        df['fold'] = -1\n",
        "        df.reset_index(drop=True, inplace=True)\n",
        "        for fold, (train_idx, val_idx) in enumerate(gkf.split(df, groups=df['study_id'])):\n",
        "            df.loc[val_idx, 'fold'] = fold\n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "    FOLD = opt.fold\n",
        "    if opt.vip_folder == \"train\" and opt.coco_folder == \"train\":\n",
        "        train_df = df[(df.fold!=FOLD) & (df.label==opt.label)]\n",
        "    elif opt.vip_folder == \"train\" and opt.coco_folder == \"val\":\n",
        "        train_df = df[(df.fold==FOLD) & (df.label==opt.label)]\n",
        "    else:\n",
        "        if opt.label2 == \"null\":\n",
        "            train_df = df[(df.label==opt.label)]\n",
        "        else:\n",
        "            train_df = df[(df.label==opt.label) | (df.label==opt.label2)]\n",
        "\n",
        "\n",
        "    INFO = {\n",
        "        \"description\": \"VIP CUP 2021 Dataset\",\n",
        "        \"url\": \"https://www.kaggle.com/awsaf49/ieee-vip-cup-2021-train-val-dataset\",\n",
        "        \"version\": \"0.1.0\",\n",
        "        \"year\": 2021,\n",
        "        \"contributor\": \"awsaf\",\n",
        "        \"date_created\": datetime.datetime.utcnow().isoformat(' ')\n",
        "    }\n",
        "\n",
        "    LICENSES = [\n",
        "        {\n",
        "            \"id\": 1,\n",
        "            \"name\": \"Attribution-NonCommercial-ShareAlike License\",\n",
        "            \"url\": \"http://creativecommons.org/licenses/by-nc-sa/2.0/\"\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    CATEGORIES = [\n",
        "        {\n",
        "            'id': 1,\n",
        "            'name': 'person',\n",
        "            'supercategory': 'person',\n",
        "            \"keypoints\": names,\n",
        "            \"skeleton\": skeleton\n",
        "        },\n",
        "    ]\n",
        "\n",
        "    coco_output = {\n",
        "        \"info\": INFO,\n",
        "        \"licenses\": LICENSES,\n",
        "        \"categories\": CATEGORIES,\n",
        "        \"images\": [],\n",
        "        \"annotations\": []\n",
        "    }\n",
        "\n",
        "\n",
        "\n",
        "    image_dir = f'{opt.out_dir}/coco2017/{opt.coco_folder}2017'\n",
        "    annot_dir = f'{opt.out_dir}/coco2017/annotations'\n",
        "    os.makedirs(image_dir, exist_ok=True)\n",
        "    os.makedirs(annot_dir, exist_ok=True)\n",
        "\n",
        "\n",
        "\n",
        "    IMAGES = []\n",
        "    ANNOTATIONS = []\n",
        "    \n",
        "    coco_df = {\n",
        "        'coco_name' : [],\n",
        "        'orig_name' : []\n",
        "    }\n",
        "    \n",
        "    CHANGED = 0\n",
        "    coco_image_id=1\n",
        "    coco_annot_id=1\n",
        "    for idx in tqdm(range(train_df.shape[0]), total=len(train_df)):\n",
        "        image_path = train_df.image_path.iloc[idx]\n",
        "        image_id   = train_df.image_id.iloc[idx]\n",
        "        study_id   = train_df.study_id.iloc[idx]\n",
        "        image_idx  = int(image_id.split('.')[0].split('_')[-1])-1\n",
        "        if opt.is_aspect_ratio:\n",
        "            image, width, height  = read_resize(image_path, dim=opt.dim)\n",
        "        else:\n",
        "            image, width, height  = read_resize(image_path, width=opt.width, height=opt.height, aspect_ratio=False)\n",
        "        new_height, new_width = image.shape[:2]\n",
        "        orig_file_name = study_id + '_' + image_path.split(\"/\")[3] + \"_\" + image_path.split('/')[-1]\n",
        "\n",
        "        # for transpose\n",
        "        file_name = '%012d.png' % coco_image_id\n",
        "        coco_df['coco_name'].append(file_name)\n",
        "        coco_df['orig_name'].append(orig_file_name)\n",
        "        \n",
        "        new_image_path  = os.path.join(image_dir,file_name)\n",
        "        # writing image\n",
        "        cv2.imwrite(new_image_path, image[...,::-1])\n",
        "        # writing data\n",
        "        IMAGES.append(get_image_info(file_name, \n",
        "                                    height=int(new_height), \n",
        "                                    width=int(new_width), \n",
        "                                    id=coco_image_id,))\n",
        "        \n",
        "        if opt.is_test:\n",
        "            label_file = opt.bbox_label_test + '/' + orig_file_name[:-3] + 'txt'\n",
        "            # SEE THIS \n",
        "            label_file = label_file.replace('test1', 'ieee-vip-cup-2021-train-val-dataset')\n",
        "            with open(label_file, \"r\") as f:\n",
        "                data = np.array(f.read().replace('\\n', ' ').strip().split(' ')).astype(np.float32).reshape(-1, 6)[0]\n",
        "                xc, yc, w, h, conf = data[1], data[2], data[3], data[4], data[5]\n",
        "                # using new height, new width --> REMEMBER THIS\n",
        "                xc, yc = xc*new_width, yc*new_height\n",
        "                w, h = w*new_width, h*new_height\n",
        "                if conf < 0.5:\n",
        "                    new_h = h + h*.1\n",
        "                    h = min(new_h, new_height)\n",
        "                    CHANGED += 1\n",
        "                \n",
        "            xmin, ymin = xc - (w/2), yc - (h/2)\n",
        "            bbox = [int(xmin), int(ymin), int(w), int(h)]\n",
        "            ANNOTATIONS.append(get_bbox_info(id=coco_annot_id, image_id=coco_image_id, category_id=1,\n",
        "                                            bbox=bbox, \n",
        "                                            area=w*h,\n",
        "                                            iscrowd=0,\n",
        "                                            segmentation=None))\n",
        "            # print(ANNOTATIONS)\n",
        "            coco_annot_id+=1\n",
        "            \n",
        "\n",
        "        if opt.is_annot:\n",
        "            kp_path = train_df.ir_gt_path.iloc[idx]\n",
        "            kps = load_kps(kp_path, \n",
        "                        width, height,\n",
        "                        new_width, new_height)\n",
        "            # kp of a image\n",
        "            kps_img = kps[image_idx]\n",
        "            # bbox from keypoints\n",
        "            xmin, ymin, xmax, ymax = np.min(kps_img[...,0]), np.min(kps_img[...,1]), np.max(kps_img[...,0]), np.max(kps_img[...,1])\n",
        "            offsetMin = int(15 * np.square((new_height*new_width) / (512*384)))\n",
        "            offsetMax = int(35 * np.square((new_height*new_width) / (512*384)))\n",
        "            xmin, ymin = int(xmin-offsetMin), int(ymin-offsetMax) # kp are too close to body so taking offset\n",
        "            xmin = max(0, xmin)\n",
        "            ymin = max(0, ymin)\n",
        "            w,h = int(xmax-xmin+offsetMin), int(ymax-ymin+offsetMax)\n",
        "            if opt.is_aspect_ratio:\n",
        "                w = min(w, opt.dim)\n",
        "                h = min(h, opt.dim)\n",
        "            else:\n",
        "                w = min(w, opt.width)\n",
        "                h = min(h, opt.height)\n",
        "            bbox = [xmin, ymin, w, h]\n",
        "\n",
        "            #============================\n",
        "            kps_img = [int(x) for x in kps_img.reshape(-1).tolist()]\n",
        "            \n",
        "            \n",
        "            \n",
        "            ANNOTATIONS.append(get_annot_info(kps=kps_img, id=coco_annot_id, image_id=coco_image_id, category_id=1,\n",
        "                                            bbox=bbox, \n",
        "                                            area=w*h,\n",
        "                                            iscrowd=0,\n",
        "                                            segmentation=None))\n",
        "            \n",
        "            coco_annot_id+=1\n",
        "        coco_image_id+=1\n",
        "        \n",
        "    #===========================\n",
        "    coco_output[\"images\"]      = IMAGES\n",
        "    coco_output[\"annotations\"] = ANNOTATIONS\n",
        "\n",
        "    # json file\n",
        "    with open(f'{annot_dir}/person_keypoints_{opt.coco_folder}2017.json', 'w') as output_json_file:\n",
        "        json.dump(coco_output, output_json_file)   \n",
        "\n",
        "    coco_df = pd.DataFrame(coco_df)\n",
        "    coco_df.to_csv(f'{opt.coco_folder}.csv',index=False)\n",
        "    print(f\"Total {len(os.listdir(image_dir))} images found\")\n",
        "    print(f\"Changed bboxes : {CHANGED}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing /content/infer_data_coco.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gyZAGMAqNo1d",
        "outputId": "571021d2-6675-47f5-9048-944f05a1c3ad"
      },
      "source": [
        "!python /content/infer_data_coco.py --width 384 \\\n",
        "--height 512 \\\n",
        "--vip_folder \"test1\" \\\n",
        "--coco_folder \"val\" \\\n",
        "--label \"cover1\" \\\n",
        "--label2 \"cover2\" \\\n",
        "--fold 0 \\\n",
        "--base_dir \"/content/data\" \\\n",
        "--out_dir \"/content\" \\\n",
        "--is_test \\\n",
        "--bbox_label_test $LABELS_TEST"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tqdm/std.py:658: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
            "  from pandas import Panel\n",
            "HBox(children=(FloatProgress(value=0.0, max=8550.0), HTML(value='')))\n",
            "\n",
            "HBox(children=(FloatProgress(value=0.0, max=450.0), HTML(value='')))\n",
            "\n",
            "Total 450 images found\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "jf9gRqxBNz53",
        "outputId": "607d64c6-27c1-474b-b6fc-fb7a5546fd19"
      },
      "source": [
        "shutil.move(\"/content/coco2017/val2017\", \"/content/coco2017/images/val2017\")\n",
        "shutil.copytree('/content/coco2017', '/content/transpose/data/coco')\n",
        "\n",
        "# configuring config\n",
        "import yaml\n",
        "with open(\"/content/config.yaml\", 'r') as stream:\n",
        "    try:\n",
        "        model_config = yaml.safe_load(stream)\n",
        "    except yaml.YAMLError as exc:\n",
        "        print(exc)\n",
        "\n",
        "model_config['TEST']['MODEL_FILE'] = MODEL_PATH\n",
        "\n",
        "with open('/content/config.yaml', 'w') as file:\n",
        "    yaml.dump(model_config, file)\n",
        "\n",
        "model_config['TEST']['MODEL_FILE']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/transpose/output/coco/transpose_r/config/model_best.pth'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "59oEF05FONCo",
        "outputId": "68e12f6a-d13c-4fa2-8a41-f1565a396b45"
      },
      "source": [
        "%cd /content/transpose\n",
        "\n",
        "# The score shown here is dummy. Only inference is done.\n",
        "\n",
        "!python tools/test.py --cfg /content/config.yaml"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/transpose\n",
            "=> creating output/coco/transpose_r/config\n",
            "=> creating log/coco/transpose_r/config_2021-08-01-20-48\n",
            "Namespace(cfg='/content/config.yaml', dataDir='', logDir='', modelDir='', opts=[], prevModelDir='')\n",
            "AUTO_RESUME: True\n",
            "CUDNN:\n",
            "  BENCHMARK: True\n",
            "  DETERMINISTIC: False\n",
            "  ENABLED: True\n",
            "DATASET:\n",
            "  COLOR_RGB: True\n",
            "  DATASET: coco\n",
            "  DATA_FORMAT: jpg\n",
            "  FLIP: True\n",
            "  HYBRID_JOINTS_TYPE: \n",
            "  NUM_JOINTS_HALF_BODY: 7\n",
            "  PROB_HALF_BODY: 0.3\n",
            "  ROOT: data/coco/\n",
            "  ROT_FACTOR: 45\n",
            "  SCALE_FACTOR: 0.35\n",
            "  SELECT_DATA: False\n",
            "  TEST_SET: val2017\n",
            "  TRAIN_SET: train2017\n",
            "DATA_DIR: \n",
            "DEBUG:\n",
            "  DEBUG: True\n",
            "  SAVE_BATCH_IMAGES_GT: True\n",
            "  SAVE_BATCH_IMAGES_PRED: True\n",
            "  SAVE_HEATMAPS_GT: True\n",
            "  SAVE_HEATMAPS_PRED: True\n",
            "GPUS: (0,)\n",
            "LOG_DIR: log\n",
            "LOSS:\n",
            "  TOPK: 8\n",
            "  USE_DIFFERENT_JOINTS_WEIGHT: False\n",
            "  USE_OHKM: False\n",
            "  USE_TARGET_WEIGHT: True\n",
            "MODEL:\n",
            "  ATTENTION_ACTIVATION: relu\n",
            "  BOTTLENECK_NUM: 0\n",
            "  DIM_FEEDFORWARD: 1024\n",
            "  DIM_MODEL: 256\n",
            "  ENCODER_LAYERS: 4\n",
            "  EXTRA:\n",
            "    DECONV_WITH_BIAS: False\n",
            "    FINAL_CONV_KERNEL: 1\n",
            "    NUM_DECONV_FILTERS: [256]\n",
            "    NUM_DECONV_KERNELS: [4]\n",
            "    NUM_DECONV_LAYERS: 1\n",
            "    NUM_LAYERS: 50\n",
            "  HEATMAP_SIZE: [48, 64]\n",
            "  IMAGE_SIZE: [192, 256]\n",
            "  INIT_WEIGHTS: True\n",
            "  INTERMEDIATE_SUP: False\n",
            "  NAME: transpose_r\n",
            "  NUM_JOINTS: 14\n",
            "  N_HEAD: 8\n",
            "  PE_ONLY_AT_BEGIN: False\n",
            "  POS_EMBEDDING: sine\n",
            "  PRETRAINED: models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
            "  SIGMA: 2\n",
            "  TAG_PER_JOINT: True\n",
            "  TARGET_TYPE: gaussian\n",
            "OUTPUT_DIR: output\n",
            "PIN_MEMORY: True\n",
            "PRINT_FREQ: 100\n",
            "RANK: 0\n",
            "TEST:\n",
            "  BATCH_SIZE_PER_GPU: 8\n",
            "  BBOX_THRE: 1.0\n",
            "  BLUR_KERNEL: 11\n",
            "  COCO_BBOX_FILE: \n",
            "  FLIP_TEST: True\n",
            "  IMAGE_THRE: 0.0\n",
            "  IN_VIS_THRE: 0.2\n",
            "  MODEL_FILE: /content/transpose/output/coco/transpose_r/config/model_best.pth\n",
            "  NMS_THRE: 1.0\n",
            "  OKS_THRE: 0.9\n",
            "  POST_PROCESS: True\n",
            "  SHIFT_HEATMAP: True\n",
            "  SOFT_NMS: False\n",
            "  USE_GT_BBOX: True\n",
            "TRAIN:\n",
            "  BATCH_SIZE_PER_GPU: 20\n",
            "  BEGIN_EPOCH: 0\n",
            "  CHECKPOINT: \n",
            "  END_EPOCH: 100\n",
            "  GAMMA1: 0.99\n",
            "  GAMMA2: 0.0\n",
            "  LR: 0.0001\n",
            "  LR_END: 1e-05\n",
            "  LR_FACTOR: 0.25\n",
            "  LR_STEP: [100, 150, 200, 220]\n",
            "  MOMENTUM: 0.9\n",
            "  NESTEROV: False\n",
            "  OPTIMIZER: adam\n",
            "  RESUME: False\n",
            "  SHUFFLE: True\n",
            "  WD: 0.0001\n",
            "WORKERS: 24\n",
            "/usr/local/lib/python3.7/dist-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\n",
            "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /pytorch/aten/src/ATen/native/BinaryOps.cpp:467.)\n",
            "  return torch.floor_divide(self, other)\n",
            "==> Add Sine PositionEmbedding~\n",
            "=> loading model from /content/transpose/output/coco/transpose_r/config/model_best.pth\n",
            "loading annotations into memory...\n",
            "Done (t=0.00s)\n",
            "creating index...\n",
            "index created!\n",
            "=> classes: ['__background__', 'person']\n",
            "=> num_images: 450\n",
            "=> load 450 samples\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 24 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
            "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n",
            "Test: [0/57]\tTime 2.370 (2.370)\tLoss 0.0030 (0.0030)\tAccuracy 0.000 (0.000)\n",
            "=> writing results json to output/coco/transpose_r/config/results/keypoints_val2017_results_0.json\n",
            "Loading and preparing results...\n",
            "DONE (t=0.02s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *keypoints*\n",
            "DONE (t=0.10s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.01s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets= 20 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets= 20 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets= 20 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets= 20 ] = -1.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets= 20 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 20 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50      | area=   all | maxDets= 20 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.75      | area=   all | maxDets= 20 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets= 20 ] = -1.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets= 20 ] = 0.000\n",
            "| Arch | AP | Ap .5 | AP .75 | AP (M) | AP (L) | AR | AR .5 | AR .75 | AR (M) | AR (L) |\n",
            "|---|---|---|---|---|---|---|---|---|---|---|\n",
            "| transpose_r | 0.000 | 0.000 | 0.000 | -1.000 | 0.000 | 0.000 | 0.000 | 0.000 | -1.000 | 0.000 |\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "Q8LOmsp1OVYV",
        "outputId": "a0f771d8-19bd-4490-ac0d-ec0de028beb6"
      },
      "source": [
        "RESULT = '/content/transpose/output/coco/transpose_r/config/results/keypoints_val2017_results_0.json'\n",
        "RESULT"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/transpose/output/coco/transpose_r/config/results/keypoints_val2017_results_0.json'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7dmwgzVwOfo1",
        "outputId": "064542bb-611e-4844-97a5-a60cdc9933e4"
      },
      "source": [
        "%%writefile /content/submission.py\n",
        "\n",
        "import argparse\n",
        "import numpy as np\n",
        "import os, shutil\n",
        "import json\n",
        "import datetime\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('--train_width', type=int, default=0, help='height of training image')\n",
        "    parser.add_argument('--train_height', type=int, default=0, help='height of training image')\n",
        "    parser.add_argument('--orig_width', type=int, default=0, help='original width')\n",
        "    parser.add_argument('--orig_height', type=int, default=0, help='original height')\n",
        "    parser.add_argument('--result_json_path', type=str, default=\"result.json\", help=\"result json\")\n",
        "\n",
        "    opt = parser.parse_args()\n",
        "\n",
        "    with open(opt.result_json_path) as f:\n",
        "        result = json.load(f)\n",
        "\n",
        "    points = []\n",
        "    for pred in result:\n",
        "        one_person = []\n",
        "        pred = pred['keypoints']\n",
        "        for idx in range(14):\n",
        "            x = int(pred[3*idx])\n",
        "            y = int(pred[3*idx + 1])\n",
        "            \n",
        "            x = int(x*(opt.orig_width/opt.train_width))\n",
        "            y = int(y*(opt.orig_height/opt.train_height))\n",
        "            one_person.append([x, y])\n",
        "        points.append(one_person)\n",
        "\n",
        "    array = np.array(points)\n",
        "    array[:,:,0] = array[:,:,0].clip(min=0, max=opt.orig_width-1)\n",
        "    array[:,:,1] = array[:,:,1].clip(min=0, max=opt.orig_height-1)\n",
        "\n",
        "    points = array.tolist()\n",
        "\n",
        "    \n",
        "    with open(\"/content/preds.json\", \"w\") as f:\n",
        "        json.dump(points, f)\n",
        "\n",
        "    print(\"Result saved in /content/preds.json.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing /content/submission.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k139e8srOtO8",
        "outputId": "0754acb2-1b85-48fd-d774-2a2d13e7249f"
      },
      "source": [
        "!python /content/submission.py --result_json_path $RESULT \\\n",
        "--train_width 384 \\\n",
        "--train_height 512 \\\n",
        "--orig_width 120 \\\n",
        "--orig_height 160"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Result saved in /content/preds.json.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zPNytxYeOwI6"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}